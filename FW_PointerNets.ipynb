{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FW-PointerNets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushbooG9/Pointer-Networks-Using-Fast-Weights/blob/master/FW_PointerNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FC0d2HChJIWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers import LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "#import tensorflow.contrib.eager as tfe\n",
        "import numpy as np \n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2Zjx0dbLqJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(Encoder, self).__init__()\n",
        "\t\tself.encoder = LSTM(hidden_size, return_sequence = True, return_state = True)\n",
        "\n",
        "\tdef call(self, x):\n",
        "\t\te, state_h, state_c  = self.encoder(x)        \n",
        "\t\treturn e, [state_h, state_c]\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(Decoder, self).__init__()\n",
        "\t\tself.decoder = LSTM(hidden_size, return_sequence = True, return_state = True)\n",
        "\n",
        "\tdef call(self, x, hidden_states):\n",
        "\t\td, state_h, state_c  = self.decoder(x, initial_state=hidden_states)\n",
        "\t\treturn d, [state_h, state_c]\n",
        "\n",
        "class PtrnetLSTM(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(PtrnetLSTM, self).__init__()\n",
        "\t\t#self.W1 = tfe.variable(tf.random_uniform([hidden_size, hidden_size], -0.08, 0.08), dtype=float32)\n",
        "\t\tself.W1 = tf.keras.layers.Dense(hidden_size, kernel_initializer= tf.keras.initializers.RandomUniform(minval = -0.08, maxval = 0.08, seed = None), use_bias=False)\n",
        "\t\tself.W2 = tf.keras.layers.Dense(hidden_size, kernel_initializer= tf.keras.initializers.RandomUniform(minval = -0.08, maxval = 0.08, seed = None), use_bias=False)\n",
        "\t\t#Dense layer -> dot(input, kernel) -> so now Ui = vT . tanh(W1 . e + W2 . di)  becomes Ui = tanh(e . W1 + di . W2) . v\n",
        "\t\tself.VT = tf.keras.layers.Dense(1, use_bias=False)\n",
        "\t# e= encoder output and d = decoder output\n",
        "\tdef call(self, e,d):\n",
        "\t\tu = self.VT(tf.nn.tanh(self.W1(e) + self.W2(d)))\n",
        "\t\tattention = tf.nn.softmax(u, axis = 1)\n",
        "\n",
        "\t\treturn tf.reshape(attention, attention.shape[0], attention.shape[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsMqoxxoEyPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class fastweights(tf.keras.Model):\n",
        "\tdef __init__(self, input_dim, elemnum , batch_size=128, decay_rate = 0.9, learning_rate = 0.5, hidden_size=512):\n",
        "\t\tsuper(fastweights, self).__init__()\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.DR = decay_rate\n",
        "\t\tself.LR = learning_rate\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.W_x = tf.Variable(tf.random_uniform([elemnum, hidden_size], -np.sqrt(2/elemnum), np.sqrt(2/elemnum)), dtype=tf.float32)\n",
        "\t\tself.B_x = tf.Variable(tf.zeros(hidden_size), dtype=tf.float32)\n",
        "\t\tself.W_h = tf.Variable(initial_value = 0.5 * np.identity(hidden_size), dtype = tf.float32)\n",
        "\t\tself.W_y = tf.Variable(tf.random_uniform([hidden_size, elemnum], -np.sqrt(2/hidden_size), np.sqrt(2/hidden_size)), dtype = tf.float32)\n",
        "\t\tself.B_y = tf.Variable(tf.zeros(elemnum), dtype= tf.float32)\n",
        "\t\tself.scale = tf.Variable(tf.ones(hidden_size), dtype = tf.float32)\n",
        "\t\tself.shift = tf.Variable(tf.zeros(hidden_size), dtype = tf.float32) \n",
        "\t\t#initial values of A and H matricies\n",
        "\t\tself.A = tf.zeros([self.batch_size, self.hidden_size,self.hidden_size], dtype = tf.float32)\n",
        "\t\tself.H = tf.zeros([self.batch_size, self.hidden_size], dtype = tf.float32)\n",
        "    \n",
        "\tdef call(self,X, Y, S=1):\n",
        "\t\tX = tf.cast(X, tf.float32)\n",
        "\t\tfor t in range(tf.shape(X)[1]):\n",
        "\t\t\t#first hidden state, A and H_s are  zero at this point so the part A(t)H_s(t+1) becomes zero\n",
        "\t\t\tself.H = tf.nn.relu((tf.matmul(self.H,self.W_h))+(tf.matmul(X[:, t, :],self.W_x)+self.B_x))\n",
        "\t\t\t#reshaping to use it with A, to calculate the A(t)H_s(t+1)\n",
        "\t\t\tH_s = tf.reshape(self.H, [self.batch_size, 1, self.hidden_size])\n",
        "\t\t\t#Initial A for this particular time step: A(t) = decay*A(t-1)+ learning*h(t)h(t).T\n",
        "\t\t\t#self.A = tf.add((tf.scalar_mul(self.DR, self.A)),(tf.batch_matmul(tf.transpose(self.H_s, [0,2,1]),self.H_s)))\n",
        "\t\t\tself.A = (tf.scalar_mul(self.DR, self.A))+ tf.scalar_mul(self.LR,(tf.matmul(tf.transpose(H_s, [0,2,1]),H_s)))\n",
        "\t\t\t#inner loop for fast weights, tfor S steps\n",
        "\t\t\tfor _ in range(S):\n",
        "\t\t\t\t#calculating H_s without the non linearity first, so we can use linear normalization \n",
        "\t\t\t\tH_s = tf.reshape(tf.matmul(self.H,self.W_h),tf.shape(H_s)) + tf.reshape(tf.matmul(X[:,t,:],self.W_x)+self.B_x,tf.shape(H_s)) + tf.matmul(H_s, self.A)\n",
        "\t\t\t\t#Applying Layer Normalization \n",
        "\t\t\t\tmean, var = tf.nn.moments(H_s, axes =2, keep_dims = True)\n",
        "\t\t\t\tH_s = (self.scale*(H_s - mean))/(tf.sqrt(var + 1e-5) + self.shift)\n",
        "\t\t\t\t#applying non linearity\n",
        "\t\t\t\tH_s = tf.nn.relu(H_s)\n",
        "\t\t\tself.H = tf.reshape(H_s,[self.batch_size, self.hidden_size])\n",
        "\t\tfinallayer = tf.matmul(self.H, self.W_y) + self.B_y\n",
        "\t\treturn finallayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8E7fGM1GazK",
        "colab_type": "code",
        "outputId": "a08244ae-4c30-41e0-a7b4-bf4e41034aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class fastweights(object):\n",
        "  def __init__(self, inputt, target, batch_size=128, decay_rate = 0.9, learning_rate = 0.5, hidden_size=512):\n",
        "    super(fastweights, self).__init__()\n",
        "    self.X = inputt\n",
        "    self.Y = target\n",
        "    self.Y_size = tf.shape(target)[0]\n",
        "    self.batch_size = batch_size\n",
        "    self.DR = decay_rate\n",
        "    self.LR = learning_rate\n",
        "    self.hidden_size = hidden_size\n",
        "    self.W_x = tfe.Variable(tf.random_uniform([tf.shape(inputt)[1], hidden_size], -np.sqrt(2/tf.shape(inputt)[1]), np.sqrt(2/tf.shape(inputt)[1])), dtype=float32)\n",
        "    self.B_x = tfe.Variable(tf.zeros(hidden_size), dtype=float32)\n",
        "    self.W_h = tfe.Variable(initial_value = 0.5 * np.identity(hidden_size), dtype = float32)\n",
        "    self.W_y = tfe.Variable(tf.random_uniform([hidden_size, output_size], -np.sqrt(2/hidden_size), np.sqrt(2/hidden_size)), dtype = float32)\n",
        "    self.B_y = tfe.Variable(tf.zeros(output_size), dtype=float32)\n",
        "    self.scale = tfe.Variable(tf.ones(hidden_size), dtype =float32)\n",
        "    self.shift = tfe.Variable(tf.zeros(hidden_size), dtype = float32) \n",
        "    #initial values of A and H matricies\n",
        "    self.A = tf.zeros([self.batch_size, self.hidden_size,self.hidden_size], dtype = float32)\n",
        "    self.H = tf.zeros([self.batch_size, self.hidden_size], dtype = float32)\n",
        "    \n",
        "  def call(self,S=1):\n",
        "    \n",
        "    for t in range(self.X.shape[1]):\n",
        "\t\t\t#first hidden state, A and H_s are  zero at this point so the part A(t)H_s(t+1) becomes zero\n",
        "      self.H = tf.nn.relu((tf.matmul(self.H,self.W_h))+(tf.matmul(self.X[:, t, :],self.W_x)+self.B_x))\n",
        "\t\t\t#reshaping to use it with A, to calculate the A(t)H_s(t+1)\n",
        "      H_s = tf.reshape(self.H, [self.batch_size, 1, self.hidden_size])\n",
        "\t\t\t#Initial A for this particular time step: A(t) = decay*A(t-1)+ learning*h(t)h(t).T\n",
        "\t\t\t#self.A = tf.add((tf.scalar_mul(self.DR, self.A)),(tf.batch_matmul(tf.transpose(self.H_s, [0,2,1]),self.H_s)))\n",
        "      self.A = (tf.scalar_mul(self.DR, self.A))+ tf.scalar_mul(self.LR,(tf.batch_matmul(tf.transpose(H_s, [0,2,1]),H_s)))\n",
        "\t\t\t#inner loop for fast weights, tfor S steps\n",
        "      for _ in range(S):\n",
        "\t\t\t\t#calculating H_s without the non linearity first, so we can use linear normalization \n",
        "        H_s = tf.reshape(tf.matmul(self.H,self.W_h),tf.shape(H_s)) + tf.reshape(tf.matmul(self.X[:,t,:],self.W_x)+self.B_x,tf.shape(H_s)) + tf.batch_matmul(H_s, self.A)\n",
        "\t\t\t\t#Applying Layer Normalization \n",
        "        mean, var = tf.nn.moments(H_s, axes =2, keep_dims = True)\n",
        "        H_s = (self.scale*(H_s - mean))/(tf.sqrt(var + 1e-5) + self.shift)\n",
        "\t\t\t\t#applying non linearity\n",
        "        H_s = tf.nn.relu(H_s)\n",
        "      self.H = tf.reshape(H_s,[self.batch_size, self.hidden_size])\n",
        "    finallayer = tf.matmul(self.H, self.W_y) + self.B_y\n",
        "    #self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(finallayer, self.Y))\n",
        "    #self.trainable_vars = tf.trainable_variables()\n",
        "    #self.lr = tfe.Variable(0.0, trainable = False)\n",
        "    #self.grads, self.norm = tf.clip_by_global_norm(tf.gradients(self.loss, self.trainable_vars), 5.0 )\n",
        "    #optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "    #self.update = optimizer.apply_gradients(zip(self.grads, self.trainable_vars))\n",
        "\n",
        "    return finallayer\n",
        "    \n",
        "   \"\"\" "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass fastweights(object):\\n  def __init__(self, inputt, target, batch_size=128, decay_rate = 0.9, learning_rate = 0.5, hidden_size=512):\\n    super(fastweights, self).__init__()\\n    self.X = inputt\\n    self.Y = target\\n    self.Y_size = tf.shape(target)[0]\\n    self.batch_size = batch_size\\n    self.DR = decay_rate\\n    self.LR = learning_rate\\n    self.hidden_size = hidden_size\\n    self.W_x = tfe.Variable(tf.random_uniform([tf.shape(inputt)[1], hidden_size], -np.sqrt(2/tf.shape(inputt)[1]), np.sqrt(2/tf.shape(inputt)[1])), dtype=float32)\\n    self.B_x = tfe.Variable(tf.zeros(hidden_size), dtype=float32)\\n    self.W_h = tfe.Variable(initial_value = 0.5 * np.identity(hidden_size), dtype = float32)\\n    self.W_y = tfe.Variable(tf.random_uniform([hidden_size, output_size], -np.sqrt(2/hidden_size), np.sqrt(2/hidden_size)), dtype = float32)\\n    self.B_y = tfe.Variable(tf.zeros(output_size), dtype=float32)\\n    self.scale = tfe.Variable(tf.ones(hidden_size), dtype =float32)\\n    self.shift = tfe.Variable(tf.zeros(hidden_size), dtype = float32) \\n    #initial values of A and H matricies\\n    self.A = tf.zeros([self.batch_size, self.hidden_size,self.hidden_size], dtype = float32)\\n    self.H = tf.zeros([self.batch_size, self.hidden_size], dtype = float32)\\n    \\n  def call(self,S=1):\\n    \\n    for t in range(self.X.shape[1]):\\n\\t\\t\\t#first hidden state, A and H_s are  zero at this point so the part A(t)H_s(t+1) becomes zero\\n      self.H = tf.nn.relu((tf.matmul(self.H,self.W_h))+(tf.matmul(self.X[:, t, :],self.W_x)+self.B_x))\\n\\t\\t\\t#reshaping to use it with A, to calculate the A(t)H_s(t+1)\\n      H_s = tf.reshape(self.H, [self.batch_size, 1, self.hidden_size])\\n\\t\\t\\t#Initial A for this particular time step: A(t) = decay*A(t-1)+ learning*h(t)h(t).T\\n\\t\\t\\t#self.A = tf.add((tf.scalar_mul(self.DR, self.A)),(tf.batch_matmul(tf.transpose(self.H_s, [0,2,1]),self.H_s)))\\n      self.A = (tf.scalar_mul(self.DR, self.A))+ tf.scalar_mul(self.LR,(tf.batch_matmul(tf.transpose(H_s, [0,2,1]),H_s)))\\n\\t\\t\\t#inner loop for fast weights, tfor S steps\\n      for _ in range(S):\\n\\t\\t\\t\\t#calculating H_s without the non linearity first, so we can use linear normalization \\n        H_s = tf.reshape(tf.matmul(self.H,self.W_h),tf.shape(H_s)) + tf.reshape(tf.matmul(self.X[:,t,:],self.W_x)+self.B_x,tf.shape(H_s)) + tf.batch_matmul(H_s, self.A)\\n\\t\\t\\t\\t#Applying Layer Normalization \\n        mean, var = tf.nn.moments(H_s, axes =2, keep_dims = True)\\n        H_s = (self.scale*(H_s - mean))/(tf.sqrt(var + 1e-5) + self.shift)\\n\\t\\t\\t\\t#applying non linearity\\n        H_s = tf.nn.relu(H_s)\\n      self.H = tf.reshape(H_s,[self.batch_size, self.hidden_size])\\n    finallayer = tf.matmul(self.H, self.W_y) + self.B_y\\n    #self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(finallayer, self.Y))\\n    #self.trainable_vars = tf.trainable_variables()\\n    #self.lr = tfe.Variable(0.0, trainable = False)\\n    #self.grads, self.norm = tf.clip_by_global_norm(tf.gradients(self.loss, self.trainable_vars), 5.0 )\\n    #optimizer = tf.train.AdamOptimizer(self.lr)\\n    #self.update = optimizer.apply_gradients(zip(self.grads, self.trainable_vars))\\n\\n    return finallayer\\n    \\n   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6CLG6-wx5m4P",
        "colab_type": "code",
        "outputId": "15d4d197-7039-4cdc-80cf-73c1ff9eb4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\"\"\"\n",
        "    Creating the data set for fast weights implementation.\n",
        "    Data will minmic the synthetic dataset created in\n",
        "    https://arxiv.org/abs/1610.06258 Ba et al.\n",
        "    Ex.\n",
        "    c6a7s4??a = 7 (it asking for the value for the key a)\n",
        "    This is a very interesting dataset because\n",
        "    it requires the model to retrieve and use temporary memory\n",
        "    in order to accurately predict the proper value for the key.\n",
        "\"\"\"\n",
        "\n",
        "def get_three_letters():\n",
        "    \"\"\"\n",
        "    Retrieve three random letters (a-z)\n",
        "    without replacement.\n",
        "    \"\"\"\n",
        "    return np.random.choice(range(0,26), 3, replace=False)\n",
        "\n",
        "def get_three_numbers():\n",
        "    \"\"\"\n",
        "    Retrieve three random numbers (0-9)\n",
        "    with replacement.\n",
        "    \"\"\"\n",
        "    return np.random.choice(range(26, 26+10), 3, replace=True)\n",
        "\n",
        "def create_sequence():\n",
        "    \"\"\"\n",
        "    Concatenate keys and values with\n",
        "    ?? and one of the keys.\n",
        "    Returns the input and output.\n",
        "    \"\"\"\n",
        "    letters = get_three_letters()\n",
        "    numbers = get_three_numbers()\n",
        "    X = np.zeros((9))\n",
        "    y = np.zeros((1))\n",
        "    for i in range(0, 5, 2):\n",
        "        X[i] = letters[int(i/2)]\n",
        "        X[i+1] = numbers[int(i/2)]\n",
        "\n",
        "    # append ??\n",
        "    X[6] = 26+10\n",
        "    X[7] = 26+10\n",
        "\n",
        "    # last key and respective value (y)\n",
        "    index = np.random.choice(range(0,3), 1, replace=False)\n",
        "    X[8] = letters[index]\n",
        "    y = numbers[index]\n",
        "\n",
        "    # one hot encode X and y\n",
        "    X_one_hot = np.eye(26+10+1)[np.array(X).astype('int')]\n",
        "    y_one_hot = np.eye(26+10+1)[y][0]\n",
        "\n",
        "    return X_one_hot, y_one_hot\n",
        "\n",
        "def ordinal_to_alpha(sequence):\n",
        "    \"\"\"\n",
        "    Convert from ordinal to alpha-numeric representations.\n",
        "    Just for funsies :)\n",
        "    \"\"\"\n",
        "    corpus = ['a','b','c','d','e','f','g','h','i','j','k','l',\n",
        "              'm','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, '?']\n",
        "\n",
        "    conversion = \"\"\n",
        "    for item in sequence:\n",
        "        conversion += str(corpus[int(item)])\n",
        "    return conversion\n",
        "\n",
        "def create_data(num_samples):\n",
        "    \"\"\"\n",
        "    Create a num_samples long set of X and y.\n",
        "    \"\"\"\n",
        "    X = np.zeros([num_samples, 9, 26+10+1], dtype=np.int32)\n",
        "    y = np.zeros([num_samples, 26+10+1], dtype=np.int32)\n",
        "    for i in range(num_samples):\n",
        "        X[i], y[i] = create_sequence()\n",
        "    return X, y\n",
        "\n",
        "def generate_epoch(X, y, num_epochs, batch_size):\n",
        "\n",
        "    for epoch_num in range(num_epochs):\n",
        "        yield generate_batch(X, y, batch_size)\n",
        "\n",
        "def generate_batch(X, y, batch_size):\n",
        "\n",
        "    data_size = len(X)\n",
        "\n",
        "    num_batches = (data_size // batch_size)\n",
        "    for batch_num in range(num_batches):\n",
        "        start_index = batch_num * batch_size\n",
        "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "        yield X[start_index:end_index], y[start_index:end_index]\n",
        "\n",
        "# Sampling\n",
        "sample_X, sample_y = create_sequence()\n",
        "print (\"Sample:\", ordinal_to_alpha([np.argmax(X) for X in sample_X]), ordinal_to_alpha([np.argmax(sample_y)]))\n",
        "# Train/valid sets\n",
        "train_X, train_y = create_data(640)\n",
        "print (\"train_X:\", np.shape(train_X), \",train_y:\", np.shape(train_y))\n",
        "valid_X, valid_y = create_data(384)\n",
        "print (\"valid_X:\", np.shape(valid_X), \",valid_y:\", np.shape(valid_y))\n",
        "test_X, test_y = create_data(384)\n",
        "print (\"test_X:\", np.shape(test_X), \",test_y:\", np.shape(test_y))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample: r0j0q7??q 7\n",
            "train_X: (640, 9, 37) ,train_y: (640, 37)\n",
            "valid_X: (384, 9, 37) ,valid_y: (384, 37)\n",
            "test_X: (384, 9, 37) ,test_y: (384, 37)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7-KsQD4O_o8-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = tf.Variable(0.0, trainable = False)\n",
        "optimizer = tf.train.AdamOptimizer(0.1)\n",
        "inputdim = 9\n",
        "elem = 37\n",
        "fw = fastweights(inputdim, elem, 128, 0.9, 0.5, 512)\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=fw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qj1MkIxzP4NG",
        "colab_type": "code",
        "outputId": "e0a3b64b-3f5e-4858-fa4d-96a648e68c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25534
        }
      },
      "cell_type": "code",
      "source": [
        "t_loss_history = []\n",
        "  \n",
        "for train_epoch_num, train_epoch in enumerate(generate_epoch(train_X, train_y, 500, 128)):\n",
        "  print(\"Epoch number :\", train_epoch_num)\n",
        "  for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred_y = fw(batch_X, batch_y, 1)\n",
        "      loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y))\n",
        "        \n",
        "      batch_loss = (loss/128)\n",
        "      if train_batch_num %10 ==0:\n",
        "        print(\"\\tEpoch {:03d}/{:03d}: Loss at step {:02d}: {:.9f}\".format((train_epoch_num+1), 1000, train_batch_num, tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y)) ))\n",
        "  t_loss_history.append(batch_loss.numpy())\n",
        "  variables = fw.variables\n",
        "  grads, norm =tf.clip_by_global_norm(tape.gradient(loss, variables), 0.25) \n",
        "  optimizer.apply_gradients(zip(grads, variables), global_step=tf.train.get_or_create_global_step())\n",
        "  print(\"Epoch {:03d}/{:03d} completed \\t - \\tBatch loss: {:.9f}\".format((train_epoch_num+1), 1000, tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y)) ))\n",
        "#tf.contrib.eager.Saver(variables).save('trained.ckpt')\n",
        "checkpoint.save('trained.ckpt')\n",
        "print(\"Final loss for training  set: {:.9f}\".format(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number : 0\n",
            "\tEpoch 001/1000: Loss at step 00: 3.716274738\n",
            "Epoch 001/1000 completed \t - \tBatch loss: 3.772774696\n",
            "Epoch number : 1\n",
            "\tEpoch 002/1000: Loss at step 00: 8.082309723\n",
            "Epoch 002/1000 completed \t - \tBatch loss: 5.683158398\n",
            "Epoch number : 2\n",
            "\tEpoch 003/1000: Loss at step 00: 14.983819962\n",
            "Epoch 003/1000 completed \t - \tBatch loss: 13.865386009\n",
            "Epoch number : 3\n",
            "\tEpoch 004/1000: Loss at step 00: 17.785694122\n",
            "Epoch 004/1000 completed \t - \tBatch loss: 17.812538147\n",
            "Epoch number : 4\n",
            "\tEpoch 005/1000: Loss at step 00: 20.212806702\n",
            "Epoch 005/1000 completed \t - \tBatch loss: 21.590166092\n",
            "Epoch number : 5\n",
            "\tEpoch 006/1000: Loss at step 00: 20.335016251\n",
            "Epoch 006/1000 completed \t - \tBatch loss: 22.970752716\n",
            "Epoch number : 6\n",
            "\tEpoch 007/1000: Loss at step 00: 15.158548355\n",
            "Epoch 007/1000 completed \t - \tBatch loss: 16.994529724\n",
            "Epoch number : 7\n",
            "\tEpoch 008/1000: Loss at step 00: 15.013202667\n",
            "Epoch 008/1000 completed \t - \tBatch loss: 15.379113197\n",
            "Epoch number : 8\n",
            "\tEpoch 009/1000: Loss at step 00: 13.188171387\n",
            "Epoch 009/1000 completed \t - \tBatch loss: 10.384880066\n",
            "Epoch number : 9\n",
            "\tEpoch 010/1000: Loss at step 00: 15.495711327\n",
            "Epoch 010/1000 completed \t - \tBatch loss: 13.740285873\n",
            "Epoch number : 10\n",
            "\tEpoch 011/1000: Loss at step 00: 15.003635406\n",
            "Epoch 011/1000 completed \t - \tBatch loss: 12.987072945\n",
            "Epoch number : 11\n",
            "\tEpoch 012/1000: Loss at step 00: 9.352424622\n",
            "Epoch 012/1000 completed \t - \tBatch loss: 7.692342758\n",
            "Epoch number : 12\n",
            "\tEpoch 013/1000: Loss at step 00: 7.124816895\n",
            "Epoch 013/1000 completed \t - \tBatch loss: 5.482788086\n",
            "Epoch number : 13\n",
            "\tEpoch 014/1000: Loss at step 00: 5.559887886\n",
            "Epoch 014/1000 completed \t - \tBatch loss: 4.254691124\n",
            "Epoch number : 14\n",
            "\tEpoch 015/1000: Loss at step 00: 5.433651924\n",
            "Epoch 015/1000 completed \t - \tBatch loss: 4.856916904\n",
            "Epoch number : 15\n",
            "\tEpoch 016/1000: Loss at step 00: 5.057005882\n",
            "Epoch 016/1000 completed \t - \tBatch loss: 4.061474800\n",
            "Epoch number : 16\n",
            "\tEpoch 017/1000: Loss at step 00: 4.884413719\n",
            "Epoch 017/1000 completed \t - \tBatch loss: 4.190013409\n",
            "Epoch number : 17\n",
            "\tEpoch 018/1000: Loss at step 00: 4.156168461\n",
            "Epoch 018/1000 completed \t - \tBatch loss: 3.399145126\n",
            "Epoch number : 18\n",
            "\tEpoch 019/1000: Loss at step 00: 3.015132427\n",
            "Epoch 019/1000 completed \t - \tBatch loss: 2.529293060\n",
            "Epoch number : 19\n",
            "\tEpoch 020/1000: Loss at step 00: 2.746209860\n",
            "Epoch 020/1000 completed \t - \tBatch loss: 2.496176243\n",
            "Epoch number : 20\n",
            "\tEpoch 021/1000: Loss at step 00: 2.468841076\n",
            "Epoch 021/1000 completed \t - \tBatch loss: 2.421386719\n",
            "Epoch number : 21\n",
            "\tEpoch 022/1000: Loss at step 00: 2.361498117\n",
            "Epoch 022/1000 completed \t - \tBatch loss: 2.472390175\n",
            "Epoch number : 22\n",
            "\tEpoch 023/1000: Loss at step 00: 2.318558693\n",
            "Epoch 023/1000 completed \t - \tBatch loss: 2.284496784\n",
            "Epoch number : 23\n",
            "\tEpoch 024/1000: Loss at step 00: 2.407670498\n",
            "Epoch 024/1000 completed \t - \tBatch loss: 2.198276997\n",
            "Epoch number : 24\n",
            "\tEpoch 025/1000: Loss at step 00: 2.454359055\n",
            "Epoch 025/1000 completed \t - \tBatch loss: 2.203491211\n",
            "Epoch number : 25\n",
            "\tEpoch 026/1000: Loss at step 00: 2.437329292\n",
            "Epoch 026/1000 completed \t - \tBatch loss: 2.168372869\n",
            "Epoch number : 26\n",
            "\tEpoch 027/1000: Loss at step 00: 2.404025078\n",
            "Epoch 027/1000 completed \t - \tBatch loss: 2.920570850\n",
            "Epoch number : 27\n",
            "\tEpoch 028/1000: Loss at step 00: 2.820101261\n",
            "Epoch 028/1000 completed \t - \tBatch loss: 2.531486988\n",
            "Epoch number : 28\n",
            "\tEpoch 029/1000: Loss at step 00: 2.457830667\n",
            "Epoch 029/1000 completed \t - \tBatch loss: 2.652581215\n",
            "Epoch number : 29\n",
            "\tEpoch 030/1000: Loss at step 00: 2.447708607\n",
            "Epoch 030/1000 completed \t - \tBatch loss: 2.438871861\n",
            "Epoch number : 30\n",
            "\tEpoch 031/1000: Loss at step 00: 2.498101234\n",
            "Epoch 031/1000 completed \t - \tBatch loss: 2.337438583\n",
            "Epoch number : 31\n",
            "\tEpoch 032/1000: Loss at step 00: 2.489426136\n",
            "Epoch 032/1000 completed \t - \tBatch loss: 2.193808079\n",
            "Epoch number : 32\n",
            "\tEpoch 033/1000: Loss at step 00: 2.415950775\n",
            "Epoch 033/1000 completed \t - \tBatch loss: 2.110115290\n",
            "Epoch number : 33\n",
            "\tEpoch 034/1000: Loss at step 00: 2.376910925\n",
            "Epoch 034/1000 completed \t - \tBatch loss: 2.027723551\n",
            "Epoch number : 34\n",
            "\tEpoch 035/1000: Loss at step 00: 2.363310337\n",
            "Epoch 035/1000 completed \t - \tBatch loss: 1.986825466\n",
            "Epoch number : 35\n",
            "\tEpoch 036/1000: Loss at step 00: 2.293190479\n",
            "Epoch 036/1000 completed \t - \tBatch loss: 1.897855639\n",
            "Epoch number : 36\n",
            "\tEpoch 037/1000: Loss at step 00: 2.392192364\n",
            "Epoch 037/1000 completed \t - \tBatch loss: 1.941064000\n",
            "Epoch number : 37\n",
            "\tEpoch 038/1000: Loss at step 00: 2.406196594\n",
            "Epoch 038/1000 completed \t - \tBatch loss: 1.882872820\n",
            "Epoch number : 38\n",
            "\tEpoch 039/1000: Loss at step 00: 2.416784286\n",
            "Epoch 039/1000 completed \t - \tBatch loss: 1.910960555\n",
            "Epoch number : 39\n",
            "\tEpoch 040/1000: Loss at step 00: 2.408177376\n",
            "Epoch 040/1000 completed \t - \tBatch loss: 1.870756507\n",
            "Epoch number : 40\n",
            "\tEpoch 041/1000: Loss at step 00: 2.365799904\n",
            "Epoch 041/1000 completed \t - \tBatch loss: 1.789888144\n",
            "Epoch number : 41\n",
            "\tEpoch 042/1000: Loss at step 00: 2.396687508\n",
            "Epoch 042/1000 completed \t - \tBatch loss: 2.056657553\n",
            "Epoch number : 42\n",
            "\tEpoch 043/1000: Loss at step 00: 2.577424765\n",
            "Epoch 043/1000 completed \t - \tBatch loss: 1.696108818\n",
            "Epoch number : 43\n",
            "\tEpoch 044/1000: Loss at step 00: 2.446439505\n",
            "Epoch 044/1000 completed \t - \tBatch loss: 1.832831025\n",
            "Epoch number : 44\n",
            "\tEpoch 045/1000: Loss at step 00: 2.665604591\n",
            "Epoch 045/1000 completed \t - \tBatch loss: 1.767463803\n",
            "Epoch number : 45\n",
            "\tEpoch 046/1000: Loss at step 00: 2.624627352\n",
            "Epoch 046/1000 completed \t - \tBatch loss: 1.713161469\n",
            "Epoch number : 46\n",
            "\tEpoch 047/1000: Loss at step 00: 2.661766052\n",
            "Epoch 047/1000 completed \t - \tBatch loss: 1.729394674\n",
            "Epoch number : 47\n",
            "\tEpoch 048/1000: Loss at step 00: 2.490566492\n",
            "Epoch 048/1000 completed \t - \tBatch loss: 1.554214239\n",
            "Epoch number : 48\n",
            "\tEpoch 049/1000: Loss at step 00: 2.599856377\n",
            "Epoch 049/1000 completed \t - \tBatch loss: 1.773389697\n",
            "Epoch number : 49\n",
            "\tEpoch 050/1000: Loss at step 00: 2.806606293\n",
            "Epoch 050/1000 completed \t - \tBatch loss: 1.522703290\n",
            "Epoch number : 50\n",
            "\tEpoch 051/1000: Loss at step 00: 2.456377983\n",
            "Epoch 051/1000 completed \t - \tBatch loss: 1.596550465\n",
            "Epoch number : 51\n",
            "\tEpoch 052/1000: Loss at step 00: 2.520135880\n",
            "Epoch 052/1000 completed \t - \tBatch loss: 1.522187233\n",
            "Epoch number : 52\n",
            "\tEpoch 053/1000: Loss at step 00: 2.569244385\n",
            "Epoch 053/1000 completed \t - \tBatch loss: 1.452356339\n",
            "Epoch number : 53\n",
            "\tEpoch 054/1000: Loss at step 00: 2.499861002\n",
            "Epoch 054/1000 completed \t - \tBatch loss: 1.322197914\n",
            "Epoch number : 54\n",
            "\tEpoch 055/1000: Loss at step 00: 2.430952549\n",
            "Epoch 055/1000 completed \t - \tBatch loss: 1.315499187\n",
            "Epoch number : 55\n",
            "\tEpoch 056/1000: Loss at step 00: 2.471433878\n",
            "Epoch 056/1000 completed \t - \tBatch loss: 1.293161392\n",
            "Epoch number : 56\n",
            "\tEpoch 057/1000: Loss at step 00: 2.621178627\n",
            "Epoch 057/1000 completed \t - \tBatch loss: 1.285224676\n",
            "Epoch number : 57\n",
            "\tEpoch 058/1000: Loss at step 00: 2.561016083\n",
            "Epoch 058/1000 completed \t - \tBatch loss: 1.200668693\n",
            "Epoch number : 58\n",
            "\tEpoch 059/1000: Loss at step 00: 2.444562435\n",
            "Epoch 059/1000 completed \t - \tBatch loss: 1.206140757\n",
            "Epoch number : 59\n",
            "\tEpoch 060/1000: Loss at step 00: 2.458750486\n",
            "Epoch 060/1000 completed \t - \tBatch loss: 1.217438459\n",
            "Epoch number : 60\n",
            "\tEpoch 061/1000: Loss at step 00: 2.665466070\n",
            "Epoch 061/1000 completed \t - \tBatch loss: 1.078758359\n",
            "Epoch number : 61\n",
            "\tEpoch 062/1000: Loss at step 00: 2.523901939\n",
            "Epoch 062/1000 completed \t - \tBatch loss: 1.418743134\n",
            "Epoch number : 62\n",
            "\tEpoch 063/1000: Loss at step 00: 2.673388481\n",
            "Epoch 063/1000 completed \t - \tBatch loss: 1.241014957\n",
            "Epoch number : 63\n",
            "\tEpoch 064/1000: Loss at step 00: 3.148201466\n",
            "Epoch 064/1000 completed \t - \tBatch loss: 1.505825162\n",
            "Epoch number : 64\n",
            "\tEpoch 065/1000: Loss at step 00: 3.658299446\n",
            "Epoch 065/1000 completed \t - \tBatch loss: 1.348300457\n",
            "Epoch number : 65\n",
            "\tEpoch 066/1000: Loss at step 00: 2.981530666\n",
            "Epoch 066/1000 completed \t - \tBatch loss: 1.209960222\n",
            "Epoch number : 66\n",
            "\tEpoch 067/1000: Loss at step 00: 2.699247360\n",
            "Epoch 067/1000 completed \t - \tBatch loss: 1.131021976\n",
            "Epoch number : 67\n",
            "\tEpoch 068/1000: Loss at step 00: 2.549112558\n",
            "Epoch 068/1000 completed \t - \tBatch loss: 0.858899593\n",
            "Epoch number : 68\n",
            "\tEpoch 069/1000: Loss at step 00: 3.014181137\n",
            "Epoch 069/1000 completed \t - \tBatch loss: 1.018770218\n",
            "Epoch number : 69\n",
            "\tEpoch 070/1000: Loss at step 00: 3.165418386\n",
            "Epoch 070/1000 completed \t - \tBatch loss: 0.927348614\n",
            "Epoch number : 70\n",
            "\tEpoch 071/1000: Loss at step 00: 2.917403936\n",
            "Epoch 071/1000 completed \t - \tBatch loss: 0.830514789\n",
            "Epoch number : 71\n",
            "\tEpoch 072/1000: Loss at step 00: 2.727025986\n",
            "Epoch 072/1000 completed \t - \tBatch loss: 0.873071671\n",
            "Epoch number : 72\n",
            "\tEpoch 073/1000: Loss at step 00: 2.839226961\n",
            "Epoch 073/1000 completed \t - \tBatch loss: 0.902520239\n",
            "Epoch number : 73\n",
            "\tEpoch 074/1000: Loss at step 00: 3.093532324\n",
            "Epoch 074/1000 completed \t - \tBatch loss: 0.829730570\n",
            "Epoch number : 74\n",
            "\tEpoch 075/1000: Loss at step 00: 2.849030256\n",
            "Epoch 075/1000 completed \t - \tBatch loss: 0.693208456\n",
            "Epoch number : 75\n",
            "\tEpoch 076/1000: Loss at step 00: 3.051694393\n",
            "Epoch 076/1000 completed \t - \tBatch loss: 0.877760231\n",
            "Epoch number : 76\n",
            "\tEpoch 077/1000: Loss at step 00: 3.272862911\n",
            "Epoch 077/1000 completed \t - \tBatch loss: 0.925939560\n",
            "Epoch number : 77\n",
            "\tEpoch 078/1000: Loss at step 00: 2.896533966\n",
            "Epoch 078/1000 completed \t - \tBatch loss: 0.869288564\n",
            "Epoch number : 78\n",
            "\tEpoch 079/1000: Loss at step 00: 2.912785053\n",
            "Epoch 079/1000 completed \t - \tBatch loss: 0.859468818\n",
            "Epoch number : 79\n",
            "\tEpoch 080/1000: Loss at step 00: 3.110299587\n",
            "Epoch 080/1000 completed \t - \tBatch loss: 0.816042304\n",
            "Epoch number : 80\n",
            "\tEpoch 081/1000: Loss at step 00: 2.985338926\n",
            "Epoch 081/1000 completed \t - \tBatch loss: 0.739950478\n",
            "Epoch number : 81\n",
            "\tEpoch 082/1000: Loss at step 00: 3.090569019\n",
            "Epoch 082/1000 completed \t - \tBatch loss: 0.825104356\n",
            "Epoch number : 82\n",
            "\tEpoch 083/1000: Loss at step 00: 3.182167053\n",
            "Epoch 083/1000 completed \t - \tBatch loss: 0.715741813\n",
            "Epoch number : 83\n",
            "\tEpoch 084/1000: Loss at step 00: 3.270842314\n",
            "Epoch 084/1000 completed \t - \tBatch loss: 0.805952132\n",
            "Epoch number : 84\n",
            "\tEpoch 085/1000: Loss at step 00: 3.115949392\n",
            "Epoch 085/1000 completed \t - \tBatch loss: 0.631423831\n",
            "Epoch number : 85\n",
            "\tEpoch 086/1000: Loss at step 00: 3.082547188\n",
            "Epoch 086/1000 completed \t - \tBatch loss: 0.604530752\n",
            "Epoch number : 86\n",
            "\tEpoch 087/1000: Loss at step 00: 3.398842335\n",
            "Epoch 087/1000 completed \t - \tBatch loss: 0.699805975\n",
            "Epoch number : 87\n",
            "\tEpoch 088/1000: Loss at step 00: 3.285013914\n",
            "Epoch 088/1000 completed \t - \tBatch loss: 0.523271561\n",
            "Epoch number : 88\n",
            "\tEpoch 089/1000: Loss at step 00: 2.992788792\n",
            "Epoch 089/1000 completed \t - \tBatch loss: 0.645361662\n",
            "Epoch number : 89\n",
            "\tEpoch 090/1000: Loss at step 00: 2.975896358\n",
            "Epoch 090/1000 completed \t - \tBatch loss: 0.557720423\n",
            "Epoch number : 90\n",
            "\tEpoch 091/1000: Loss at step 00: 3.305974960\n",
            "Epoch 091/1000 completed \t - \tBatch loss: 0.775480032\n",
            "Epoch number : 91\n",
            "\tEpoch 092/1000: Loss at step 00: 3.741701365\n",
            "Epoch 092/1000 completed \t - \tBatch loss: 0.694548368\n",
            "Epoch number : 92\n",
            "\tEpoch 093/1000: Loss at step 00: 3.154683352\n",
            "Epoch 093/1000 completed \t - \tBatch loss: 0.491086602\n",
            "Epoch number : 93\n",
            "\tEpoch 094/1000: Loss at step 00: 2.912132263\n",
            "Epoch 094/1000 completed \t - \tBatch loss: 0.521289229\n",
            "Epoch number : 94\n",
            "\tEpoch 095/1000: Loss at step 00: 2.893110752\n",
            "Epoch 095/1000 completed \t - \tBatch loss: 0.408035874\n",
            "Epoch number : 95\n",
            "\tEpoch 096/1000: Loss at step 00: 3.285435677\n",
            "Epoch 096/1000 completed \t - \tBatch loss: 0.454714358\n",
            "Epoch number : 96\n",
            "\tEpoch 097/1000: Loss at step 00: 3.383714676\n",
            "Epoch 097/1000 completed \t - \tBatch loss: 0.367561847\n",
            "Epoch number : 97\n",
            "\tEpoch 098/1000: Loss at step 00: 3.057942867\n",
            "Epoch 098/1000 completed \t - \tBatch loss: 0.489501983\n",
            "Epoch number : 98\n",
            "\tEpoch 099/1000: Loss at step 00: 2.967218637\n",
            "Epoch 099/1000 completed \t - \tBatch loss: 0.433420181\n",
            "Epoch number : 99\n",
            "\tEpoch 100/1000: Loss at step 00: 3.144975901\n",
            "Epoch 100/1000 completed \t - \tBatch loss: 0.382161379\n",
            "Epoch number : 100\n",
            "\tEpoch 101/1000: Loss at step 00: 3.546304941\n",
            "Epoch 101/1000 completed \t - \tBatch loss: 0.418219179\n",
            "Epoch number : 101\n",
            "\tEpoch 102/1000: Loss at step 00: 3.488743544\n",
            "Epoch 102/1000 completed \t - \tBatch loss: 0.316445500\n",
            "Epoch number : 102\n",
            "\tEpoch 103/1000: Loss at step 00: 3.336156368\n",
            "Epoch 103/1000 completed \t - \tBatch loss: 0.423961222\n",
            "Epoch number : 103\n",
            "\tEpoch 104/1000: Loss at step 00: 3.342611074\n",
            "Epoch 104/1000 completed \t - \tBatch loss: 0.399998486\n",
            "Epoch number : 104\n",
            "\tEpoch 105/1000: Loss at step 00: 3.355337620\n",
            "Epoch 105/1000 completed \t - \tBatch loss: 0.526668429\n",
            "Epoch number : 105\n",
            "\tEpoch 106/1000: Loss at step 00: 3.405715704\n",
            "Epoch 106/1000 completed \t - \tBatch loss: 0.422610164\n",
            "Epoch number : 106\n",
            "\tEpoch 107/1000: Loss at step 00: 3.560378313\n",
            "Epoch 107/1000 completed \t - \tBatch loss: 0.408899307\n",
            "Epoch number : 107\n",
            "\tEpoch 108/1000: Loss at step 00: 3.988806725\n",
            "Epoch 108/1000 completed \t - \tBatch loss: 0.588626742\n",
            "Epoch number : 108\n",
            "\tEpoch 109/1000: Loss at step 00: 4.047773838\n",
            "Epoch 109/1000 completed \t - \tBatch loss: 0.487447441\n",
            "Epoch number : 109\n",
            "\tEpoch 110/1000: Loss at step 00: 3.638408184\n",
            "Epoch 110/1000 completed \t - \tBatch loss: 0.358853310\n",
            "Epoch number : 110\n",
            "\tEpoch 111/1000: Loss at step 00: 3.795837402\n",
            "Epoch 111/1000 completed \t - \tBatch loss: 0.456615776\n",
            "Epoch number : 111\n",
            "\tEpoch 112/1000: Loss at step 00: 4.155593395\n",
            "Epoch 112/1000 completed \t - \tBatch loss: 0.621050358\n",
            "Epoch number : 112\n",
            "\tEpoch 113/1000: Loss at step 00: 4.224652290\n",
            "Epoch 113/1000 completed \t - \tBatch loss: 0.454152882\n",
            "Epoch number : 113\n",
            "\tEpoch 114/1000: Loss at step 00: 3.809361696\n",
            "Epoch 114/1000 completed \t - \tBatch loss: 0.204631895\n",
            "Epoch number : 114\n",
            "\tEpoch 115/1000: Loss at step 00: 3.729438305\n",
            "Epoch 115/1000 completed \t - \tBatch loss: 0.455397099\n",
            "Epoch number : 115\n",
            "\tEpoch 116/1000: Loss at step 00: 3.854135513\n",
            "Epoch 116/1000 completed \t - \tBatch loss: 0.393745542\n",
            "Epoch number : 116\n",
            "\tEpoch 117/1000: Loss at step 00: 4.031594753\n",
            "Epoch 117/1000 completed \t - \tBatch loss: 0.273647577\n",
            "Epoch number : 117\n",
            "\tEpoch 118/1000: Loss at step 00: 4.315917015\n",
            "Epoch 118/1000 completed \t - \tBatch loss: 0.359775424\n",
            "Epoch number : 118\n",
            "\tEpoch 119/1000: Loss at step 00: 4.413897991\n",
            "Epoch 119/1000 completed \t - \tBatch loss: 0.498492777\n",
            "Epoch number : 119\n",
            "\tEpoch 120/1000: Loss at step 00: 3.966455936\n",
            "Epoch 120/1000 completed \t - \tBatch loss: 0.434366822\n",
            "Epoch number : 120\n",
            "\tEpoch 121/1000: Loss at step 00: 3.566593170\n",
            "Epoch 121/1000 completed \t - \tBatch loss: 0.294786334\n",
            "Epoch number : 121\n",
            "\tEpoch 122/1000: Loss at step 00: 3.737190247\n",
            "Epoch 122/1000 completed \t - \tBatch loss: 0.330374956\n",
            "Epoch number : 122\n",
            "\tEpoch 123/1000: Loss at step 00: 4.337085247\n",
            "Epoch 123/1000 completed \t - \tBatch loss: 0.529080749\n",
            "Epoch number : 123\n",
            "\tEpoch 124/1000: Loss at step 00: 4.637328148\n",
            "Epoch 124/1000 completed \t - \tBatch loss: 0.450446367\n",
            "Epoch number : 124\n",
            "\tEpoch 125/1000: Loss at step 00: 4.344506264\n",
            "Epoch 125/1000 completed \t - \tBatch loss: 0.292174190\n",
            "Epoch number : 125\n",
            "\tEpoch 126/1000: Loss at step 00: 4.066549301\n",
            "Epoch 126/1000 completed \t - \tBatch loss: 0.328665674\n",
            "Epoch number : 126\n",
            "\tEpoch 127/1000: Loss at step 00: 3.866767645\n",
            "Epoch 127/1000 completed \t - \tBatch loss: 0.403123289\n",
            "Epoch number : 127\n",
            "\tEpoch 128/1000: Loss at step 00: 3.837688446\n",
            "Epoch 128/1000 completed \t - \tBatch loss: 0.551215708\n",
            "Epoch number : 128\n",
            "\tEpoch 129/1000: Loss at step 00: 4.063995361\n",
            "Epoch 129/1000 completed \t - \tBatch loss: 0.308069915\n",
            "Epoch number : 129\n",
            "\tEpoch 130/1000: Loss at step 00: 4.308660030\n",
            "Epoch 130/1000 completed \t - \tBatch loss: 0.242509693\n",
            "Epoch number : 130\n",
            "\tEpoch 131/1000: Loss at step 00: 4.510888100\n",
            "Epoch 131/1000 completed \t - \tBatch loss: 0.274961650\n",
            "Epoch number : 131\n",
            "\tEpoch 132/1000: Loss at step 00: 4.444036484\n",
            "Epoch 132/1000 completed \t - \tBatch loss: 0.278793752\n",
            "Epoch number : 132\n",
            "\tEpoch 133/1000: Loss at step 00: 4.217350960\n",
            "Epoch 133/1000 completed \t - \tBatch loss: 0.231127009\n",
            "Epoch number : 133\n",
            "\tEpoch 134/1000: Loss at step 00: 4.059812069\n",
            "Epoch 134/1000 completed \t - \tBatch loss: 0.141808569\n",
            "Epoch number : 134\n",
            "\tEpoch 135/1000: Loss at step 00: 3.891389608\n",
            "Epoch 135/1000 completed \t - \tBatch loss: 0.158288702\n",
            "Epoch number : 135\n",
            "\tEpoch 136/1000: Loss at step 00: 4.152716637\n",
            "Epoch 136/1000 completed \t - \tBatch loss: 0.139063880\n",
            "Epoch number : 136\n",
            "\tEpoch 137/1000: Loss at step 00: 4.479827404\n",
            "Epoch 137/1000 completed \t - \tBatch loss: 0.130482584\n",
            "Epoch number : 137\n",
            "\tEpoch 138/1000: Loss at step 00: 4.650260925\n",
            "Epoch 138/1000 completed \t - \tBatch loss: 0.116614923\n",
            "Epoch number : 138\n",
            "\tEpoch 139/1000: Loss at step 00: 4.672231197\n",
            "Epoch 139/1000 completed \t - \tBatch loss: 0.162220269\n",
            "Epoch number : 139\n",
            "\tEpoch 140/1000: Loss at step 00: 4.473396301\n",
            "Epoch 140/1000 completed \t - \tBatch loss: 0.113213405\n",
            "Epoch number : 140\n",
            "\tEpoch 141/1000: Loss at step 00: 4.237387657\n",
            "Epoch 141/1000 completed \t - \tBatch loss: 0.114640698\n",
            "Epoch number : 141\n",
            "\tEpoch 142/1000: Loss at step 00: 4.158390999\n",
            "Epoch 142/1000 completed \t - \tBatch loss: 0.115890391\n",
            "Epoch number : 142\n",
            "\tEpoch 143/1000: Loss at step 00: 4.257087708\n",
            "Epoch 143/1000 completed \t - \tBatch loss: 0.131161138\n",
            "Epoch number : 143\n",
            "\tEpoch 144/1000: Loss at step 00: 4.489277363\n",
            "Epoch 144/1000 completed \t - \tBatch loss: 0.139854804\n",
            "Epoch number : 144\n",
            "\tEpoch 145/1000: Loss at step 00: 4.580624580\n",
            "Epoch 145/1000 completed \t - \tBatch loss: 0.099045545\n",
            "Epoch number : 145\n",
            "\tEpoch 146/1000: Loss at step 00: 4.676056862\n",
            "Epoch 146/1000 completed \t - \tBatch loss: 0.175489649\n",
            "Epoch number : 146\n",
            "\tEpoch 147/1000: Loss at step 00: 4.695480347\n",
            "Epoch 147/1000 completed \t - \tBatch loss: 0.271357656\n",
            "Epoch number : 147\n",
            "\tEpoch 148/1000: Loss at step 00: 4.535199165\n",
            "Epoch 148/1000 completed \t - \tBatch loss: 0.162173256\n",
            "Epoch number : 148\n",
            "\tEpoch 149/1000: Loss at step 00: 4.424116611\n",
            "Epoch 149/1000 completed \t - \tBatch loss: 0.148373783\n",
            "Epoch number : 149\n",
            "\tEpoch 150/1000: Loss at step 00: 4.686961651\n",
            "Epoch 150/1000 completed \t - \tBatch loss: 0.212885037\n",
            "Epoch number : 150\n",
            "\tEpoch 151/1000: Loss at step 00: 4.739541531\n",
            "Epoch 151/1000 completed \t - \tBatch loss: 0.263836890\n",
            "Epoch number : 151\n",
            "\tEpoch 152/1000: Loss at step 00: 4.660276890\n",
            "Epoch 152/1000 completed \t - \tBatch loss: 0.327593565\n",
            "Epoch number : 152\n",
            "\tEpoch 153/1000: Loss at step 00: 4.561709404\n",
            "Epoch 153/1000 completed \t - \tBatch loss: 0.217213094\n",
            "Epoch number : 153\n",
            "\tEpoch 154/1000: Loss at step 00: 4.624224186\n",
            "Epoch 154/1000 completed \t - \tBatch loss: 0.185993150\n",
            "Epoch number : 154\n",
            "\tEpoch 155/1000: Loss at step 00: 5.046612740\n",
            "Epoch 155/1000 completed \t - \tBatch loss: 0.273009837\n",
            "Epoch number : 155\n",
            "\tEpoch 156/1000: Loss at step 00: 5.459127426\n",
            "Epoch 156/1000 completed \t - \tBatch loss: 0.268918425\n",
            "Epoch number : 156\n",
            "\tEpoch 157/1000: Loss at step 00: 5.321913719\n",
            "Epoch 157/1000 completed \t - \tBatch loss: 0.204469144\n",
            "Epoch number : 157\n",
            "\tEpoch 158/1000: Loss at step 00: 5.170247555\n",
            "Epoch 158/1000 completed \t - \tBatch loss: 0.193847224\n",
            "Epoch number : 158\n",
            "\tEpoch 159/1000: Loss at step 00: 5.174994946\n",
            "Epoch 159/1000 completed \t - \tBatch loss: 0.170166656\n",
            "Epoch number : 159\n",
            "\tEpoch 160/1000: Loss at step 00: 5.220805168\n",
            "Epoch 160/1000 completed \t - \tBatch loss: 0.146973670\n",
            "Epoch number : 160\n",
            "\tEpoch 161/1000: Loss at step 00: 5.184938908\n",
            "Epoch 161/1000 completed \t - \tBatch loss: 0.183491617\n",
            "Epoch number : 161\n",
            "\tEpoch 162/1000: Loss at step 00: 5.084406376\n",
            "Epoch 162/1000 completed \t - \tBatch loss: 0.179419950\n",
            "Epoch number : 162\n",
            "\tEpoch 163/1000: Loss at step 00: 5.001439095\n",
            "Epoch 163/1000 completed \t - \tBatch loss: 0.167346656\n",
            "Epoch number : 163\n",
            "\tEpoch 164/1000: Loss at step 00: 4.999636650\n",
            "Epoch 164/1000 completed \t - \tBatch loss: 0.159808964\n",
            "Epoch number : 164\n",
            "\tEpoch 165/1000: Loss at step 00: 5.106899738\n",
            "Epoch 165/1000 completed \t - \tBatch loss: 0.221402794\n",
            "Epoch number : 165\n",
            "\tEpoch 166/1000: Loss at step 00: 5.384190559\n",
            "Epoch 166/1000 completed \t - \tBatch loss: 0.300119162\n",
            "Epoch number : 166\n",
            "\tEpoch 167/1000: Loss at step 00: 5.705638409\n",
            "Epoch 167/1000 completed \t - \tBatch loss: 0.307900071\n",
            "Epoch number : 167\n",
            "\tEpoch 168/1000: Loss at step 00: 5.657226562\n",
            "Epoch 168/1000 completed \t - \tBatch loss: 0.188936561\n",
            "Epoch number : 168\n",
            "\tEpoch 169/1000: Loss at step 00: 5.480566978\n",
            "Epoch 169/1000 completed \t - \tBatch loss: 0.168541744\n",
            "Epoch number : 169\n",
            "\tEpoch 170/1000: Loss at step 00: 5.574097633\n",
            "Epoch 170/1000 completed \t - \tBatch loss: 0.296324670\n",
            "Epoch number : 170\n",
            "\tEpoch 171/1000: Loss at step 00: 5.658623695\n",
            "Epoch 171/1000 completed \t - \tBatch loss: 0.297173202\n",
            "Epoch number : 171\n",
            "\tEpoch 172/1000: Loss at step 00: 5.449944019\n",
            "Epoch 172/1000 completed \t - \tBatch loss: 0.236438259\n",
            "Epoch number : 172\n",
            "\tEpoch 173/1000: Loss at step 00: 5.267286777\n",
            "Epoch 173/1000 completed \t - \tBatch loss: 0.166720450\n",
            "Epoch number : 173\n",
            "\tEpoch 174/1000: Loss at step 00: 5.171166897\n",
            "Epoch 174/1000 completed \t - \tBatch loss: 0.127890632\n",
            "Epoch number : 174\n",
            "\tEpoch 175/1000: Loss at step 00: 5.198005676\n",
            "Epoch 175/1000 completed \t - \tBatch loss: 0.131190315\n",
            "Epoch number : 175\n",
            "\tEpoch 176/1000: Loss at step 00: 5.341404915\n",
            "Epoch 176/1000 completed \t - \tBatch loss: 0.119660676\n",
            "Epoch number : 176\n",
            "\tEpoch 177/1000: Loss at step 00: 5.449591637\n",
            "Epoch 177/1000 completed \t - \tBatch loss: 0.146841317\n",
            "Epoch number : 177\n",
            "\tEpoch 178/1000: Loss at step 00: 5.384355545\n",
            "Epoch 178/1000 completed \t - \tBatch loss: 0.132937625\n",
            "Epoch number : 178\n",
            "\tEpoch 179/1000: Loss at step 00: 5.401131630\n",
            "Epoch 179/1000 completed \t - \tBatch loss: 0.081578366\n",
            "Epoch number : 179\n",
            "\tEpoch 180/1000: Loss at step 00: 5.676460743\n",
            "Epoch 180/1000 completed \t - \tBatch loss: 0.060196888\n",
            "Epoch number : 180\n",
            "\tEpoch 181/1000: Loss at step 00: 6.134598732\n",
            "Epoch 181/1000 completed \t - \tBatch loss: 0.085651197\n",
            "Epoch number : 181\n",
            "\tEpoch 182/1000: Loss at step 00: 6.415780067\n",
            "Epoch 182/1000 completed \t - \tBatch loss: 0.108464509\n",
            "Epoch number : 182\n",
            "\tEpoch 183/1000: Loss at step 00: 6.240407944\n",
            "Epoch 183/1000 completed \t - \tBatch loss: 0.075411484\n",
            "Epoch number : 183\n",
            "\tEpoch 184/1000: Loss at step 00: 5.979362488\n",
            "Epoch 184/1000 completed \t - \tBatch loss: 0.055781450\n",
            "Epoch number : 184\n",
            "\tEpoch 185/1000: Loss at step 00: 5.847694874\n",
            "Epoch 185/1000 completed \t - \tBatch loss: 0.040493779\n",
            "Epoch number : 185\n",
            "\tEpoch 186/1000: Loss at step 00: 5.782938957\n",
            "Epoch 186/1000 completed \t - \tBatch loss: 0.037266809\n",
            "Epoch number : 186\n",
            "\tEpoch 187/1000: Loss at step 00: 5.807186127\n",
            "Epoch 187/1000 completed \t - \tBatch loss: 0.031179342\n",
            "Epoch number : 187\n",
            "\tEpoch 188/1000: Loss at step 00: 5.793709755\n",
            "Epoch 188/1000 completed \t - \tBatch loss: 0.024072457\n",
            "Epoch number : 188\n",
            "\tEpoch 189/1000: Loss at step 00: 5.730043888\n",
            "Epoch 189/1000 completed \t - \tBatch loss: 0.028364491\n",
            "Epoch number : 189\n",
            "\tEpoch 190/1000: Loss at step 00: 5.681213379\n",
            "Epoch 190/1000 completed \t - \tBatch loss: 0.032052469\n",
            "Epoch number : 190\n",
            "\tEpoch 191/1000: Loss at step 00: 5.683926582\n",
            "Epoch 191/1000 completed \t - \tBatch loss: 0.026305761\n",
            "Epoch number : 191\n",
            "\tEpoch 192/1000: Loss at step 00: 5.759682655\n",
            "Epoch 192/1000 completed \t - \tBatch loss: 0.019154733\n",
            "Epoch number : 192\n",
            "\tEpoch 193/1000: Loss at step 00: 5.957383633\n",
            "Epoch 193/1000 completed \t - \tBatch loss: 0.021312214\n",
            "Epoch number : 193\n",
            "\tEpoch 194/1000: Loss at step 00: 6.250390053\n",
            "Epoch 194/1000 completed \t - \tBatch loss: 0.030003054\n",
            "Epoch number : 194\n",
            "\tEpoch 195/1000: Loss at step 00: 6.324148178\n",
            "Epoch 195/1000 completed \t - \tBatch loss: 0.024468020\n",
            "Epoch number : 195\n",
            "\tEpoch 196/1000: Loss at step 00: 6.181180000\n",
            "Epoch 196/1000 completed \t - \tBatch loss: 0.020033583\n",
            "Epoch number : 196\n",
            "\tEpoch 197/1000: Loss at step 00: 6.031223297\n",
            "Epoch 197/1000 completed \t - \tBatch loss: 0.020238589\n",
            "Epoch number : 197\n",
            "\tEpoch 198/1000: Loss at step 00: 5.955421448\n",
            "Epoch 198/1000 completed \t - \tBatch loss: 0.017629687\n",
            "Epoch number : 198\n",
            "\tEpoch 199/1000: Loss at step 00: 5.952980042\n",
            "Epoch 199/1000 completed \t - \tBatch loss: 0.017705232\n",
            "Epoch number : 199\n",
            "\tEpoch 200/1000: Loss at step 00: 6.029068947\n",
            "Epoch 200/1000 completed \t - \tBatch loss: 0.017374165\n",
            "Epoch number : 200\n",
            "\tEpoch 201/1000: Loss at step 00: 6.137944221\n",
            "Epoch 201/1000 completed \t - \tBatch loss: 0.017711287\n",
            "Epoch number : 201\n",
            "\tEpoch 202/1000: Loss at step 00: 6.211919785\n",
            "Epoch 202/1000 completed \t - \tBatch loss: 0.015017888\n",
            "Epoch number : 202\n",
            "\tEpoch 203/1000: Loss at step 00: 6.229419708\n",
            "Epoch 203/1000 completed \t - \tBatch loss: 0.011719627\n",
            "Epoch number : 203\n",
            "\tEpoch 204/1000: Loss at step 00: 6.189297676\n",
            "Epoch 204/1000 completed \t - \tBatch loss: 0.011940823\n",
            "Epoch number : 204\n",
            "\tEpoch 205/1000: Loss at step 00: 6.103385448\n",
            "Epoch 205/1000 completed \t - \tBatch loss: 0.014703029\n",
            "Epoch number : 205\n",
            "\tEpoch 206/1000: Loss at step 00: 5.997450352\n",
            "Epoch 206/1000 completed \t - \tBatch loss: 0.016083930\n",
            "Epoch number : 206\n",
            "\tEpoch 207/1000: Loss at step 00: 5.900367260\n",
            "Epoch 207/1000 completed \t - \tBatch loss: 0.012853394\n",
            "Epoch number : 207\n",
            "\tEpoch 208/1000: Loss at step 00: 5.846866608\n",
            "Epoch 208/1000 completed \t - \tBatch loss: 0.010788821\n",
            "Epoch number : 208\n",
            "\tEpoch 209/1000: Loss at step 00: 5.850958824\n",
            "Epoch 209/1000 completed \t - \tBatch loss: 0.013321780\n",
            "Epoch number : 209\n",
            "\tEpoch 210/1000: Loss at step 00: 5.894087791\n",
            "Epoch 210/1000 completed \t - \tBatch loss: 0.017189533\n",
            "Epoch number : 210\n",
            "\tEpoch 211/1000: Loss at step 00: 5.918895721\n",
            "Epoch 211/1000 completed \t - \tBatch loss: 0.016578563\n",
            "Epoch number : 211\n",
            "\tEpoch 212/1000: Loss at step 00: 5.922924042\n",
            "Epoch 212/1000 completed \t - \tBatch loss: 0.013992460\n",
            "Epoch number : 212\n",
            "\tEpoch 213/1000: Loss at step 00: 5.932015896\n",
            "Epoch 213/1000 completed \t - \tBatch loss: 0.013278604\n",
            "Epoch number : 213\n",
            "\tEpoch 214/1000: Loss at step 00: 5.956667900\n",
            "Epoch 214/1000 completed \t - \tBatch loss: 0.016263455\n",
            "Epoch number : 214\n",
            "\tEpoch 215/1000: Loss at step 00: 5.962702751\n",
            "Epoch 215/1000 completed \t - \tBatch loss: 0.016952012\n",
            "Epoch number : 215\n",
            "\tEpoch 216/1000: Loss at step 00: 5.902390480\n",
            "Epoch 216/1000 completed \t - \tBatch loss: 0.015340287\n",
            "Epoch number : 216\n",
            "\tEpoch 217/1000: Loss at step 00: 5.831104279\n",
            "Epoch 217/1000 completed \t - \tBatch loss: 0.015927561\n",
            "Epoch number : 217\n",
            "\tEpoch 218/1000: Loss at step 00: 5.815367699\n",
            "Epoch 218/1000 completed \t - \tBatch loss: 0.018669309\n",
            "Epoch number : 218\n",
            "\tEpoch 219/1000: Loss at step 00: 5.865390778\n",
            "Epoch 219/1000 completed \t - \tBatch loss: 0.018030552\n",
            "Epoch number : 219\n",
            "\tEpoch 220/1000: Loss at step 00: 5.954115868\n",
            "Epoch 220/1000 completed \t - \tBatch loss: 0.016744502\n",
            "Epoch number : 220\n",
            "\tEpoch 221/1000: Loss at step 00: 6.020075321\n",
            "Epoch 221/1000 completed \t - \tBatch loss: 0.017926816\n",
            "Epoch number : 221\n",
            "\tEpoch 222/1000: Loss at step 00: 6.003619671\n",
            "Epoch 222/1000 completed \t - \tBatch loss: 0.015251441\n",
            "Epoch number : 222\n",
            "\tEpoch 223/1000: Loss at step 00: 5.948468685\n",
            "Epoch 223/1000 completed \t - \tBatch loss: 0.014715496\n",
            "Epoch number : 223\n",
            "\tEpoch 224/1000: Loss at step 00: 5.943446159\n",
            "Epoch 224/1000 completed \t - \tBatch loss: 0.018852418\n",
            "Epoch number : 224\n",
            "\tEpoch 225/1000: Loss at step 00: 5.928782463\n",
            "Epoch 225/1000 completed \t - \tBatch loss: 0.016979707\n",
            "Epoch number : 225\n",
            "\tEpoch 226/1000: Loss at step 00: 5.927888870\n",
            "Epoch 226/1000 completed \t - \tBatch loss: 0.013222798\n",
            "Epoch number : 226\n",
            "\tEpoch 227/1000: Loss at step 00: 5.953098297\n",
            "Epoch 227/1000 completed \t - \tBatch loss: 0.015894704\n",
            "Epoch number : 227\n",
            "\tEpoch 228/1000: Loss at step 00: 6.011159420\n",
            "Epoch 228/1000 completed \t - \tBatch loss: 0.014196943\n",
            "Epoch number : 228\n",
            "\tEpoch 229/1000: Loss at step 00: 6.054964542\n",
            "Epoch 229/1000 completed \t - \tBatch loss: 0.018069781\n",
            "Epoch number : 229\n",
            "\tEpoch 230/1000: Loss at step 00: 6.019828320\n",
            "Epoch 230/1000 completed \t - \tBatch loss: 0.013768246\n",
            "Epoch number : 230\n",
            "\tEpoch 231/1000: Loss at step 00: 5.964311600\n",
            "Epoch 231/1000 completed \t - \tBatch loss: 0.010521880\n",
            "Epoch number : 231\n",
            "\tEpoch 232/1000: Loss at step 00: 5.986088276\n",
            "Epoch 232/1000 completed \t - \tBatch loss: 0.008023774\n",
            "Epoch number : 232\n",
            "\tEpoch 233/1000: Loss at step 00: 6.095273018\n",
            "Epoch 233/1000 completed \t - \tBatch loss: 0.010835056\n",
            "Epoch number : 233\n",
            "\tEpoch 234/1000: Loss at step 00: 6.212708473\n",
            "Epoch 234/1000 completed \t - \tBatch loss: 0.014538970\n",
            "Epoch number : 234\n",
            "\tEpoch 235/1000: Loss at step 00: 6.243752480\n",
            "Epoch 235/1000 completed \t - \tBatch loss: 0.011795552\n",
            "Epoch number : 235\n",
            "\tEpoch 236/1000: Loss at step 00: 6.174975395\n",
            "Epoch 236/1000 completed \t - \tBatch loss: 0.006822227\n",
            "Epoch number : 236\n",
            "\tEpoch 237/1000: Loss at step 00: 6.103198051\n",
            "Epoch 237/1000 completed \t - \tBatch loss: 0.005969078\n",
            "Epoch number : 237\n",
            "\tEpoch 238/1000: Loss at step 00: 6.097064495\n",
            "Epoch 238/1000 completed \t - \tBatch loss: 0.007974203\n",
            "Epoch number : 238\n",
            "\tEpoch 239/1000: Loss at step 00: 6.122508526\n",
            "Epoch 239/1000 completed \t - \tBatch loss: 0.009599077\n",
            "Epoch number : 239\n",
            "\tEpoch 240/1000: Loss at step 00: 6.122491837\n",
            "Epoch 240/1000 completed \t - \tBatch loss: 0.008605251\n",
            "Epoch number : 240\n",
            "\tEpoch 241/1000: Loss at step 00: 6.118551731\n",
            "Epoch 241/1000 completed \t - \tBatch loss: 0.005984054\n",
            "Epoch number : 241\n",
            "\tEpoch 242/1000: Loss at step 00: 6.152823925\n",
            "Epoch 242/1000 completed \t - \tBatch loss: 0.004947413\n",
            "Epoch number : 242\n",
            "\tEpoch 243/1000: Loss at step 00: 6.231351376\n",
            "Epoch 243/1000 completed \t - \tBatch loss: 0.005140224\n",
            "Epoch number : 243\n",
            "\tEpoch 244/1000: Loss at step 00: 6.291188240\n",
            "Epoch 244/1000 completed \t - \tBatch loss: 0.005388675\n",
            "Epoch number : 244\n",
            "\tEpoch 245/1000: Loss at step 00: 6.323354721\n",
            "Epoch 245/1000 completed \t - \tBatch loss: 0.005202363\n",
            "Epoch number : 245\n",
            "\tEpoch 246/1000: Loss at step 00: 6.329391479\n",
            "Epoch 246/1000 completed \t - \tBatch loss: 0.004730287\n",
            "Epoch number : 246\n",
            "\tEpoch 247/1000: Loss at step 00: 6.319848061\n",
            "Epoch 247/1000 completed \t - \tBatch loss: 0.004398827\n",
            "Epoch number : 247\n",
            "\tEpoch 248/1000: Loss at step 00: 6.309689045\n",
            "Epoch 248/1000 completed \t - \tBatch loss: 0.004037071\n",
            "Epoch number : 248\n",
            "\tEpoch 249/1000: Loss at step 00: 6.298210144\n",
            "Epoch 249/1000 completed \t - \tBatch loss: 0.003848719\n",
            "Epoch number : 249\n",
            "\tEpoch 250/1000: Loss at step 00: 6.291081429\n",
            "Epoch 250/1000 completed \t - \tBatch loss: 0.003778015\n",
            "Epoch number : 250\n",
            "\tEpoch 251/1000: Loss at step 00: 6.285313606\n",
            "Epoch 251/1000 completed \t - \tBatch loss: 0.003730349\n",
            "Epoch number : 251\n",
            "\tEpoch 252/1000: Loss at step 00: 6.279888153\n",
            "Epoch 252/1000 completed \t - \tBatch loss: 0.003704442\n",
            "Epoch number : 252\n",
            "\tEpoch 253/1000: Loss at step 00: 6.276579857\n",
            "Epoch 253/1000 completed \t - \tBatch loss: 0.003693907\n",
            "Epoch number : 253\n",
            "\tEpoch 254/1000: Loss at step 00: 6.277405739\n",
            "Epoch 254/1000 completed \t - \tBatch loss: 0.003639259\n",
            "Epoch number : 254\n",
            "\tEpoch 255/1000: Loss at step 00: 6.283876419\n",
            "Epoch 255/1000 completed \t - \tBatch loss: 0.003521061\n",
            "Epoch number : 255\n",
            "\tEpoch 256/1000: Loss at step 00: 6.297788620\n",
            "Epoch 256/1000 completed \t - \tBatch loss: 0.003427198\n",
            "Epoch number : 256\n",
            "\tEpoch 257/1000: Loss at step 00: 6.319397926\n",
            "Epoch 257/1000 completed \t - \tBatch loss: 0.003467908\n",
            "Epoch number : 257\n",
            "\tEpoch 258/1000: Loss at step 00: 6.344966888\n",
            "Epoch 258/1000 completed \t - \tBatch loss: 0.003638441\n",
            "Epoch number : 258\n",
            "\tEpoch 259/1000: Loss at step 00: 6.365375996\n",
            "Epoch 259/1000 completed \t - \tBatch loss: 0.003802858\n",
            "Epoch number : 259\n",
            "\tEpoch 260/1000: Loss at step 00: 6.373638153\n",
            "Epoch 260/1000 completed \t - \tBatch loss: 0.003808472\n",
            "Epoch number : 260\n",
            "\tEpoch 261/1000: Loss at step 00: 6.367105961\n",
            "Epoch 261/1000 completed \t - \tBatch loss: 0.003681784\n",
            "Epoch number : 261\n",
            "\tEpoch 262/1000: Loss at step 00: 6.351233006\n",
            "Epoch 262/1000 completed \t - \tBatch loss: 0.003561907\n",
            "Epoch number : 262\n",
            "\tEpoch 263/1000: Loss at step 00: 6.335349083\n",
            "Epoch 263/1000 completed \t - \tBatch loss: 0.003488902\n",
            "Epoch number : 263\n",
            "\tEpoch 264/1000: Loss at step 00: 6.324804306\n",
            "Epoch 264/1000 completed \t - \tBatch loss: 0.003413725\n",
            "Epoch number : 264\n",
            "\tEpoch 265/1000: Loss at step 00: 6.321315289\n",
            "Epoch 265/1000 completed \t - \tBatch loss: 0.003328337\n",
            "Epoch number : 265\n",
            "\tEpoch 266/1000: Loss at step 00: 6.325900078\n",
            "Epoch 266/1000 completed \t - \tBatch loss: 0.003274974\n",
            "Epoch number : 266\n",
            "\tEpoch 267/1000: Loss at step 00: 6.337670326\n",
            "Epoch 267/1000 completed \t - \tBatch loss: 0.003275590\n",
            "Epoch number : 267\n",
            "\tEpoch 268/1000: Loss at step 00: 6.352898598\n",
            "Epoch 268/1000 completed \t - \tBatch loss: 0.003250620\n",
            "Epoch number : 268\n",
            "\tEpoch 269/1000: Loss at step 00: 6.366908550\n",
            "Epoch 269/1000 completed \t - \tBatch loss: 0.003156123\n",
            "Epoch number : 269\n",
            "\tEpoch 270/1000: Loss at step 00: 6.378045082\n",
            "Epoch 270/1000 completed \t - \tBatch loss: 0.003056191\n",
            "Epoch number : 270\n",
            "\tEpoch 271/1000: Loss at step 00: 6.388550758\n",
            "Epoch 271/1000 completed \t - \tBatch loss: 0.003029981\n",
            "Epoch number : 271\n",
            "\tEpoch 272/1000: Loss at step 00: 6.399298668\n",
            "Epoch 272/1000 completed \t - \tBatch loss: 0.003109710\n",
            "Epoch number : 272\n",
            "\tEpoch 273/1000: Loss at step 00: 6.411035061\n",
            "Epoch 273/1000 completed \t - \tBatch loss: 0.003275992\n",
            "Epoch number : 273\n",
            "\tEpoch 274/1000: Loss at step 00: 6.423948765\n",
            "Epoch 274/1000 completed \t - \tBatch loss: 0.003471940\n",
            "Epoch number : 274\n",
            "\tEpoch 275/1000: Loss at step 00: 6.436742306\n",
            "Epoch 275/1000 completed \t - \tBatch loss: 0.003608539\n",
            "Epoch number : 275\n",
            "\tEpoch 276/1000: Loss at step 00: 6.445634842\n",
            "Epoch 276/1000 completed \t - \tBatch loss: 0.003614727\n",
            "Epoch number : 276\n",
            "\tEpoch 277/1000: Loss at step 00: 6.452373981\n",
            "Epoch 277/1000 completed \t - \tBatch loss: 0.003521734\n",
            "Epoch number : 277\n",
            "\tEpoch 278/1000: Loss at step 00: 6.456968307\n",
            "Epoch 278/1000 completed \t - \tBatch loss: 0.003412636\n",
            "Epoch number : 278\n",
            "\tEpoch 279/1000: Loss at step 00: 6.458690643\n",
            "Epoch 279/1000 completed \t - \tBatch loss: 0.003340233\n",
            "Epoch number : 279\n",
            "\tEpoch 280/1000: Loss at step 00: 6.458199501\n",
            "Epoch 280/1000 completed \t - \tBatch loss: 0.003350155\n",
            "Epoch number : 280\n",
            "\tEpoch 281/1000: Loss at step 00: 6.459712982\n",
            "Epoch 281/1000 completed \t - \tBatch loss: 0.003421130\n",
            "Epoch number : 281\n",
            "\tEpoch 282/1000: Loss at step 00: 6.463125229\n",
            "Epoch 282/1000 completed \t - \tBatch loss: 0.003436239\n",
            "Epoch number : 282\n",
            "\tEpoch 283/1000: Loss at step 00: 6.467197418\n",
            "Epoch 283/1000 completed \t - \tBatch loss: 0.003325117\n",
            "Epoch number : 283\n",
            "\tEpoch 284/1000: Loss at step 00: 6.472894669\n",
            "Epoch 284/1000 completed \t - \tBatch loss: 0.003169309\n",
            "Epoch number : 284\n",
            "\tEpoch 285/1000: Loss at step 00: 6.481207848\n",
            "Epoch 285/1000 completed \t - \tBatch loss: 0.003143217\n",
            "Epoch number : 285\n",
            "\tEpoch 286/1000: Loss at step 00: 6.492368698\n",
            "Epoch 286/1000 completed \t - \tBatch loss: 0.003324442\n",
            "Epoch number : 286\n",
            "\tEpoch 287/1000: Loss at step 00: 6.503583908\n",
            "Epoch 287/1000 completed \t - \tBatch loss: 0.003561068\n",
            "Epoch number : 287\n",
            "\tEpoch 288/1000: Loss at step 00: 6.507769585\n",
            "Epoch 288/1000 completed \t - \tBatch loss: 0.003568488\n",
            "Epoch number : 288\n",
            "\tEpoch 289/1000: Loss at step 00: 6.500494480\n",
            "Epoch 289/1000 completed \t - \tBatch loss: 0.003391589\n",
            "Epoch number : 289\n",
            "\tEpoch 290/1000: Loss at step 00: 6.485513687\n",
            "Epoch 290/1000 completed \t - \tBatch loss: 0.003307933\n",
            "Epoch number : 290\n",
            "\tEpoch 291/1000: Loss at step 00: 6.471714020\n",
            "Epoch 291/1000 completed \t - \tBatch loss: 0.003350472\n",
            "Epoch number : 291\n",
            "\tEpoch 292/1000: Loss at step 00: 6.460740089\n",
            "Epoch 292/1000 completed \t - \tBatch loss: 0.003348067\n",
            "Epoch number : 292\n",
            "\tEpoch 293/1000: Loss at step 00: 6.454294682\n",
            "Epoch 293/1000 completed \t - \tBatch loss: 0.003261914\n",
            "Epoch number : 293\n",
            "\tEpoch 294/1000: Loss at step 00: 6.452096939\n",
            "Epoch 294/1000 completed \t - \tBatch loss: 0.003175294\n",
            "Epoch number : 294\n",
            "\tEpoch 295/1000: Loss at step 00: 6.454293728\n",
            "Epoch 295/1000 completed \t - \tBatch loss: 0.003130749\n",
            "Epoch number : 295\n",
            "\tEpoch 296/1000: Loss at step 00: 6.459531307\n",
            "Epoch 296/1000 completed \t - \tBatch loss: 0.003111771\n",
            "Epoch number : 296\n",
            "\tEpoch 297/1000: Loss at step 00: 6.465530872\n",
            "Epoch 297/1000 completed \t - \tBatch loss: 0.003099463\n",
            "Epoch number : 297\n",
            "\tEpoch 298/1000: Loss at step 00: 6.467685699\n",
            "Epoch 298/1000 completed \t - \tBatch loss: 0.003088246\n",
            "Epoch number : 298\n",
            "\tEpoch 299/1000: Loss at step 00: 6.467340469\n",
            "Epoch 299/1000 completed \t - \tBatch loss: 0.003082176\n",
            "Epoch number : 299\n",
            "\tEpoch 300/1000: Loss at step 00: 6.467097282\n",
            "Epoch 300/1000 completed \t - \tBatch loss: 0.003125170\n",
            "Epoch number : 300\n",
            "\tEpoch 301/1000: Loss at step 00: 6.467042923\n",
            "Epoch 301/1000 completed \t - \tBatch loss: 0.003207327\n",
            "Epoch number : 301\n",
            "\tEpoch 302/1000: Loss at step 00: 6.470033169\n",
            "Epoch 302/1000 completed \t - \tBatch loss: 0.003282650\n",
            "Epoch number : 302\n",
            "\tEpoch 303/1000: Loss at step 00: 6.477317810\n",
            "Epoch 303/1000 completed \t - \tBatch loss: 0.003342574\n",
            "Epoch number : 303\n",
            "\tEpoch 304/1000: Loss at step 00: 6.486972809\n",
            "Epoch 304/1000 completed \t - \tBatch loss: 0.003396302\n",
            "Epoch number : 304\n",
            "\tEpoch 305/1000: Loss at step 00: 6.496109486\n",
            "Epoch 305/1000 completed \t - \tBatch loss: 0.003350629\n",
            "Epoch number : 305\n",
            "\tEpoch 306/1000: Loss at step 00: 6.502780437\n",
            "Epoch 306/1000 completed \t - \tBatch loss: 0.003168746\n",
            "Epoch number : 306\n",
            "\tEpoch 307/1000: Loss at step 00: 6.506061077\n",
            "Epoch 307/1000 completed \t - \tBatch loss: 0.002996629\n",
            "Epoch number : 307\n",
            "\tEpoch 308/1000: Loss at step 00: 6.508378506\n",
            "Epoch 308/1000 completed \t - \tBatch loss: 0.002941419\n",
            "Epoch number : 308\n",
            "\tEpoch 309/1000: Loss at step 00: 6.510014534\n",
            "Epoch 309/1000 completed \t - \tBatch loss: 0.002986874\n",
            "Epoch number : 309\n",
            "\tEpoch 310/1000: Loss at step 00: 6.507991791\n",
            "Epoch 310/1000 completed \t - \tBatch loss: 0.003035651\n",
            "Epoch number : 310\n",
            "\tEpoch 311/1000: Loss at step 00: 6.498884201\n",
            "Epoch 311/1000 completed \t - \tBatch loss: 0.002991864\n",
            "Epoch number : 311\n",
            "\tEpoch 312/1000: Loss at step 00: 6.484562874\n",
            "Epoch 312/1000 completed \t - \tBatch loss: 0.002869012\n",
            "Epoch number : 312\n",
            "\tEpoch 313/1000: Loss at step 00: 6.471790314\n",
            "Epoch 313/1000 completed \t - \tBatch loss: 0.002773480\n",
            "Epoch number : 313\n",
            "\tEpoch 314/1000: Loss at step 00: 6.466391087\n",
            "Epoch 314/1000 completed \t - \tBatch loss: 0.002782716\n",
            "Epoch number : 314\n",
            "\tEpoch 315/1000: Loss at step 00: 6.472381115\n",
            "Epoch 315/1000 completed \t - \tBatch loss: 0.002865461\n",
            "Epoch number : 315\n",
            "\tEpoch 316/1000: Loss at step 00: 6.489123821\n",
            "Epoch 316/1000 completed \t - \tBatch loss: 0.002961410\n",
            "Epoch number : 316\n",
            "\tEpoch 317/1000: Loss at step 00: 6.512781143\n",
            "Epoch 317/1000 completed \t - \tBatch loss: 0.003043205\n",
            "Epoch number : 317\n",
            "\tEpoch 318/1000: Loss at step 00: 6.537335873\n",
            "Epoch 318/1000 completed \t - \tBatch loss: 0.003082610\n",
            "Epoch number : 318\n",
            "\tEpoch 319/1000: Loss at step 00: 6.559500694\n",
            "Epoch 319/1000 completed \t - \tBatch loss: 0.003016788\n",
            "Epoch number : 319\n",
            "\tEpoch 320/1000: Loss at step 00: 6.576919556\n",
            "Epoch 320/1000 completed \t - \tBatch loss: 0.002880006\n",
            "Epoch number : 320\n",
            "\tEpoch 321/1000: Loss at step 00: 6.586946487\n",
            "Epoch 321/1000 completed \t - \tBatch loss: 0.002771969\n",
            "Epoch number : 321\n",
            "\tEpoch 322/1000: Loss at step 00: 6.590430260\n",
            "Epoch 322/1000 completed \t - \tBatch loss: 0.002739947\n",
            "Epoch number : 322\n",
            "\tEpoch 323/1000: Loss at step 00: 6.585529804\n",
            "Epoch 323/1000 completed \t - \tBatch loss: 0.002746672\n",
            "Epoch number : 323\n",
            "\tEpoch 324/1000: Loss at step 00: 6.569345474\n",
            "Epoch 324/1000 completed \t - \tBatch loss: 0.002739068\n",
            "Epoch number : 324\n",
            "\tEpoch 325/1000: Loss at step 00: 6.545237064\n",
            "Epoch 325/1000 completed \t - \tBatch loss: 0.002680913\n",
            "Epoch number : 325\n",
            "\tEpoch 326/1000: Loss at step 00: 6.522665024\n",
            "Epoch 326/1000 completed \t - \tBatch loss: 0.002592154\n",
            "Epoch number : 326\n",
            "\tEpoch 327/1000: Loss at step 00: 6.509786606\n",
            "Epoch 327/1000 completed \t - \tBatch loss: 0.002532005\n",
            "Epoch number : 327\n",
            "\tEpoch 328/1000: Loss at step 00: 6.511574268\n",
            "Epoch 328/1000 completed \t - \tBatch loss: 0.002546145\n",
            "Epoch number : 328\n",
            "\tEpoch 329/1000: Loss at step 00: 6.530304432\n",
            "Epoch 329/1000 completed \t - \tBatch loss: 0.002614574\n",
            "Epoch number : 329\n",
            "\tEpoch 330/1000: Loss at step 00: 6.560653687\n",
            "Epoch 330/1000 completed \t - \tBatch loss: 0.002688306\n",
            "Epoch number : 330\n",
            "\tEpoch 331/1000: Loss at step 00: 6.594039917\n",
            "Epoch 331/1000 completed \t - \tBatch loss: 0.002738489\n",
            "Epoch number : 331\n",
            "\tEpoch 332/1000: Loss at step 00: 6.621079922\n",
            "Epoch 332/1000 completed \t - \tBatch loss: 0.002795608\n",
            "Epoch number : 332\n",
            "\tEpoch 333/1000: Loss at step 00: 6.639770508\n",
            "Epoch 333/1000 completed \t - \tBatch loss: 0.002895819\n",
            "Epoch number : 333\n",
            "\tEpoch 334/1000: Loss at step 00: 6.650039196\n",
            "Epoch 334/1000 completed \t - \tBatch loss: 0.002926957\n",
            "Epoch number : 334\n",
            "\tEpoch 335/1000: Loss at step 00: 6.652046204\n",
            "Epoch 335/1000 completed \t - \tBatch loss: 0.002797069\n",
            "Epoch number : 335\n",
            "\tEpoch 336/1000: Loss at step 00: 6.645947456\n",
            "Epoch 336/1000 completed \t - \tBatch loss: 0.002633733\n",
            "Epoch number : 336\n",
            "\tEpoch 337/1000: Loss at step 00: 6.634197235\n",
            "Epoch 337/1000 completed \t - \tBatch loss: 0.002563673\n",
            "Epoch number : 337\n",
            "\tEpoch 338/1000: Loss at step 00: 6.616336346\n",
            "Epoch 338/1000 completed \t - \tBatch loss: 0.002574855\n",
            "Epoch number : 338\n",
            "\tEpoch 339/1000: Loss at step 00: 6.594703197\n",
            "Epoch 339/1000 completed \t - \tBatch loss: 0.002552733\n",
            "Epoch number : 339\n",
            "\tEpoch 340/1000: Loss at step 00: 6.577703953\n",
            "Epoch 340/1000 completed \t - \tBatch loss: 0.002492477\n",
            "Epoch number : 340\n",
            "\tEpoch 341/1000: Loss at step 00: 6.571534634\n",
            "Epoch 341/1000 completed \t - \tBatch loss: 0.002480284\n",
            "Epoch number : 341\n",
            "\tEpoch 342/1000: Loss at step 00: 6.579793930\n",
            "Epoch 342/1000 completed \t - \tBatch loss: 0.002573975\n",
            "Epoch number : 342\n",
            "\tEpoch 343/1000: Loss at step 00: 6.603367329\n",
            "Epoch 343/1000 completed \t - \tBatch loss: 0.002697364\n",
            "Epoch number : 343\n",
            "\tEpoch 344/1000: Loss at step 00: 6.637328148\n",
            "Epoch 344/1000 completed \t - \tBatch loss: 0.002670246\n",
            "Epoch number : 344\n",
            "\tEpoch 345/1000: Loss at step 00: 6.671860218\n",
            "Epoch 345/1000 completed \t - \tBatch loss: 0.002572288\n",
            "Epoch number : 345\n",
            "\tEpoch 346/1000: Loss at step 00: 6.697755814\n",
            "Epoch 346/1000 completed \t - \tBatch loss: 0.002622735\n",
            "Epoch number : 346\n",
            "\tEpoch 347/1000: Loss at step 00: 6.710843086\n",
            "Epoch 347/1000 completed \t - \tBatch loss: 0.002972098\n",
            "Epoch number : 347\n",
            "\tEpoch 348/1000: Loss at step 00: 6.714150429\n",
            "Epoch 348/1000 completed \t - \tBatch loss: 0.003262348\n",
            "Epoch number : 348\n",
            "\tEpoch 349/1000: Loss at step 00: 6.714728355\n",
            "Epoch 349/1000 completed \t - \tBatch loss: 0.002979931\n",
            "Epoch number : 349\n",
            "\tEpoch 350/1000: Loss at step 00: 6.710668564\n",
            "Epoch 350/1000 completed \t - \tBatch loss: 0.002675411\n",
            "Epoch number : 350\n",
            "\tEpoch 351/1000: Loss at step 00: 6.702970505\n",
            "Epoch 351/1000 completed \t - \tBatch loss: 0.002570052\n",
            "Epoch number : 351\n",
            "\tEpoch 352/1000: Loss at step 00: 6.690852165\n",
            "Epoch 352/1000 completed \t - \tBatch loss: 0.002582679\n",
            "Epoch number : 352\n",
            "\tEpoch 353/1000: Loss at step 00: 6.670785427\n",
            "Epoch 353/1000 completed \t - \tBatch loss: 0.002583408\n",
            "Epoch number : 353\n",
            "\tEpoch 354/1000: Loss at step 00: 6.654635429\n",
            "Epoch 354/1000 completed \t - \tBatch loss: 0.002603776\n",
            "Epoch number : 354\n",
            "\tEpoch 355/1000: Loss at step 00: 6.647516251\n",
            "Epoch 355/1000 completed \t - \tBatch loss: 0.002731552\n",
            "Epoch number : 355\n",
            "\tEpoch 356/1000: Loss at step 00: 6.655694962\n",
            "Epoch 356/1000 completed \t - \tBatch loss: 0.002914229\n",
            "Epoch number : 356\n",
            "\tEpoch 357/1000: Loss at step 00: 6.685798168\n",
            "Epoch 357/1000 completed \t - \tBatch loss: 0.002927464\n",
            "Epoch number : 357\n",
            "\tEpoch 358/1000: Loss at step 00: 6.733288288\n",
            "Epoch 358/1000 completed \t - \tBatch loss: 0.002735811\n",
            "Epoch number : 358\n",
            "\tEpoch 359/1000: Loss at step 00: 6.785407066\n",
            "Epoch 359/1000 completed \t - \tBatch loss: 0.002549090\n",
            "Epoch number : 359\n",
            "\tEpoch 360/1000: Loss at step 00: 6.828765869\n",
            "Epoch 360/1000 completed \t - \tBatch loss: 0.002474522\n",
            "Epoch number : 360\n",
            "\tEpoch 361/1000: Loss at step 00: 6.846402168\n",
            "Epoch 361/1000 completed \t - \tBatch loss: 0.002585163\n",
            "Epoch number : 361\n",
            "\tEpoch 362/1000: Loss at step 00: 6.832883835\n",
            "Epoch 362/1000 completed \t - \tBatch loss: 0.003025702\n",
            "Epoch number : 362\n",
            "\tEpoch 363/1000: Loss at step 00: 6.806426048\n",
            "Epoch 363/1000 completed \t - \tBatch loss: 0.003410789\n",
            "Epoch number : 363\n",
            "\tEpoch 364/1000: Loss at step 00: 6.785412312\n",
            "Epoch 364/1000 completed \t - \tBatch loss: 0.003005406\n",
            "Epoch number : 364\n",
            "\tEpoch 365/1000: Loss at step 00: 6.772579670\n",
            "Epoch 365/1000 completed \t - \tBatch loss: 0.002578809\n",
            "Epoch number : 365\n",
            "\tEpoch 366/1000: Loss at step 00: 6.776729584\n",
            "Epoch 366/1000 completed \t - \tBatch loss: 0.002452419\n",
            "Epoch number : 366\n",
            "\tEpoch 367/1000: Loss at step 00: 6.785877228\n",
            "Epoch 367/1000 completed \t - \tBatch loss: 0.002465851\n",
            "Epoch number : 367\n",
            "\tEpoch 368/1000: Loss at step 00: 6.789054394\n",
            "Epoch 368/1000 completed \t - \tBatch loss: 0.002491107\n",
            "Epoch number : 368\n",
            "\tEpoch 369/1000: Loss at step 00: 6.781304359\n",
            "Epoch 369/1000 completed \t - \tBatch loss: 0.002603598\n",
            "Epoch number : 369\n",
            "\tEpoch 370/1000: Loss at step 00: 6.770118713\n",
            "Epoch 370/1000 completed \t - \tBatch loss: 0.002857785\n",
            "Epoch number : 370\n",
            "\tEpoch 371/1000: Loss at step 00: 6.765642166\n",
            "Epoch 371/1000 completed \t - \tBatch loss: 0.003066010\n",
            "Epoch number : 371\n",
            "\tEpoch 372/1000: Loss at step 00: 6.779943943\n",
            "Epoch 372/1000 completed \t - \tBatch loss: 0.002865532\n",
            "Epoch number : 372\n",
            "\tEpoch 373/1000: Loss at step 00: 6.816919804\n",
            "Epoch 373/1000 completed \t - \tBatch loss: 0.002439448\n",
            "Epoch number : 373\n",
            "\tEpoch 374/1000: Loss at step 00: 6.870925903\n",
            "Epoch 374/1000 completed \t - \tBatch loss: 0.002221155\n",
            "Epoch number : 374\n",
            "\tEpoch 375/1000: Loss at step 00: 6.928686142\n",
            "Epoch 375/1000 completed \t - \tBatch loss: 0.002250703\n",
            "Epoch number : 375\n",
            "\tEpoch 376/1000: Loss at step 00: 6.970511436\n",
            "Epoch 376/1000 completed \t - \tBatch loss: 0.002420675\n",
            "Epoch number : 376\n",
            "\tEpoch 377/1000: Loss at step 00: 6.975028038\n",
            "Epoch 377/1000 completed \t - \tBatch loss: 0.002719798\n",
            "Epoch number : 377\n",
            "\tEpoch 378/1000: Loss at step 00: 6.947213173\n",
            "Epoch 378/1000 completed \t - \tBatch loss: 0.003177396\n",
            "Epoch number : 378\n",
            "\tEpoch 379/1000: Loss at step 00: 6.907804966\n",
            "Epoch 379/1000 completed \t - \tBatch loss: 0.003225619\n",
            "Epoch number : 379\n",
            "\tEpoch 380/1000: Loss at step 00: 6.887582779\n",
            "Epoch 380/1000 completed \t - \tBatch loss: 0.002746476\n",
            "Epoch number : 380\n",
            "\tEpoch 381/1000: Loss at step 00: 6.894266129\n",
            "Epoch 381/1000 completed \t - \tBatch loss: 0.002421239\n",
            "Epoch number : 381\n",
            "\tEpoch 382/1000: Loss at step 00: 6.915554047\n",
            "Epoch 382/1000 completed \t - \tBatch loss: 0.002287318\n",
            "Epoch number : 382\n",
            "\tEpoch 383/1000: Loss at step 00: 6.932401180\n",
            "Epoch 383/1000 completed \t - \tBatch loss: 0.002272343\n",
            "Epoch number : 383\n",
            "\tEpoch 384/1000: Loss at step 00: 6.928016663\n",
            "Epoch 384/1000 completed \t - \tBatch loss: 0.002367115\n",
            "Epoch number : 384\n",
            "\tEpoch 385/1000: Loss at step 00: 6.906271935\n",
            "Epoch 385/1000 completed \t - \tBatch loss: 0.002639047\n",
            "Epoch number : 385\n",
            "\tEpoch 386/1000: Loss at step 00: 6.884192944\n",
            "Epoch 386/1000 completed \t - \tBatch loss: 0.002981951\n",
            "Epoch number : 386\n",
            "\tEpoch 387/1000: Loss at step 00: 6.874146938\n",
            "Epoch 387/1000 completed \t - \tBatch loss: 0.003037288\n",
            "Epoch number : 387\n",
            "\tEpoch 388/1000: Loss at step 00: 6.895958424\n",
            "Epoch 388/1000 completed \t - \tBatch loss: 0.002543963\n",
            "Epoch number : 388\n",
            "\tEpoch 389/1000: Loss at step 00: 6.955757618\n",
            "Epoch 389/1000 completed \t - \tBatch loss: 0.002107759\n",
            "Epoch number : 389\n",
            "\tEpoch 390/1000: Loss at step 00: 7.044304371\n",
            "Epoch 390/1000 completed \t - \tBatch loss: 0.002064304\n",
            "Epoch number : 390\n",
            "\tEpoch 391/1000: Loss at step 00: 7.140164375\n",
            "Epoch 391/1000 completed \t - \tBatch loss: 0.002327200\n",
            "Epoch number : 391\n",
            "\tEpoch 392/1000: Loss at step 00: 7.222308636\n",
            "Epoch 392/1000 completed \t - \tBatch loss: 0.002658670\n",
            "Epoch number : 392\n",
            "\tEpoch 393/1000: Loss at step 00: 7.264345646\n",
            "Epoch 393/1000 completed \t - \tBatch loss: 0.002773158\n",
            "Epoch number : 393\n",
            "\tEpoch 394/1000: Loss at step 00: 7.230311394\n",
            "Epoch 394/1000 completed \t - \tBatch loss: 0.002709042\n",
            "Epoch number : 394\n",
            "\tEpoch 395/1000: Loss at step 00: 7.133843422\n",
            "Epoch 395/1000 completed \t - \tBatch loss: 0.002806463\n",
            "Epoch number : 395\n",
            "\tEpoch 396/1000: Loss at step 00: 7.053961277\n",
            "Epoch 396/1000 completed \t - \tBatch loss: 0.002611034\n",
            "Epoch number : 396\n",
            "\tEpoch 397/1000: Loss at step 00: 7.041229725\n",
            "Epoch 397/1000 completed \t - \tBatch loss: 0.002360371\n",
            "Epoch number : 397\n",
            "\tEpoch 398/1000: Loss at step 00: 7.079018593\n",
            "Epoch 398/1000 completed \t - \tBatch loss: 0.002392358\n",
            "Epoch number : 398\n",
            "\tEpoch 399/1000: Loss at step 00: 7.124111652\n",
            "Epoch 399/1000 completed \t - \tBatch loss: 0.002491027\n",
            "Epoch number : 399\n",
            "\tEpoch 400/1000: Loss at step 00: 7.134950638\n",
            "Epoch 400/1000 completed \t - \tBatch loss: 0.002710935\n",
            "Epoch number : 400\n",
            "\tEpoch 401/1000: Loss at step 00: 7.098212242\n",
            "Epoch 401/1000 completed \t - \tBatch loss: 0.002538906\n",
            "Epoch number : 401\n",
            "\tEpoch 402/1000: Loss at step 00: 7.040509224\n",
            "Epoch 402/1000 completed \t - \tBatch loss: 0.002675802\n",
            "Epoch number : 402\n",
            "\tEpoch 403/1000: Loss at step 00: 7.006456852\n",
            "Epoch 403/1000 completed \t - \tBatch loss: 0.003101878\n",
            "Epoch number : 403\n",
            "\tEpoch 404/1000: Loss at step 00: 7.001606464\n",
            "Epoch 404/1000 completed \t - \tBatch loss: 0.002900801\n",
            "Epoch number : 404\n",
            "\tEpoch 405/1000: Loss at step 00: 7.029848576\n",
            "Epoch 405/1000 completed \t - \tBatch loss: 0.002331043\n",
            "Epoch number : 405\n",
            "\tEpoch 406/1000: Loss at step 00: 7.108733177\n",
            "Epoch 406/1000 completed \t - \tBatch loss: 0.002282391\n",
            "Epoch number : 406\n",
            "\tEpoch 407/1000: Loss at step 00: 7.217901230\n",
            "Epoch 407/1000 completed \t - \tBatch loss: 0.002640687\n",
            "Epoch number : 407\n",
            "\tEpoch 408/1000: Loss at step 00: 7.323665142\n",
            "Epoch 408/1000 completed \t - \tBatch loss: 0.003104048\n",
            "Epoch number : 408\n",
            "\tEpoch 409/1000: Loss at step 00: 7.393083572\n",
            "Epoch 409/1000 completed \t - \tBatch loss: 0.003035441\n",
            "Epoch number : 409\n",
            "\tEpoch 410/1000: Loss at step 00: 7.409330845\n",
            "Epoch 410/1000 completed \t - \tBatch loss: 0.002544219\n",
            "Epoch number : 410\n",
            "\tEpoch 411/1000: Loss at step 00: 7.390661716\n",
            "Epoch 411/1000 completed \t - \tBatch loss: 0.002218509\n",
            "Epoch number : 411\n",
            "\tEpoch 412/1000: Loss at step 00: 7.334063530\n",
            "Epoch 412/1000 completed \t - \tBatch loss: 0.002179447\n",
            "Epoch number : 412\n",
            "\tEpoch 413/1000: Loss at step 00: 7.256328106\n",
            "Epoch 413/1000 completed \t - \tBatch loss: 0.002117602\n",
            "Epoch number : 413\n",
            "\tEpoch 414/1000: Loss at step 00: 7.178607941\n",
            "Epoch 414/1000 completed \t - \tBatch loss: 0.001909187\n",
            "Epoch number : 414\n",
            "\tEpoch 415/1000: Loss at step 00: 7.137672424\n",
            "Epoch 415/1000 completed \t - \tBatch loss: 0.001889791\n",
            "Epoch number : 415\n",
            "\tEpoch 416/1000: Loss at step 00: 7.147937298\n",
            "Epoch 416/1000 completed \t - \tBatch loss: 0.002122210\n",
            "Epoch number : 416\n",
            "\tEpoch 417/1000: Loss at step 00: 7.184721947\n",
            "Epoch 417/1000 completed \t - \tBatch loss: 0.002390169\n",
            "Epoch number : 417\n",
            "\tEpoch 418/1000: Loss at step 00: 7.211886406\n",
            "Epoch 418/1000 completed \t - \tBatch loss: 0.002277725\n",
            "Epoch number : 418\n",
            "\tEpoch 419/1000: Loss at step 00: 7.206193447\n",
            "Epoch 419/1000 completed \t - \tBatch loss: 0.001950277\n",
            "Epoch number : 419\n",
            "\tEpoch 420/1000: Loss at step 00: 7.178269386\n",
            "Epoch 420/1000 completed \t - \tBatch loss: 0.001816649\n",
            "Epoch number : 420\n",
            "\tEpoch 421/1000: Loss at step 00: 7.165692329\n",
            "Epoch 421/1000 completed \t - \tBatch loss: 0.001962998\n",
            "Epoch number : 421\n",
            "\tEpoch 422/1000: Loss at step 00: 7.172640800\n",
            "Epoch 422/1000 completed \t - \tBatch loss: 0.002264356\n",
            "Epoch number : 422\n",
            "\tEpoch 423/1000: Loss at step 00: 7.197772026\n",
            "Epoch 423/1000 completed \t - \tBatch loss: 0.002140055\n",
            "Epoch number : 423\n",
            "\tEpoch 424/1000: Loss at step 00: 7.248901844\n",
            "Epoch 424/1000 completed \t - \tBatch loss: 0.001722464\n",
            "Epoch number : 424\n",
            "\tEpoch 425/1000: Loss at step 00: 7.327564240\n",
            "Epoch 425/1000 completed \t - \tBatch loss: 0.001530779\n",
            "Epoch number : 425\n",
            "\tEpoch 426/1000: Loss at step 00: 7.415743828\n",
            "Epoch 426/1000 completed \t - \tBatch loss: 0.001593653\n",
            "Epoch number : 426\n",
            "\tEpoch 427/1000: Loss at step 00: 7.494297028\n",
            "Epoch 427/1000 completed \t - \tBatch loss: 0.001856243\n",
            "Epoch number : 427\n",
            "\tEpoch 428/1000: Loss at step 00: 7.552954674\n",
            "Epoch 428/1000 completed \t - \tBatch loss: 0.002236377\n",
            "Epoch number : 428\n",
            "\tEpoch 429/1000: Loss at step 00: 7.578307152\n",
            "Epoch 429/1000 completed \t - \tBatch loss: 0.002284394\n",
            "Epoch number : 429\n",
            "\tEpoch 430/1000: Loss at step 00: 7.540739536\n",
            "Epoch 430/1000 completed \t - \tBatch loss: 0.001813484\n",
            "Epoch number : 430\n",
            "\tEpoch 431/1000: Loss at step 00: 7.444930077\n",
            "Epoch 431/1000 completed \t - \tBatch loss: 0.001427214\n",
            "Epoch number : 431\n",
            "\tEpoch 432/1000: Loss at step 00: 7.351824284\n",
            "Epoch 432/1000 completed \t - \tBatch loss: 0.001322151\n",
            "Epoch number : 432\n",
            "\tEpoch 433/1000: Loss at step 00: 7.286073685\n",
            "Epoch 433/1000 completed \t - \tBatch loss: 0.001364727\n",
            "Epoch number : 433\n",
            "\tEpoch 434/1000: Loss at step 00: 7.248741150\n",
            "Epoch 434/1000 completed \t - \tBatch loss: 0.001434161\n",
            "Epoch number : 434\n",
            "\tEpoch 435/1000: Loss at step 00: 7.247411728\n",
            "Epoch 435/1000 completed \t - \tBatch loss: 0.001448126\n",
            "Epoch number : 435\n",
            "\tEpoch 436/1000: Loss at step 00: 7.286180019\n",
            "Epoch 436/1000 completed \t - \tBatch loss: 0.001464898\n",
            "Epoch number : 436\n",
            "\tEpoch 437/1000: Loss at step 00: 7.348095894\n",
            "Epoch 437/1000 completed \t - \tBatch loss: 0.001605431\n",
            "Epoch number : 437\n",
            "\tEpoch 438/1000: Loss at step 00: 7.403731823\n",
            "Epoch 438/1000 completed \t - \tBatch loss: 0.001746728\n",
            "Epoch number : 438\n",
            "\tEpoch 439/1000: Loss at step 00: 7.419941902\n",
            "Epoch 439/1000 completed \t - \tBatch loss: 0.001565490\n",
            "Epoch number : 439\n",
            "\tEpoch 440/1000: Loss at step 00: 7.400558472\n",
            "Epoch 440/1000 completed \t - \tBatch loss: 0.001421114\n",
            "Epoch number : 440\n",
            "\tEpoch 441/1000: Loss at step 00: 7.386298180\n",
            "Epoch 441/1000 completed \t - \tBatch loss: 0.001431053\n",
            "Epoch number : 441\n",
            "\tEpoch 442/1000: Loss at step 00: 7.389944553\n",
            "Epoch 442/1000 completed \t - \tBatch loss: 0.001430593\n",
            "Epoch number : 442\n",
            "\tEpoch 443/1000: Loss at step 00: 7.414649010\n",
            "Epoch 443/1000 completed \t - \tBatch loss: 0.001392079\n",
            "Epoch number : 443\n",
            "\tEpoch 444/1000: Loss at step 00: 7.452873230\n",
            "Epoch 444/1000 completed \t - \tBatch loss: 0.001336806\n",
            "Epoch number : 444\n",
            "\tEpoch 445/1000: Loss at step 00: 7.507457733\n",
            "Epoch 445/1000 completed \t - \tBatch loss: 0.001308675\n",
            "Epoch number : 445\n",
            "\tEpoch 446/1000: Loss at step 00: 7.571146011\n",
            "Epoch 446/1000 completed \t - \tBatch loss: 0.001326991\n",
            "Epoch number : 446\n",
            "\tEpoch 447/1000: Loss at step 00: 7.637547493\n",
            "Epoch 447/1000 completed \t - \tBatch loss: 0.001389517\n",
            "Epoch number : 447\n",
            "\tEpoch 448/1000: Loss at step 00: 7.693139076\n",
            "Epoch 448/1000 completed \t - \tBatch loss: 0.001461023\n",
            "Epoch number : 448\n",
            "\tEpoch 449/1000: Loss at step 00: 7.726141930\n",
            "Epoch 449/1000 completed \t - \tBatch loss: 0.001484024\n",
            "Epoch number : 449\n",
            "\tEpoch 450/1000: Loss at step 00: 7.731950760\n",
            "Epoch 450/1000 completed \t - \tBatch loss: 0.001437981\n",
            "Epoch number : 450\n",
            "\tEpoch 451/1000: Loss at step 00: 7.710552216\n",
            "Epoch 451/1000 completed \t - \tBatch loss: 0.001351781\n",
            "Epoch number : 451\n",
            "\tEpoch 452/1000: Loss at step 00: 7.671555042\n",
            "Epoch 452/1000 completed \t - \tBatch loss: 0.001254070\n",
            "Epoch number : 452\n",
            "\tEpoch 453/1000: Loss at step 00: 7.625327587\n",
            "Epoch 453/1000 completed \t - \tBatch loss: 0.001169416\n",
            "Epoch number : 453\n",
            "\tEpoch 454/1000: Loss at step 00: 7.582966805\n",
            "Epoch 454/1000 completed \t - \tBatch loss: 0.001114499\n",
            "Epoch number : 454\n",
            "\tEpoch 455/1000: Loss at step 00: 7.550851345\n",
            "Epoch 455/1000 completed \t - \tBatch loss: 0.001100216\n",
            "Epoch number : 455\n",
            "\tEpoch 456/1000: Loss at step 00: 7.530100346\n",
            "Epoch 456/1000 completed \t - \tBatch loss: 0.001117183\n",
            "Epoch number : 456\n",
            "\tEpoch 457/1000: Loss at step 00: 7.519248962\n",
            "Epoch 457/1000 completed \t - \tBatch loss: 0.001140556\n",
            "Epoch number : 457\n",
            "\tEpoch 458/1000: Loss at step 00: 7.514626980\n",
            "Epoch 458/1000 completed \t - \tBatch loss: 0.001167643\n",
            "Epoch number : 458\n",
            "\tEpoch 459/1000: Loss at step 00: 7.513260365\n",
            "Epoch 459/1000 completed \t - \tBatch loss: 0.001202243\n",
            "Epoch number : 459\n",
            "\tEpoch 460/1000: Loss at step 00: 7.513867378\n",
            "Epoch 460/1000 completed \t - \tBatch loss: 0.001245294\n",
            "Epoch number : 460\n",
            "\tEpoch 461/1000: Loss at step 00: 7.515477657\n",
            "Epoch 461/1000 completed \t - \tBatch loss: 0.001268674\n",
            "Epoch number : 461\n",
            "\tEpoch 462/1000: Loss at step 00: 7.513503075\n",
            "Epoch 462/1000 completed \t - \tBatch loss: 0.001237187\n",
            "Epoch number : 462\n",
            "\tEpoch 463/1000: Loss at step 00: 7.506652832\n",
            "Epoch 463/1000 completed \t - \tBatch loss: 0.001178188\n",
            "Epoch number : 463\n",
            "\tEpoch 464/1000: Loss at step 00: 7.500856876\n",
            "Epoch 464/1000 completed \t - \tBatch loss: 0.001154199\n",
            "Epoch number : 464\n",
            "\tEpoch 465/1000: Loss at step 00: 7.503010750\n",
            "Epoch 465/1000 completed \t - \tBatch loss: 0.001160245\n",
            "Epoch number : 465\n",
            "\tEpoch 466/1000: Loss at step 00: 7.511913776\n",
            "Epoch 466/1000 completed \t - \tBatch loss: 0.001155273\n",
            "Epoch number : 466\n",
            "\tEpoch 467/1000: Loss at step 00: 7.530772209\n",
            "Epoch 467/1000 completed \t - \tBatch loss: 0.001127517\n",
            "Epoch number : 467\n",
            "\tEpoch 468/1000: Loss at step 00: 7.558354378\n",
            "Epoch 468/1000 completed \t - \tBatch loss: 0.001079952\n",
            "Epoch number : 468\n",
            "\tEpoch 469/1000: Loss at step 00: 7.595060825\n",
            "Epoch 469/1000 completed \t - \tBatch loss: 0.001026808\n",
            "Epoch number : 469\n",
            "\tEpoch 470/1000: Loss at step 00: 7.633485794\n",
            "Epoch 470/1000 completed \t - \tBatch loss: 0.000980147\n",
            "Epoch number : 470\n",
            "\tEpoch 471/1000: Loss at step 00: 7.670394421\n",
            "Epoch 471/1000 completed \t - \tBatch loss: 0.000964217\n",
            "Epoch number : 471\n",
            "\tEpoch 472/1000: Loss at step 00: 7.700571537\n",
            "Epoch 472/1000 completed \t - \tBatch loss: 0.000989901\n",
            "Epoch number : 472\n",
            "\tEpoch 473/1000: Loss at step 00: 7.724592209\n",
            "Epoch 473/1000 completed \t - \tBatch loss: 0.001054168\n",
            "Epoch number : 473\n",
            "\tEpoch 474/1000: Loss at step 00: 7.742363930\n",
            "Epoch 474/1000 completed \t - \tBatch loss: 0.001143784\n",
            "Epoch number : 474\n",
            "\tEpoch 475/1000: Loss at step 00: 7.756160736\n",
            "Epoch 475/1000 completed \t - \tBatch loss: 0.001217304\n",
            "Epoch number : 475\n",
            "\tEpoch 476/1000: Loss at step 00: 7.763251305\n",
            "Epoch 476/1000 completed \t - \tBatch loss: 0.001239294\n",
            "Epoch number : 476\n",
            "\tEpoch 477/1000: Loss at step 00: 7.766248226\n",
            "Epoch 477/1000 completed \t - \tBatch loss: 0.001212116\n",
            "Epoch number : 477\n",
            "\tEpoch 478/1000: Loss at step 00: 7.770902157\n",
            "Epoch 478/1000 completed \t - \tBatch loss: 0.001185143\n",
            "Epoch number : 478\n",
            "\tEpoch 479/1000: Loss at step 00: 7.777893066\n",
            "Epoch 479/1000 completed \t - \tBatch loss: 0.001184597\n",
            "Epoch number : 479\n",
            "\tEpoch 480/1000: Loss at step 00: 7.781528473\n",
            "Epoch 480/1000 completed \t - \tBatch loss: 0.001184081\n",
            "Epoch number : 480\n",
            "\tEpoch 481/1000: Loss at step 00: 7.772815704\n",
            "Epoch 481/1000 completed \t - \tBatch loss: 0.001138418\n",
            "Epoch number : 481\n",
            "\tEpoch 482/1000: Loss at step 00: 7.748648167\n",
            "Epoch 482/1000 completed \t - \tBatch loss: 0.001050790\n",
            "Epoch number : 482\n",
            "\tEpoch 483/1000: Loss at step 00: 7.712285042\n",
            "Epoch 483/1000 completed \t - \tBatch loss: 0.000969441\n",
            "Epoch number : 483\n",
            "\tEpoch 484/1000: Loss at step 00: 7.669387341\n",
            "Epoch 484/1000 completed \t - \tBatch loss: 0.000925621\n",
            "Epoch number : 484\n",
            "\tEpoch 485/1000: Loss at step 00: 7.626687527\n",
            "Epoch 485/1000 completed \t - \tBatch loss: 0.000924079\n",
            "Epoch number : 485\n",
            "\tEpoch 486/1000: Loss at step 00: 7.591450691\n",
            "Epoch 486/1000 completed \t - \tBatch loss: 0.000957500\n",
            "Epoch number : 486\n",
            "\tEpoch 487/1000: Loss at step 00: 7.570651054\n",
            "Epoch 487/1000 completed \t - \tBatch loss: 0.001005157\n",
            "Epoch number : 487\n",
            "\tEpoch 488/1000: Loss at step 00: 7.565008163\n",
            "Epoch 488/1000 completed \t - \tBatch loss: 0.001040112\n",
            "Epoch number : 488\n",
            "\tEpoch 489/1000: Loss at step 00: 7.575734138\n",
            "Epoch 489/1000 completed \t - \tBatch loss: 0.001034871\n",
            "Epoch number : 489\n",
            "\tEpoch 490/1000: Loss at step 00: 7.600474358\n",
            "Epoch 490/1000 completed \t - \tBatch loss: 0.000989536\n",
            "Epoch number : 490\n",
            "\tEpoch 491/1000: Loss at step 00: 7.634386539\n",
            "Epoch 491/1000 completed \t - \tBatch loss: 0.000928850\n",
            "Epoch number : 491\n",
            "\tEpoch 492/1000: Loss at step 00: 7.669701099\n",
            "Epoch 492/1000 completed \t - \tBatch loss: 0.000876089\n",
            "Epoch number : 492\n",
            "\tEpoch 493/1000: Loss at step 00: 7.699192524\n",
            "Epoch 493/1000 completed \t - \tBatch loss: 0.000836291\n",
            "Epoch number : 493\n",
            "\tEpoch 494/1000: Loss at step 00: 7.718439102\n",
            "Epoch 494/1000 completed \t - \tBatch loss: 0.000808201\n",
            "Epoch number : 494\n",
            "\tEpoch 495/1000: Loss at step 00: 7.726193428\n",
            "Epoch 495/1000 completed \t - \tBatch loss: 0.000792927\n",
            "Epoch number : 495\n",
            "\tEpoch 496/1000: Loss at step 00: 7.724521637\n",
            "Epoch 496/1000 completed \t - \tBatch loss: 0.000791525\n",
            "Epoch number : 496\n",
            "\tEpoch 497/1000: Loss at step 00: 7.717318535\n",
            "Epoch 497/1000 completed \t - \tBatch loss: 0.000802484\n",
            "Epoch number : 497\n",
            "\tEpoch 498/1000: Loss at step 00: 7.707692146\n",
            "Epoch 498/1000 completed \t - \tBatch loss: 0.000819719\n",
            "Epoch number : 498\n",
            "\tEpoch 499/1000: Loss at step 00: 7.699718475\n",
            "Epoch 499/1000 completed \t - \tBatch loss: 0.000834273\n",
            "Epoch number : 499\n",
            "\tEpoch 500/1000: Loss at step 00: 7.696949482\n",
            "Epoch 500/1000 completed \t - \tBatch loss: 0.000840473\n",
            "Final loss for training  set: 0.000840473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cRkMl3vwUwSl",
        "colab_type": "code",
        "outputId": "ffe2517c-b306-4944-ebdf-27a7e1eb6743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(t_loss_history)\n",
        "\n",
        "plt.ylabel('train loss value')\n",
        "plt.xlabel('train batches')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJxIzMxsQP4/qLslcrtWhYUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JI-AoqN3vhE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loss = []\n",
        "for train_epoch_num, train_epoch in enumerate(generate_epoch(train_X, train_y, 1, 128)):\n",
        "  for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n",
        "    loss = 0\n",
        "    p = fw(batch_X, batch_y)\n",
        "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p, labels= batch_y))\n",
        "    bloss = (loss/128)\n",
        "  test_loss.append(bloss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WqyQ8C0r1yg7",
        "colab_type": "code",
        "outputId": "8599e4a1-139d-4e64-9ff1-10214eba8006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(test_loss)\n",
        "\n",
        "plt.ylabel('test loss value')\n",
        "plt.xlabel('test batches')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEKCAYAAACsUXomAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=4942641, shape=(), dtype=float32, numpy=6.5760664e-06>]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}