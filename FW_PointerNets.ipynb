{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FW-PointerNets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FC0d2HChJIWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers import LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "#import tensorflow.contrib.eager as tfe\n",
        "import numpy as np \n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2Zjx0dbLqJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(Encoder, self).__init__()\n",
        "\t\tself.encoder = LSTM(hidden_size, return_sequence = True, return_state = True)\n",
        "\n",
        "\tdef call(self, x):\n",
        "\t\te, state_h, state_c  = self.encoder(x)        \n",
        "\t\treturn e, [state_h, state_c]\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(Decoder, self).__init__()\n",
        "\t\tself.decoder = LSTM(hidden_size, return_sequence = True, return_state = True)\n",
        "\n",
        "\tdef call(self, x, hidden_states):\n",
        "\t\td, state_h, state_c  = self.decoder(x, initial_state=hidden_states)\n",
        "\t\treturn d, [state_h, state_c]\n",
        "\n",
        "class PtrnetLSTM(tf.keras.Model):\n",
        "\tdef __init__(self, hidden_size=512):\n",
        "\t\tsuper(PtrnetLSTM, self).__init__()\n",
        "\t\t#self.W1 = tfe.variable(tf.random_uniform([hidden_size, hidden_size], -0.08, 0.08), dtype=float32)\n",
        "\t\tself.W1 = tf.keras.layers.Dense(hidden_size, kernel_initializer= tf.keras.initializers.RandomUniform(minval = -0.08, maxval = 0.08, seed = None), use_bias=False)\n",
        "\t\tself.W2 = tf.keras.layers.Dense(hidden_size, kernel_initializer= tf.keras.initializers.RandomUniform(minval = -0.08, maxval = 0.08, seed = None), use_bias=False)\n",
        "\t\t#Dense layer -> dot(input, kernel) -> so now Ui = vT . tanh(W1 . e + W2 . di)  becomes Ui = tanh(e . W1 + di . W2) . v\n",
        "\t\tself.VT = tf.keras.layers.Dense(1, use_bias=False)\n",
        "\t# e= encoder output and d = decoder output\n",
        "\tdef call(self, e,d):\n",
        "\t\tu = self.VT(tf.nn.tanh(self.W1(e) + self.W2(d)))\n",
        "\t\tattention = tf.nn.softmax(u, axis = 1)\n",
        "\n",
        "\t\treturn tf.reshape(attention, attention.shape[0], attention.shape[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsMqoxxoEyPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class fastweights(tf.keras.Model):\n",
        "\tdef __init__(self, input_dim, elemnum , batch_size=128, decay_rate = 0.9, learning_rate = 0.5, hidden_size=512):\n",
        "\t\tsuper(fastweights, self).__init__()\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.DR = decay_rate\n",
        "\t\tself.LR = learning_rate\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.W_x = tf.Variable(tf.random_uniform([elemnum, hidden_size], -np.sqrt(2/elemnum), np.sqrt(2/elemnum)), dtype=tf.float32)\n",
        "\t\tself.B_x = tf.Variable(tf.zeros(hidden_size), dtype=tf.float32)\n",
        "\t\tself.W_h = tf.Variable(initial_value = 0.5 * np.identity(hidden_size), dtype = tf.float32)\n",
        "\t\tself.W_y = tf.Variable(tf.random_uniform([hidden_size, elemnum], -np.sqrt(2/hidden_size), np.sqrt(2/hidden_size)), dtype = tf.float32)\n",
        "\t\tself.B_y = tf.Variable(tf.zeros(elemnum), dtype= tf.float32)\n",
        "\t\tself.scale = tf.Variable(tf.ones(hidden_size), dtype = tf.float32)\n",
        "\t\tself.shift = tf.Variable(tf.zeros(hidden_size), dtype = tf.float32) \n",
        "\t\t#initial values of A and H matricies\n",
        "\t\tself.A = tf.zeros([self.batch_size, self.hidden_size,self.hidden_size], dtype = tf.float32)\n",
        "\t\tself.H = tf.zeros([self.batch_size, self.hidden_size], dtype = tf.float32)\n",
        "    \n",
        "\tdef call(self,X, S=1):\n",
        "\t\tX = tf.cast(X, tf.float32)\n",
        "\t\tfor t in range(tf.shape(X)[1]):\n",
        "\t\t\t#first hidden state, A and H_s are  zero at this point so the part A(t)H_s(t+1) becomes zero\n",
        "\t\t\tself.H = tf.nn.relu((tf.matmul(self.H,self.W_h))+(tf.matmul(X[:, t, :],self.W_x)+self.B_x))\n",
        "\t\t\t#reshaping to use it with A, to calculate the A(t)H_s(t+1)\n",
        "\t\t\tH_s = tf.reshape(self.H, [self.batch_size, 1, self.hidden_size])\n",
        "\t\t\t#Initial A for this particular time step: A(t) = decay*A(t-1)+ learning*h(t)h(t).T\n",
        "\t\t\t#self.A = tf.add((tf.scalar_mul(self.DR, self.A)),(tf.batch_matmul(tf.transpose(self.H_s, [0,2,1]),self.H_s)))\n",
        "\t\t\tself.A = (tf.scalar_mul(self.DR, self.A))+ tf.scalar_mul(self.LR,(tf.matmul(tf.transpose(H_s, [0,2,1]),H_s)))\n",
        "\t\t\t#inner loop for fast weights, tfor S steps\n",
        "\t\t\tfor _ in range(S):\n",
        "\t\t\t\t#calculating H_s without the non linearity first, so we can use linear normalization \n",
        "\t\t\t\tH_s = tf.reshape(tf.matmul(self.H,self.W_h),tf.shape(H_s)) + tf.reshape(tf.matmul(X[:,t,:],self.W_x)+self.B_x,tf.shape(H_s)) + tf.matmul(H_s, self.A)\n",
        "\t\t\t\t#Applying Layer Normalization \n",
        "\t\t\t\tmean, var = tf.nn.moments(H_s, axes =2, keep_dims = True)\n",
        "\t\t\t\tH_s = (self.scale*(H_s - mean))/(tf.sqrt(var + 1e-5) + self.shift)\n",
        "\t\t\t\t#applying non linearity\n",
        "\t\t\t\tH_s = tf.nn.relu(H_s)\n",
        "\t\t\tself.H = tf.reshape(H_s,[self.batch_size, self.hidden_size])\n",
        "\t\tfinallayer = tf.matmul(self.H, self.W_y) + self.B_y\n",
        "\t\treturn finallayer, self.H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6CLG6-wx5m4P",
        "colab_type": "code",
        "outputId": "cd355b77-5c1b-463d-9b3e-465bcfea6b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def get_three_letters():\n",
        "    \"\"\"\n",
        "    Retrieve three random letters (a-z)\n",
        "    without replacement.\n",
        "    \"\"\"\n",
        "    return np.random.choice(range(0,26), 3, replace=False)\n",
        "\n",
        "def get_three_numbers():\n",
        "    \"\"\"\n",
        "    Retrieve three random numbers (0-9)\n",
        "    with replacement.\n",
        "    \"\"\"\n",
        "    return np.random.choice(range(26, 26+10), 3, replace=True)\n",
        "\n",
        "def create_sequence():\n",
        "    \"\"\"\n",
        "    Concatenate keys and values with\n",
        "    ?? and one of the keys.\n",
        "    Returns the input and output.\n",
        "    \"\"\"\n",
        "    letters = get_three_letters()\n",
        "    numbers = get_three_numbers()\n",
        "    X = np.zeros((9))\n",
        "    y = np.zeros((1))\n",
        "    for i in range(0, 5, 2):\n",
        "        X[i] = letters[int(i/2)]\n",
        "        X[i+1] = numbers[int(i/2)]\n",
        "\n",
        "    # append ??\n",
        "    X[6] = 26+10\n",
        "    X[7] = 26+10\n",
        "\n",
        "    # last key and respective value (y)\n",
        "    index = np.random.choice(range(0,3), 1, replace=False)\n",
        "    X[8] = letters[index]\n",
        "    y = numbers[index]\n",
        "\n",
        "    # one hot encode X and y\n",
        "    X_one_hot = np.eye(26+10+1)[np.array(X).astype('int')]\n",
        "    y_one_hot = np.eye(26+10+1)[y][0]\n",
        "\n",
        "    return X_one_hot, y_one_hot\n",
        "\n",
        "def ordinal_to_alpha(sequence):\n",
        "    \"\"\"\n",
        "    Convert from ordinal to alpha-numeric representations.\n",
        "    Just for funsies :)\n",
        "    \"\"\"\n",
        "    corpus = ['a','b','c','d','e','f','g','h','i','j','k','l',\n",
        "              'm','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, '?']\n",
        "\n",
        "    conversion = \"\"\n",
        "    for item in sequence:\n",
        "        conversion += str(corpus[int(item)])\n",
        "    return conversion\n",
        "\n",
        "def create_data(num_samples):\n",
        "    \"\"\"\n",
        "    Create a num_samples long set of X and y.\n",
        "    \"\"\"\n",
        "    X = np.zeros([num_samples, 9, 26+10+1], dtype=np.int32)\n",
        "    y = np.zeros([num_samples, 26+10+1], dtype=np.int32)\n",
        "    for i in range(num_samples):\n",
        "        X[i], y[i] = create_sequence()\n",
        "    return X, y\n",
        "\n",
        "def generate_epoch(X, y, num_epochs, batch_size):\n",
        "\n",
        "    for epoch_num in range(num_epochs):\n",
        "        yield generate_batch(X, y, batch_size)\n",
        "\n",
        "def generate_batch(X, y, batch_size):\n",
        "\n",
        "    data_size = len(X)\n",
        "\n",
        "    num_batches = (data_size // batch_size)\n",
        "    for batch_num in range(num_batches):\n",
        "        start_index = batch_num * batch_size\n",
        "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "        yield X[start_index:end_index], y[start_index:end_index]\n",
        "\n",
        "# Sampling\n",
        "sample_X, sample_y = create_sequence()\n",
        "print (\"Sample:\", ordinal_to_alpha([np.argmax(X) for X in sample_X]), ordinal_to_alpha([np.argmax(sample_y)]))\n",
        "# Train/valid sets\n",
        "train_X, train_y = create_data(640)\n",
        "print (\"train_X:\", np.shape(train_X), \",train_y:\", np.shape(train_y))\n",
        "valid_X, valid_y = create_data(384)\n",
        "print (\"valid_X:\", np.shape(valid_X), \",valid_y:\", np.shape(valid_y))\n",
        "test_X, test_y = create_data(384)\n",
        "print (\"test_X:\", np.shape(test_X), \",test_y:\", np.shape(test_y))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample: u6y4j1??j 1\n",
            "train_X: (640, 9, 37) ,train_y: (640, 37)\n",
            "valid_X: (384, 9, 37) ,valid_y: (384, 37)\n",
            "test_X: (384, 9, 37) ,test_y: (384, 37)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7-KsQD4O_o8-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = tf.Variable(0.0, trainable = False)\n",
        "optimizer = tf.train.AdamOptimizer(0.1)\n",
        "inputdim = 9\n",
        "elem = 37\n",
        "fw = fastweights(inputdim, elem, 128, 0.9, 0.5, 512)\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=fw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qj1MkIxzP4NG",
        "colab_type": "code",
        "outputId": "ba17b2d0-ec68-46a2-efe4-5f2954d79dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17105
        }
      },
      "cell_type": "code",
      "source": [
        "t_loss_history = []\n",
        "  \n",
        "for train_epoch_num, train_epoch in enumerate(generate_epoch(train_X, train_y, 500, 128)):\n",
        "  print(\"Epoch number :\", train_epoch_num)\n",
        "  for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred_y = fw(batch_X, batch_y, 1)\n",
        "      loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y))\n",
        "        \n",
        "    batch_loss = (loss/128)\n",
        "  #if train_batch_num %10 ==0:\n",
        "  #print(\"\\tEpoch {:03d}/{:03d}: Loss at step {:02d}: {:.9f}\".format((train_epoch_num+1), 1000, train_batch_num, tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y)) ))\n",
        "    t_loss_history.append(batch_loss.numpy())\n",
        "    variables = fw.variables\n",
        "    grads, norm =tf.clip_by_global_norm(tape.gradient(loss, variables), 0.25) \n",
        "    optimizer.apply_gradients(zip(grads, variables), global_step=tf.train.get_or_create_global_step())\n",
        "  print(\"Epoch {:03d}/{:03d} completed \\t - \\tBatch loss: {:.9f}\".format((train_epoch_num+1), 1000, tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y)) ))\n",
        "#tf.contrib.eager.Saver(variables).save('trained.ckpt')\n",
        "checkpoint.save('trained.ckpt')\n",
        "print(\"Final loss for training  set: {:.9f}\".format(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_y, labels= batch_y))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number : 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Epoch 001/1000 completed \t - \tBatch loss: 4.029645443\n",
            "Epoch number : 1\n",
            "Epoch 002/1000 completed \t - \tBatch loss: 7.132364273\n",
            "Epoch number : 2\n",
            "Epoch 003/1000 completed \t - \tBatch loss: 14.067187309\n",
            "Epoch number : 3\n",
            "Epoch 004/1000 completed \t - \tBatch loss: 19.636137009\n",
            "Epoch number : 4\n",
            "Epoch 005/1000 completed \t - \tBatch loss: 23.782295227\n",
            "Epoch number : 5\n",
            "Epoch 006/1000 completed \t - \tBatch loss: 27.654804230\n",
            "Epoch number : 6\n",
            "Epoch 007/1000 completed \t - \tBatch loss: 31.752246857\n",
            "Epoch number : 7\n",
            "Epoch 008/1000 completed \t - \tBatch loss: 31.392410278\n",
            "Epoch number : 8\n",
            "Epoch 009/1000 completed \t - \tBatch loss: 33.001548767\n",
            "Epoch number : 9\n",
            "Epoch 010/1000 completed \t - \tBatch loss: 25.920097351\n",
            "Epoch number : 10\n",
            "Epoch 011/1000 completed \t - \tBatch loss: 25.559238434\n",
            "Epoch number : 11\n",
            "Epoch 012/1000 completed \t - \tBatch loss: 13.133987427\n",
            "Epoch number : 12\n",
            "Epoch 013/1000 completed \t - \tBatch loss: 10.362925529\n",
            "Epoch number : 13\n",
            "Epoch 014/1000 completed \t - \tBatch loss: 8.845339775\n",
            "Epoch number : 14\n",
            "Epoch 015/1000 completed \t - \tBatch loss: 7.937438011\n",
            "Epoch number : 15\n",
            "Epoch 016/1000 completed \t - \tBatch loss: 7.381336212\n",
            "Epoch number : 16\n",
            "Epoch 017/1000 completed \t - \tBatch loss: 6.506069660\n",
            "Epoch number : 17\n",
            "Epoch 018/1000 completed \t - \tBatch loss: 4.857667446\n",
            "Epoch number : 18\n",
            "Epoch 019/1000 completed \t - \tBatch loss: 2.922487497\n",
            "Epoch number : 19\n",
            "Epoch 020/1000 completed \t - \tBatch loss: 2.890507221\n",
            "Epoch number : 20\n",
            "Epoch 021/1000 completed \t - \tBatch loss: 2.645602226\n",
            "Epoch number : 21\n",
            "Epoch 022/1000 completed \t - \tBatch loss: 2.441868782\n",
            "Epoch number : 22\n",
            "Epoch 023/1000 completed \t - \tBatch loss: 2.297691822\n",
            "Epoch number : 23\n",
            "Epoch 024/1000 completed \t - \tBatch loss: 2.196032524\n",
            "Epoch number : 24\n",
            "Epoch 025/1000 completed \t - \tBatch loss: 2.200995684\n",
            "Epoch number : 25\n",
            "Epoch 026/1000 completed \t - \tBatch loss: 2.144354343\n",
            "Epoch number : 26\n",
            "Epoch 027/1000 completed \t - \tBatch loss: 2.075927734\n",
            "Epoch number : 27\n",
            "Epoch 028/1000 completed \t - \tBatch loss: 2.138591528\n",
            "Epoch number : 28\n",
            "Epoch 029/1000 completed \t - \tBatch loss: 1.993498087\n",
            "Epoch number : 29\n",
            "Epoch 030/1000 completed \t - \tBatch loss: 1.971667290\n",
            "Epoch number : 30\n",
            "Epoch 031/1000 completed \t - \tBatch loss: 1.871322751\n",
            "Epoch number : 31\n",
            "Epoch 032/1000 completed \t - \tBatch loss: 1.807520270\n",
            "Epoch number : 32\n",
            "Epoch 033/1000 completed \t - \tBatch loss: 1.740869284\n",
            "Epoch number : 33\n",
            "Epoch 034/1000 completed \t - \tBatch loss: 1.843400002\n",
            "Epoch number : 34\n",
            "Epoch 035/1000 completed \t - \tBatch loss: 1.691479087\n",
            "Epoch number : 35\n",
            "Epoch 036/1000 completed \t - \tBatch loss: 1.597851992\n",
            "Epoch number : 36\n",
            "Epoch 037/1000 completed \t - \tBatch loss: 1.622166276\n",
            "Epoch number : 37\n",
            "Epoch 038/1000 completed \t - \tBatch loss: 1.514390469\n",
            "Epoch number : 38\n",
            "Epoch 039/1000 completed \t - \tBatch loss: 1.423163533\n",
            "Epoch number : 39\n",
            "Epoch 040/1000 completed \t - \tBatch loss: 1.474725962\n",
            "Epoch number : 40\n",
            "Epoch 041/1000 completed \t - \tBatch loss: 1.322583675\n",
            "Epoch number : 41\n",
            "Epoch 042/1000 completed \t - \tBatch loss: 1.649644613\n",
            "Epoch number : 42\n",
            "Epoch 043/1000 completed \t - \tBatch loss: 1.622834325\n",
            "Epoch number : 43\n",
            "Epoch 044/1000 completed \t - \tBatch loss: 1.334544420\n",
            "Epoch number : 44\n",
            "Epoch 045/1000 completed \t - \tBatch loss: 1.121008039\n",
            "Epoch number : 45\n",
            "Epoch 046/1000 completed \t - \tBatch loss: 1.180829167\n",
            "Epoch number : 46\n",
            "Epoch 047/1000 completed \t - \tBatch loss: 1.052269459\n",
            "Epoch number : 47\n",
            "Epoch 048/1000 completed \t - \tBatch loss: 1.040915370\n",
            "Epoch number : 48\n",
            "Epoch 049/1000 completed \t - \tBatch loss: 0.982517064\n",
            "Epoch number : 49\n",
            "Epoch 050/1000 completed \t - \tBatch loss: 1.278472543\n",
            "Epoch number : 50\n",
            "Epoch 051/1000 completed \t - \tBatch loss: 1.496369123\n",
            "Epoch number : 51\n",
            "Epoch 052/1000 completed \t - \tBatch loss: 1.439202547\n",
            "Epoch number : 52\n",
            "Epoch 053/1000 completed \t - \tBatch loss: 1.188697577\n",
            "Epoch number : 53\n",
            "Epoch 054/1000 completed \t - \tBatch loss: 1.083415985\n",
            "Epoch number : 54\n",
            "Epoch 055/1000 completed \t - \tBatch loss: 0.942236423\n",
            "Epoch number : 55\n",
            "Epoch 056/1000 completed \t - \tBatch loss: 0.807085872\n",
            "Epoch number : 56\n",
            "Epoch 057/1000 completed \t - \tBatch loss: 0.992325187\n",
            "Epoch number : 57\n",
            "Epoch 058/1000 completed \t - \tBatch loss: 0.752839684\n",
            "Epoch number : 58\n",
            "Epoch 059/1000 completed \t - \tBatch loss: 0.594358683\n",
            "Epoch number : 59\n",
            "Epoch 060/1000 completed \t - \tBatch loss: 0.591653943\n",
            "Epoch number : 60\n",
            "Epoch 061/1000 completed \t - \tBatch loss: 0.539018095\n",
            "Epoch number : 61\n",
            "Epoch 062/1000 completed \t - \tBatch loss: 0.739031613\n",
            "Epoch number : 62\n",
            "Epoch 063/1000 completed \t - \tBatch loss: 0.596824884\n",
            "Epoch number : 63\n",
            "Epoch 064/1000 completed \t - \tBatch loss: 0.778775930\n",
            "Epoch number : 64\n",
            "Epoch 065/1000 completed \t - \tBatch loss: 0.760459363\n",
            "Epoch number : 65\n",
            "Epoch 066/1000 completed \t - \tBatch loss: 0.892904520\n",
            "Epoch number : 66\n",
            "Epoch 067/1000 completed \t - \tBatch loss: 0.600829065\n",
            "Epoch number : 67\n",
            "Epoch 068/1000 completed \t - \tBatch loss: 0.504341364\n",
            "Epoch number : 68\n",
            "Epoch 069/1000 completed \t - \tBatch loss: 0.441751420\n",
            "Epoch number : 69\n",
            "Epoch 070/1000 completed \t - \tBatch loss: 0.523293138\n",
            "Epoch number : 70\n",
            "Epoch 071/1000 completed \t - \tBatch loss: 0.530623913\n",
            "Epoch number : 71\n",
            "Epoch 072/1000 completed \t - \tBatch loss: 0.406669915\n",
            "Epoch number : 72\n",
            "Epoch 073/1000 completed \t - \tBatch loss: 0.429119527\n",
            "Epoch number : 73\n",
            "Epoch 074/1000 completed \t - \tBatch loss: 0.282386124\n",
            "Epoch number : 74\n",
            "Epoch 075/1000 completed \t - \tBatch loss: 0.195376962\n",
            "Epoch number : 75\n",
            "Epoch 076/1000 completed \t - \tBatch loss: 0.215646774\n",
            "Epoch number : 76\n",
            "Epoch 077/1000 completed \t - \tBatch loss: 0.177672818\n",
            "Epoch number : 77\n",
            "Epoch 078/1000 completed \t - \tBatch loss: 0.164546102\n",
            "Epoch number : 78\n",
            "Epoch 079/1000 completed \t - \tBatch loss: 0.125279576\n",
            "Epoch number : 79\n",
            "Epoch 080/1000 completed \t - \tBatch loss: 0.169824630\n",
            "Epoch number : 80\n",
            "Epoch 081/1000 completed \t - \tBatch loss: 0.145268530\n",
            "Epoch number : 81\n",
            "Epoch 082/1000 completed \t - \tBatch loss: 0.131200820\n",
            "Epoch number : 82\n",
            "Epoch 083/1000 completed \t - \tBatch loss: 0.093237102\n",
            "Epoch number : 83\n",
            "Epoch 084/1000 completed \t - \tBatch loss: 0.115575358\n",
            "Epoch number : 84\n",
            "Epoch 085/1000 completed \t - \tBatch loss: 0.202536792\n",
            "Epoch number : 85\n",
            "Epoch 086/1000 completed \t - \tBatch loss: 0.179313943\n",
            "Epoch number : 86\n",
            "Epoch 087/1000 completed \t - \tBatch loss: 0.601122975\n",
            "Epoch number : 87\n",
            "Epoch 088/1000 completed \t - \tBatch loss: 0.709783256\n",
            "Epoch number : 88\n",
            "Epoch 089/1000 completed \t - \tBatch loss: 0.334936976\n",
            "Epoch number : 89\n",
            "Epoch 090/1000 completed \t - \tBatch loss: 0.329866052\n",
            "Epoch number : 90\n",
            "Epoch 091/1000 completed \t - \tBatch loss: 0.256128222\n",
            "Epoch number : 91\n",
            "Epoch 092/1000 completed \t - \tBatch loss: 0.505778253\n",
            "Epoch number : 92\n",
            "Epoch 093/1000 completed \t - \tBatch loss: 0.276278526\n",
            "Epoch number : 93\n",
            "Epoch 094/1000 completed \t - \tBatch loss: 0.198566332\n",
            "Epoch number : 94\n",
            "Epoch 095/1000 completed \t - \tBatch loss: 0.211214632\n",
            "Epoch number : 95\n",
            "Epoch 096/1000 completed \t - \tBatch loss: 0.157035217\n",
            "Epoch number : 96\n",
            "Epoch 097/1000 completed \t - \tBatch loss: 0.210594609\n",
            "Epoch number : 97\n",
            "Epoch 098/1000 completed \t - \tBatch loss: 0.115524910\n",
            "Epoch number : 98\n",
            "Epoch 099/1000 completed \t - \tBatch loss: 0.085232511\n",
            "Epoch number : 99\n",
            "Epoch 100/1000 completed \t - \tBatch loss: 0.047575057\n",
            "Epoch number : 100\n",
            "Epoch 101/1000 completed \t - \tBatch loss: 0.042592697\n",
            "Epoch number : 101\n",
            "Epoch 102/1000 completed \t - \tBatch loss: 0.045541525\n",
            "Epoch number : 102\n",
            "Epoch 103/1000 completed \t - \tBatch loss: 0.039892614\n",
            "Epoch number : 103\n",
            "Epoch 104/1000 completed \t - \tBatch loss: 0.032333631\n",
            "Epoch number : 104\n",
            "Epoch 105/1000 completed \t - \tBatch loss: 0.022887181\n",
            "Epoch number : 105\n",
            "Epoch 106/1000 completed \t - \tBatch loss: 0.020693483\n",
            "Epoch number : 106\n",
            "Epoch 107/1000 completed \t - \tBatch loss: 0.021620475\n",
            "Epoch number : 107\n",
            "Epoch 108/1000 completed \t - \tBatch loss: 0.021468248\n",
            "Epoch number : 108\n",
            "Epoch 109/1000 completed \t - \tBatch loss: 0.024139959\n",
            "Epoch number : 109\n",
            "Epoch 110/1000 completed \t - \tBatch loss: 0.015184957\n",
            "Epoch number : 110\n",
            "Epoch 111/1000 completed \t - \tBatch loss: 0.017669179\n",
            "Epoch number : 111\n",
            "Epoch 112/1000 completed \t - \tBatch loss: 0.018768832\n",
            "Epoch number : 112\n",
            "Epoch 113/1000 completed \t - \tBatch loss: 0.012360681\n",
            "Epoch number : 113\n",
            "Epoch 114/1000 completed \t - \tBatch loss: 0.013992107\n",
            "Epoch number : 114\n",
            "Epoch 115/1000 completed \t - \tBatch loss: 0.014141067\n",
            "Epoch number : 115\n",
            "Epoch 116/1000 completed \t - \tBatch loss: 0.009721585\n",
            "Epoch number : 116\n",
            "Epoch 117/1000 completed \t - \tBatch loss: 0.007643216\n",
            "Epoch number : 117\n",
            "Epoch 118/1000 completed \t - \tBatch loss: 0.007215998\n",
            "Epoch number : 118\n",
            "Epoch 119/1000 completed \t - \tBatch loss: 0.007947782\n",
            "Epoch number : 119\n",
            "Epoch 120/1000 completed \t - \tBatch loss: 0.021398071\n",
            "Epoch number : 120\n",
            "Epoch 121/1000 completed \t - \tBatch loss: 0.024911560\n",
            "Epoch number : 121\n",
            "Epoch 122/1000 completed \t - \tBatch loss: 0.020842588\n",
            "Epoch number : 122\n",
            "Epoch 123/1000 completed \t - \tBatch loss: 0.010480050\n",
            "Epoch number : 123\n",
            "Epoch 124/1000 completed \t - \tBatch loss: 0.011601415\n",
            "Epoch number : 124\n",
            "Epoch 125/1000 completed \t - \tBatch loss: 0.013633602\n",
            "Epoch number : 125\n",
            "Epoch 126/1000 completed \t - \tBatch loss: 0.010462924\n",
            "Epoch number : 126\n",
            "Epoch 127/1000 completed \t - \tBatch loss: 0.008815899\n",
            "Epoch number : 127\n",
            "Epoch 128/1000 completed \t - \tBatch loss: 0.007732174\n",
            "Epoch number : 128\n",
            "Epoch 129/1000 completed \t - \tBatch loss: 0.007072655\n",
            "Epoch number : 129\n",
            "Epoch 130/1000 completed \t - \tBatch loss: 0.006935700\n",
            "Epoch number : 130\n",
            "Epoch 131/1000 completed \t - \tBatch loss: 0.006616753\n",
            "Epoch number : 131\n",
            "Epoch 132/1000 completed \t - \tBatch loss: 0.006008930\n",
            "Epoch number : 132\n",
            "Epoch 133/1000 completed \t - \tBatch loss: 0.004458573\n",
            "Epoch number : 133\n",
            "Epoch 134/1000 completed \t - \tBatch loss: 0.003581983\n",
            "Epoch number : 134\n",
            "Epoch 135/1000 completed \t - \tBatch loss: 0.003287323\n",
            "Epoch number : 135\n",
            "Epoch 136/1000 completed \t - \tBatch loss: 0.003284459\n",
            "Epoch number : 136\n",
            "Epoch 137/1000 completed \t - \tBatch loss: 0.003318854\n",
            "Epoch number : 137\n",
            "Epoch 138/1000 completed \t - \tBatch loss: 0.003314408\n",
            "Epoch number : 138\n",
            "Epoch 139/1000 completed \t - \tBatch loss: 0.003349170\n",
            "Epoch number : 139\n",
            "Epoch 140/1000 completed \t - \tBatch loss: 0.003295642\n",
            "Epoch number : 140\n",
            "Epoch 141/1000 completed \t - \tBatch loss: 0.003124903\n",
            "Epoch number : 141\n",
            "Epoch 142/1000 completed \t - \tBatch loss: 0.002981012\n",
            "Epoch number : 142\n",
            "Epoch 143/1000 completed \t - \tBatch loss: 0.002959362\n",
            "Epoch number : 143\n",
            "Epoch 144/1000 completed \t - \tBatch loss: 0.003040453\n",
            "Epoch number : 144\n",
            "Epoch 145/1000 completed \t - \tBatch loss: 0.003293284\n",
            "Epoch number : 145\n",
            "Epoch 146/1000 completed \t - \tBatch loss: 0.003558433\n",
            "Epoch number : 146\n",
            "Epoch 147/1000 completed \t - \tBatch loss: 0.003466216\n",
            "Epoch number : 147\n",
            "Epoch 148/1000 completed \t - \tBatch loss: 0.003209724\n",
            "Epoch number : 148\n",
            "Epoch 149/1000 completed \t - \tBatch loss: 0.003165056\n",
            "Epoch number : 149\n",
            "Epoch 150/1000 completed \t - \tBatch loss: 0.003333984\n",
            "Epoch number : 150\n",
            "Epoch 151/1000 completed \t - \tBatch loss: 0.003207831\n",
            "Epoch number : 151\n",
            "Epoch 152/1000 completed \t - \tBatch loss: 0.002989749\n",
            "Epoch number : 152\n",
            "Epoch 153/1000 completed \t - \tBatch loss: 0.003125813\n",
            "Epoch number : 153\n",
            "Epoch 154/1000 completed \t - \tBatch loss: 0.003376876\n",
            "Epoch number : 154\n",
            "Epoch 155/1000 completed \t - \tBatch loss: 0.003435061\n",
            "Epoch number : 155\n",
            "Epoch 156/1000 completed \t - \tBatch loss: 0.003522995\n",
            "Epoch number : 156\n",
            "Epoch 157/1000 completed \t - \tBatch loss: 0.003600025\n",
            "Epoch number : 157\n",
            "Epoch 158/1000 completed \t - \tBatch loss: 0.003568943\n",
            "Epoch number : 158\n",
            "Epoch 159/1000 completed \t - \tBatch loss: 0.003691989\n",
            "Epoch number : 159\n",
            "Epoch 160/1000 completed \t - \tBatch loss: 0.004171613\n",
            "Epoch number : 160\n",
            "Epoch 161/1000 completed \t - \tBatch loss: 0.004686873\n",
            "Epoch number : 161\n",
            "Epoch 162/1000 completed \t - \tBatch loss: 0.004363807\n",
            "Epoch number : 162\n",
            "Epoch 163/1000 completed \t - \tBatch loss: 0.004544021\n",
            "Epoch number : 163\n",
            "Epoch 164/1000 completed \t - \tBatch loss: 0.005194503\n",
            "Epoch number : 164\n",
            "Epoch 165/1000 completed \t - \tBatch loss: 0.005758426\n",
            "Epoch number : 165\n",
            "Epoch 166/1000 completed \t - \tBatch loss: 0.004915267\n",
            "Epoch number : 166\n",
            "Epoch 167/1000 completed \t - \tBatch loss: 0.004846909\n",
            "Epoch number : 167\n",
            "Epoch 168/1000 completed \t - \tBatch loss: 0.003818599\n",
            "Epoch number : 168\n",
            "Epoch 169/1000 completed \t - \tBatch loss: 0.003506249\n",
            "Epoch number : 169\n",
            "Epoch 170/1000 completed \t - \tBatch loss: 0.004167601\n",
            "Epoch number : 170\n",
            "Epoch 171/1000 completed \t - \tBatch loss: 0.005095940\n",
            "Epoch number : 171\n",
            "Epoch 172/1000 completed \t - \tBatch loss: 0.004938492\n",
            "Epoch number : 172\n",
            "Epoch 173/1000 completed \t - \tBatch loss: 0.004937212\n",
            "Epoch number : 173\n",
            "Epoch 174/1000 completed \t - \tBatch loss: 0.004638707\n",
            "Epoch number : 174\n",
            "Epoch 175/1000 completed \t - \tBatch loss: 0.006184176\n",
            "Epoch number : 175\n",
            "Epoch 176/1000 completed \t - \tBatch loss: 0.006116534\n",
            "Epoch number : 176\n",
            "Epoch 177/1000 completed \t - \tBatch loss: 0.006088209\n",
            "Epoch number : 177\n",
            "Epoch 178/1000 completed \t - \tBatch loss: 0.004422095\n",
            "Epoch number : 178\n",
            "Epoch 179/1000 completed \t - \tBatch loss: 0.005602492\n",
            "Epoch number : 179\n",
            "Epoch 180/1000 completed \t - \tBatch loss: 0.012165938\n",
            "Epoch number : 180\n",
            "Epoch 181/1000 completed \t - \tBatch loss: 0.008595298\n",
            "Epoch number : 181\n",
            "Epoch 182/1000 completed \t - \tBatch loss: 0.022317873\n",
            "Epoch number : 182\n",
            "Epoch 183/1000 completed \t - \tBatch loss: 0.325625956\n",
            "Epoch number : 183\n",
            "Epoch 184/1000 completed \t - \tBatch loss: 0.143302828\n",
            "Epoch number : 184\n",
            "Epoch 185/1000 completed \t - \tBatch loss: 0.039400198\n",
            "Epoch number : 185\n",
            "Epoch 186/1000 completed \t - \tBatch loss: 0.094811626\n",
            "Epoch number : 186\n",
            "Epoch 187/1000 completed \t - \tBatch loss: 0.111197017\n",
            "Epoch number : 187\n",
            "Epoch 188/1000 completed \t - \tBatch loss: 0.080730692\n",
            "Epoch number : 188\n",
            "Epoch 189/1000 completed \t - \tBatch loss: 0.113025241\n",
            "Epoch number : 189\n",
            "Epoch 190/1000 completed \t - \tBatch loss: 0.058505747\n",
            "Epoch number : 190\n",
            "Epoch 191/1000 completed \t - \tBatch loss: 0.049588975\n",
            "Epoch number : 191\n",
            "Epoch 192/1000 completed \t - \tBatch loss: 0.043361988\n",
            "Epoch number : 192\n",
            "Epoch 193/1000 completed \t - \tBatch loss: 0.041718423\n",
            "Epoch number : 193\n",
            "Epoch 194/1000 completed \t - \tBatch loss: 0.082228690\n",
            "Epoch number : 194\n",
            "Epoch 195/1000 completed \t - \tBatch loss: 0.081230998\n",
            "Epoch number : 195\n",
            "Epoch 196/1000 completed \t - \tBatch loss: 0.113965787\n",
            "Epoch number : 196\n",
            "Epoch 197/1000 completed \t - \tBatch loss: 0.045882516\n",
            "Epoch number : 197\n",
            "Epoch 198/1000 completed \t - \tBatch loss: 0.079827420\n",
            "Epoch number : 198\n",
            "Epoch 199/1000 completed \t - \tBatch loss: 0.049095791\n",
            "Epoch number : 199\n",
            "Epoch 200/1000 completed \t - \tBatch loss: 0.078001544\n",
            "Epoch number : 200\n",
            "Epoch 201/1000 completed \t - \tBatch loss: 0.054970562\n",
            "Epoch number : 201\n",
            "Epoch 202/1000 completed \t - \tBatch loss: 0.048112199\n",
            "Epoch number : 202\n",
            "Epoch 203/1000 completed \t - \tBatch loss: 0.027099978\n",
            "Epoch number : 203\n",
            "Epoch 204/1000 completed \t - \tBatch loss: 0.033354871\n",
            "Epoch number : 204\n",
            "Epoch 205/1000 completed \t - \tBatch loss: 0.045880847\n",
            "Epoch number : 205\n",
            "Epoch 206/1000 completed \t - \tBatch loss: 0.029575590\n",
            "Epoch number : 206\n",
            "Epoch 207/1000 completed \t - \tBatch loss: 0.041645616\n",
            "Epoch number : 207\n",
            "Epoch 208/1000 completed \t - \tBatch loss: 0.037811574\n",
            "Epoch number : 208\n",
            "Epoch 209/1000 completed \t - \tBatch loss: 0.018121343\n",
            "Epoch number : 209\n",
            "Epoch 210/1000 completed \t - \tBatch loss: 0.019391568\n",
            "Epoch number : 210\n",
            "Epoch 211/1000 completed \t - \tBatch loss: 0.023655448\n",
            "Epoch number : 211\n",
            "Epoch 212/1000 completed \t - \tBatch loss: 0.022638794\n",
            "Epoch number : 212\n",
            "Epoch 213/1000 completed \t - \tBatch loss: 0.017234242\n",
            "Epoch number : 213\n",
            "Epoch 214/1000 completed \t - \tBatch loss: 0.018553328\n",
            "Epoch number : 214\n",
            "Epoch 215/1000 completed \t - \tBatch loss: 0.031725682\n",
            "Epoch number : 215\n",
            "Epoch 216/1000 completed \t - \tBatch loss: 0.025152707\n",
            "Epoch number : 216\n",
            "Epoch 217/1000 completed \t - \tBatch loss: 0.018745732\n",
            "Epoch number : 217\n",
            "Epoch 218/1000 completed \t - \tBatch loss: 0.012842471\n",
            "Epoch number : 218\n",
            "Epoch 219/1000 completed \t - \tBatch loss: 0.011739959\n",
            "Epoch number : 219\n",
            "Epoch 220/1000 completed \t - \tBatch loss: 0.012251485\n",
            "Epoch number : 220\n",
            "Epoch 221/1000 completed \t - \tBatch loss: 0.011811105\n",
            "Epoch number : 221\n",
            "Epoch 222/1000 completed \t - \tBatch loss: 0.009416026\n",
            "Epoch number : 222\n",
            "Epoch 223/1000 completed \t - \tBatch loss: 0.009011947\n",
            "Epoch number : 223\n",
            "Epoch 224/1000 completed \t - \tBatch loss: 0.007825599\n",
            "Epoch number : 224\n",
            "Epoch 225/1000 completed \t - \tBatch loss: 0.009744330\n",
            "Epoch number : 225\n",
            "Epoch 226/1000 completed \t - \tBatch loss: 0.009700427\n",
            "Epoch number : 226\n",
            "Epoch 227/1000 completed \t - \tBatch loss: 0.008163376\n",
            "Epoch number : 227\n",
            "Epoch 228/1000 completed \t - \tBatch loss: 0.009805033\n",
            "Epoch number : 228\n",
            "Epoch 229/1000 completed \t - \tBatch loss: 0.009789687\n",
            "Epoch number : 229\n",
            "Epoch 230/1000 completed \t - \tBatch loss: 0.008230664\n",
            "Epoch number : 230\n",
            "Epoch 231/1000 completed \t - \tBatch loss: 0.007448569\n",
            "Epoch number : 231\n",
            "Epoch 232/1000 completed \t - \tBatch loss: 0.006087298\n",
            "Epoch number : 232\n",
            "Epoch 233/1000 completed \t - \tBatch loss: 0.005073942\n",
            "Epoch number : 233\n",
            "Epoch 234/1000 completed \t - \tBatch loss: 0.004038906\n",
            "Epoch number : 234\n",
            "Epoch 235/1000 completed \t - \tBatch loss: 0.003896698\n",
            "Epoch number : 235\n",
            "Epoch 236/1000 completed \t - \tBatch loss: 0.006916778\n",
            "Epoch number : 236\n",
            "Epoch 237/1000 completed \t - \tBatch loss: 0.006284110\n",
            "Epoch number : 237\n",
            "Epoch 238/1000 completed \t - \tBatch loss: 0.005466344\n",
            "Epoch number : 238\n",
            "Epoch 239/1000 completed \t - \tBatch loss: 0.004271411\n",
            "Epoch number : 239\n",
            "Epoch 240/1000 completed \t - \tBatch loss: 0.003487339\n",
            "Epoch number : 240\n",
            "Epoch 241/1000 completed \t - \tBatch loss: 0.003742141\n",
            "Epoch number : 241\n",
            "Epoch 242/1000 completed \t - \tBatch loss: 0.004810005\n",
            "Epoch number : 242\n",
            "Epoch 243/1000 completed \t - \tBatch loss: 0.005152234\n",
            "Epoch number : 243\n",
            "Epoch 244/1000 completed \t - \tBatch loss: 0.004251304\n",
            "Epoch number : 244\n",
            "Epoch 245/1000 completed \t - \tBatch loss: 0.003541297\n",
            "Epoch number : 245\n",
            "Epoch 246/1000 completed \t - \tBatch loss: 0.002507305\n",
            "Epoch number : 246\n",
            "Epoch 247/1000 completed \t - \tBatch loss: 0.001655757\n",
            "Epoch number : 247\n",
            "Epoch 248/1000 completed \t - \tBatch loss: 0.001219059\n",
            "Epoch number : 248\n",
            "Epoch 249/1000 completed \t - \tBatch loss: 0.001057311\n",
            "Epoch number : 249\n",
            "Epoch 250/1000 completed \t - \tBatch loss: 0.001072039\n",
            "Epoch number : 250\n",
            "Epoch 251/1000 completed \t - \tBatch loss: 0.001215537\n",
            "Epoch number : 251\n",
            "Epoch 252/1000 completed \t - \tBatch loss: 0.001476302\n",
            "Epoch number : 252\n",
            "Epoch 253/1000 completed \t - \tBatch loss: 0.001835208\n",
            "Epoch number : 253\n",
            "Epoch 254/1000 completed \t - \tBatch loss: 0.001926150\n",
            "Epoch number : 254\n",
            "Epoch 255/1000 completed \t - \tBatch loss: 0.001592722\n",
            "Epoch number : 255\n",
            "Epoch 256/1000 completed \t - \tBatch loss: 0.001221427\n",
            "Epoch number : 256\n",
            "Epoch 257/1000 completed \t - \tBatch loss: 0.001015806\n",
            "Epoch number : 257\n",
            "Epoch 258/1000 completed \t - \tBatch loss: 0.000952780\n",
            "Epoch number : 258\n",
            "Epoch 259/1000 completed \t - \tBatch loss: 0.000982836\n",
            "Epoch number : 259\n",
            "Epoch 260/1000 completed \t - \tBatch loss: 0.001081671\n",
            "Epoch number : 260\n",
            "Epoch 261/1000 completed \t - \tBatch loss: 0.001240174\n",
            "Epoch number : 261\n",
            "Epoch 262/1000 completed \t - \tBatch loss: 0.001362999\n",
            "Epoch number : 262\n",
            "Epoch 263/1000 completed \t - \tBatch loss: 0.001310120\n",
            "Epoch number : 263\n",
            "Epoch 264/1000 completed \t - \tBatch loss: 0.001114564\n",
            "Epoch number : 264\n",
            "Epoch 265/1000 completed \t - \tBatch loss: 0.000955435\n",
            "Epoch number : 265\n",
            "Epoch 266/1000 completed \t - \tBatch loss: 0.000898917\n",
            "Epoch number : 266\n",
            "Epoch 267/1000 completed \t - \tBatch loss: 0.000904610\n",
            "Epoch number : 267\n",
            "Epoch 268/1000 completed \t - \tBatch loss: 0.000928905\n",
            "Epoch number : 268\n",
            "Epoch 269/1000 completed \t - \tBatch loss: 0.000946328\n",
            "Epoch number : 269\n",
            "Epoch 270/1000 completed \t - \tBatch loss: 0.000947017\n",
            "Epoch number : 270\n",
            "Epoch 271/1000 completed \t - \tBatch loss: 0.000925785\n",
            "Epoch number : 271\n",
            "Epoch 272/1000 completed \t - \tBatch loss: 0.000874976\n",
            "Epoch number : 272\n",
            "Epoch 273/1000 completed \t - \tBatch loss: 0.000803661\n",
            "Epoch number : 273\n",
            "Epoch 274/1000 completed \t - \tBatch loss: 0.000731083\n",
            "Epoch number : 274\n",
            "Epoch 275/1000 completed \t - \tBatch loss: 0.000666592\n",
            "Epoch number : 275\n",
            "Epoch 276/1000 completed \t - \tBatch loss: 0.000612473\n",
            "Epoch number : 276\n",
            "Epoch 277/1000 completed \t - \tBatch loss: 0.000569359\n",
            "Epoch number : 277\n",
            "Epoch 278/1000 completed \t - \tBatch loss: 0.000534165\n",
            "Epoch number : 278\n",
            "Epoch 279/1000 completed \t - \tBatch loss: 0.000506086\n",
            "Epoch number : 279\n",
            "Epoch 280/1000 completed \t - \tBatch loss: 0.000484421\n",
            "Epoch number : 280\n",
            "Epoch 281/1000 completed \t - \tBatch loss: 0.000469072\n",
            "Epoch number : 281\n",
            "Epoch 282/1000 completed \t - \tBatch loss: 0.000458932\n",
            "Epoch number : 282\n",
            "Epoch 283/1000 completed \t - \tBatch loss: 0.000453213\n",
            "Epoch number : 283\n",
            "Epoch 284/1000 completed \t - \tBatch loss: 0.000450263\n",
            "Epoch number : 284\n",
            "Epoch 285/1000 completed \t - \tBatch loss: 0.000446821\n",
            "Epoch number : 285\n",
            "Epoch 286/1000 completed \t - \tBatch loss: 0.000441130\n",
            "Epoch number : 286\n",
            "Epoch 287/1000 completed \t - \tBatch loss: 0.000433625\n",
            "Epoch number : 287\n",
            "Epoch 288/1000 completed \t - \tBatch loss: 0.000423136\n",
            "Epoch number : 288\n",
            "Epoch 289/1000 completed \t - \tBatch loss: 0.000410839\n",
            "Epoch number : 289\n",
            "Epoch 290/1000 completed \t - \tBatch loss: 0.000397080\n",
            "Epoch number : 290\n",
            "Epoch 291/1000 completed \t - \tBatch loss: 0.000382722\n",
            "Epoch number : 291\n",
            "Epoch 292/1000 completed \t - \tBatch loss: 0.000368848\n",
            "Epoch number : 292\n",
            "Epoch 293/1000 completed \t - \tBatch loss: 0.000356856\n",
            "Epoch number : 293\n",
            "Epoch 294/1000 completed \t - \tBatch loss: 0.000346829\n",
            "Epoch number : 294\n",
            "Epoch 295/1000 completed \t - \tBatch loss: 0.000339024\n",
            "Epoch number : 295\n",
            "Epoch 296/1000 completed \t - \tBatch loss: 0.000333653\n",
            "Epoch number : 296\n",
            "Epoch 297/1000 completed \t - \tBatch loss: 0.000330763\n",
            "Epoch number : 297\n",
            "Epoch 298/1000 completed \t - \tBatch loss: 0.000330197\n",
            "Epoch number : 298\n",
            "Epoch 299/1000 completed \t - \tBatch loss: 0.000331131\n",
            "Epoch number : 299\n",
            "Epoch 300/1000 completed \t - \tBatch loss: 0.000333050\n",
            "Epoch number : 300\n",
            "Epoch 301/1000 completed \t - \tBatch loss: 0.000335510\n",
            "Epoch number : 301\n",
            "Epoch 302/1000 completed \t - \tBatch loss: 0.000338090\n",
            "Epoch number : 302\n",
            "Epoch 303/1000 completed \t - \tBatch loss: 0.000340236\n",
            "Epoch number : 303\n",
            "Epoch 304/1000 completed \t - \tBatch loss: 0.000341845\n",
            "Epoch number : 304\n",
            "Epoch 305/1000 completed \t - \tBatch loss: 0.000342149\n",
            "Epoch number : 305\n",
            "Epoch 306/1000 completed \t - \tBatch loss: 0.000341310\n",
            "Epoch number : 306\n",
            "Epoch 307/1000 completed \t - \tBatch loss: 0.000339212\n",
            "Epoch number : 307\n",
            "Epoch 308/1000 completed \t - \tBatch loss: 0.000335802\n",
            "Epoch number : 308\n",
            "Epoch 309/1000 completed \t - \tBatch loss: 0.000331173\n",
            "Epoch number : 309\n",
            "Epoch 310/1000 completed \t - \tBatch loss: 0.000325500\n",
            "Epoch number : 310\n",
            "Epoch 311/1000 completed \t - \tBatch loss: 0.000319083\n",
            "Epoch number : 311\n",
            "Epoch 312/1000 completed \t - \tBatch loss: 0.000312205\n",
            "Epoch number : 312\n",
            "Epoch 313/1000 completed \t - \tBatch loss: 0.000305122\n",
            "Epoch number : 313\n",
            "Epoch 314/1000 completed \t - \tBatch loss: 0.000298044\n",
            "Epoch number : 314\n",
            "Epoch 315/1000 completed \t - \tBatch loss: 0.000291149\n",
            "Epoch number : 315\n",
            "Epoch 316/1000 completed \t - \tBatch loss: 0.000284568\n",
            "Epoch number : 316\n",
            "Epoch 317/1000 completed \t - \tBatch loss: 0.000278429\n",
            "Epoch number : 317\n",
            "Epoch 318/1000 completed \t - \tBatch loss: 0.000272773\n",
            "Epoch number : 318\n",
            "Epoch 319/1000 completed \t - \tBatch loss: 0.000267787\n",
            "Epoch number : 319\n",
            "Epoch 320/1000 completed \t - \tBatch loss: 0.000263307\n",
            "Epoch number : 320\n",
            "Epoch 321/1000 completed \t - \tBatch loss: 0.000259314\n",
            "Epoch number : 321\n",
            "Epoch 322/1000 completed \t - \tBatch loss: 0.000255733\n",
            "Epoch number : 322\n",
            "Epoch 323/1000 completed \t - \tBatch loss: 0.000252525\n",
            "Epoch number : 323\n",
            "Epoch 324/1000 completed \t - \tBatch loss: 0.000249670\n",
            "Epoch number : 324\n",
            "Epoch 325/1000 completed \t - \tBatch loss: 0.000247134\n",
            "Epoch number : 325\n",
            "Epoch 326/1000 completed \t - \tBatch loss: 0.000244940\n",
            "Epoch number : 326\n",
            "Epoch 327/1000 completed \t - \tBatch loss: 0.000243021\n",
            "Epoch number : 327\n",
            "Epoch 328/1000 completed \t - \tBatch loss: 0.000241350\n",
            "Epoch number : 328\n",
            "Epoch 329/1000 completed \t - \tBatch loss: 0.000239902\n",
            "Epoch number : 329\n",
            "Epoch 330/1000 completed \t - \tBatch loss: 0.000238654\n",
            "Epoch number : 330\n",
            "Epoch 331/1000 completed \t - \tBatch loss: 0.000237540\n",
            "Epoch number : 331\n",
            "Epoch 332/1000 completed \t - \tBatch loss: 0.000236546\n",
            "Epoch number : 332\n",
            "Epoch 333/1000 completed \t - \tBatch loss: 0.000235692\n",
            "Epoch number : 333\n",
            "Epoch 334/1000 completed \t - \tBatch loss: 0.000234910\n",
            "Epoch number : 334\n",
            "Epoch 335/1000 completed \t - \tBatch loss: 0.000234200\n",
            "Epoch number : 335\n",
            "Epoch 336/1000 completed \t - \tBatch loss: 0.000233579\n",
            "Epoch number : 336\n",
            "Epoch 337/1000 completed \t - \tBatch loss: 0.000232987\n",
            "Epoch number : 337\n",
            "Epoch 338/1000 completed \t - \tBatch loss: 0.000232444\n",
            "Epoch number : 338\n",
            "Epoch 339/1000 completed \t - \tBatch loss: 0.000231913\n",
            "Epoch number : 339\n",
            "Epoch 340/1000 completed \t - \tBatch loss: 0.000231397\n",
            "Epoch number : 340\n",
            "Epoch 341/1000 completed \t - \tBatch loss: 0.000230920\n",
            "Epoch number : 341\n",
            "Epoch 342/1000 completed \t - \tBatch loss: 0.000230451\n",
            "Epoch number : 342\n",
            "Epoch 343/1000 completed \t - \tBatch loss: 0.000229972\n",
            "Epoch number : 343\n",
            "Epoch 344/1000 completed \t - \tBatch loss: 0.000229467\n",
            "Epoch number : 344\n",
            "Epoch 345/1000 completed \t - \tBatch loss: 0.000228926\n",
            "Epoch number : 345\n",
            "Epoch 346/1000 completed \t - \tBatch loss: 0.000228355\n",
            "Epoch number : 346\n",
            "Epoch 347/1000 completed \t - \tBatch loss: 0.000227736\n",
            "Epoch number : 347\n",
            "Epoch 348/1000 completed \t - \tBatch loss: 0.000227087\n",
            "Epoch number : 348\n",
            "Epoch 349/1000 completed \t - \tBatch loss: 0.000226423\n",
            "Epoch number : 349\n",
            "Epoch 350/1000 completed \t - \tBatch loss: 0.000225723\n",
            "Epoch number : 350\n",
            "Epoch 351/1000 completed \t - \tBatch loss: 0.000225014\n",
            "Epoch number : 351\n",
            "Epoch 352/1000 completed \t - \tBatch loss: 0.000224271\n",
            "Epoch number : 352\n",
            "Epoch 353/1000 completed \t - \tBatch loss: 0.000223510\n",
            "Epoch number : 353\n",
            "Epoch 354/1000 completed \t - \tBatch loss: 0.000222715\n",
            "Epoch number : 354\n",
            "Epoch 355/1000 completed \t - \tBatch loss: 0.000221937\n",
            "Epoch number : 355\n",
            "Epoch 356/1000 completed \t - \tBatch loss: 0.000221145\n",
            "Epoch number : 356\n",
            "Epoch 357/1000 completed \t - \tBatch loss: 0.000220329\n",
            "Epoch number : 357\n",
            "Epoch 358/1000 completed \t - \tBatch loss: 0.000219494\n",
            "Epoch number : 358\n",
            "Epoch 359/1000 completed \t - \tBatch loss: 0.000218653\n",
            "Epoch number : 359\n",
            "Epoch 360/1000 completed \t - \tBatch loss: 0.000217805\n",
            "Epoch number : 360\n",
            "Epoch 361/1000 completed \t - \tBatch loss: 0.000216932\n",
            "Epoch number : 361\n",
            "Epoch 362/1000 completed \t - \tBatch loss: 0.000216065\n",
            "Epoch number : 362\n",
            "Epoch 363/1000 completed \t - \tBatch loss: 0.000215187\n",
            "Epoch number : 363\n",
            "Epoch 364/1000 completed \t - \tBatch loss: 0.000214327\n",
            "Epoch number : 364\n",
            "Epoch 365/1000 completed \t - \tBatch loss: 0.000213478\n",
            "Epoch number : 365\n",
            "Epoch 366/1000 completed \t - \tBatch loss: 0.000212617\n",
            "Epoch number : 366\n",
            "Epoch 367/1000 completed \t - \tBatch loss: 0.000211764\n",
            "Epoch number : 367\n",
            "Epoch 368/1000 completed \t - \tBatch loss: 0.000210931\n",
            "Epoch number : 368\n",
            "Epoch 369/1000 completed \t - \tBatch loss: 0.000210093\n",
            "Epoch number : 369\n",
            "Epoch 370/1000 completed \t - \tBatch loss: 0.000209257\n",
            "Epoch number : 370\n",
            "Epoch 371/1000 completed \t - \tBatch loss: 0.000208423\n",
            "Epoch number : 371\n",
            "Epoch 372/1000 completed \t - \tBatch loss: 0.000207610\n",
            "Epoch number : 372\n",
            "Epoch 373/1000 completed \t - \tBatch loss: 0.000206785\n",
            "Epoch number : 373\n",
            "Epoch 374/1000 completed \t - \tBatch loss: 0.000205987\n",
            "Epoch number : 374\n",
            "Epoch 375/1000 completed \t - \tBatch loss: 0.000205203\n",
            "Epoch number : 375\n",
            "Epoch 376/1000 completed \t - \tBatch loss: 0.000204431\n",
            "Epoch number : 376\n",
            "Epoch 377/1000 completed \t - \tBatch loss: 0.000203695\n",
            "Epoch number : 377\n",
            "Epoch 378/1000 completed \t - \tBatch loss: 0.000202990\n",
            "Epoch number : 378\n",
            "Epoch 379/1000 completed \t - \tBatch loss: 0.000202261\n",
            "Epoch number : 379\n",
            "Epoch 380/1000 completed \t - \tBatch loss: 0.000201559\n",
            "Epoch number : 380\n",
            "Epoch 381/1000 completed \t - \tBatch loss: 0.000200852\n",
            "Epoch number : 381\n",
            "Epoch 382/1000 completed \t - \tBatch loss: 0.000200127\n",
            "Epoch number : 382\n",
            "Epoch 383/1000 completed \t - \tBatch loss: 0.000199435\n",
            "Epoch number : 383\n",
            "Epoch 384/1000 completed \t - \tBatch loss: 0.000198776\n",
            "Epoch number : 384\n",
            "Epoch 385/1000 completed \t - \tBatch loss: 0.000198099\n",
            "Epoch number : 385\n",
            "Epoch 386/1000 completed \t - \tBatch loss: 0.000197423\n",
            "Epoch number : 386\n",
            "Epoch 387/1000 completed \t - \tBatch loss: 0.000196721\n",
            "Epoch number : 387\n",
            "Epoch 388/1000 completed \t - \tBatch loss: 0.000196000\n",
            "Epoch number : 388\n",
            "Epoch 389/1000 completed \t - \tBatch loss: 0.000195282\n",
            "Epoch number : 389\n",
            "Epoch 390/1000 completed \t - \tBatch loss: 0.000194555\n",
            "Epoch number : 390\n",
            "Epoch 391/1000 completed \t - \tBatch loss: 0.000193818\n",
            "Epoch number : 391\n",
            "Epoch 392/1000 completed \t - \tBatch loss: 0.000193089\n",
            "Epoch number : 392\n",
            "Epoch 393/1000 completed \t - \tBatch loss: 0.000192349\n",
            "Epoch number : 393\n",
            "Epoch 394/1000 completed \t - \tBatch loss: 0.000191610\n",
            "Epoch number : 394\n",
            "Epoch 395/1000 completed \t - \tBatch loss: 0.000190889\n",
            "Epoch number : 395\n",
            "Epoch 396/1000 completed \t - \tBatch loss: 0.000190206\n",
            "Epoch number : 396\n",
            "Epoch 397/1000 completed \t - \tBatch loss: 0.000189548\n",
            "Epoch number : 397\n",
            "Epoch 398/1000 completed \t - \tBatch loss: 0.000188906\n",
            "Epoch number : 398\n",
            "Epoch 399/1000 completed \t - \tBatch loss: 0.000188256\n",
            "Epoch number : 399\n",
            "Epoch 400/1000 completed \t - \tBatch loss: 0.000187643\n",
            "Epoch number : 400\n",
            "Epoch 401/1000 completed \t - \tBatch loss: 0.000187066\n",
            "Epoch number : 401\n",
            "Epoch 402/1000 completed \t - \tBatch loss: 0.000186510\n",
            "Epoch number : 402\n",
            "Epoch 403/1000 completed \t - \tBatch loss: 0.000185962\n",
            "Epoch number : 403\n",
            "Epoch 404/1000 completed \t - \tBatch loss: 0.000185443\n",
            "Epoch number : 404\n",
            "Epoch 405/1000 completed \t - \tBatch loss: 0.000184914\n",
            "Epoch number : 405\n",
            "Epoch 406/1000 completed \t - \tBatch loss: 0.000184375\n",
            "Epoch number : 406\n",
            "Epoch 407/1000 completed \t - \tBatch loss: 0.000183839\n",
            "Epoch number : 407\n",
            "Epoch 408/1000 completed \t - \tBatch loss: 0.000183305\n",
            "Epoch number : 408\n",
            "Epoch 409/1000 completed \t - \tBatch loss: 0.000182772\n",
            "Epoch number : 409\n",
            "Epoch 410/1000 completed \t - \tBatch loss: 0.000182214\n",
            "Epoch number : 410\n",
            "Epoch 411/1000 completed \t - \tBatch loss: 0.000181642\n",
            "Epoch number : 411\n",
            "Epoch 412/1000 completed \t - \tBatch loss: 0.000181058\n",
            "Epoch number : 412\n",
            "Epoch 413/1000 completed \t - \tBatch loss: 0.000180497\n",
            "Epoch number : 413\n",
            "Epoch 414/1000 completed \t - \tBatch loss: 0.000179924\n",
            "Epoch number : 414\n",
            "Epoch 415/1000 completed \t - \tBatch loss: 0.000179326\n",
            "Epoch number : 415\n",
            "Epoch 416/1000 completed \t - \tBatch loss: 0.000178689\n",
            "Epoch number : 416\n",
            "Epoch 417/1000 completed \t - \tBatch loss: 0.000178031\n",
            "Epoch number : 417\n",
            "Epoch 418/1000 completed \t - \tBatch loss: 0.000177368\n",
            "Epoch number : 418\n",
            "Epoch 419/1000 completed \t - \tBatch loss: 0.000176697\n",
            "Epoch number : 419\n",
            "Epoch 420/1000 completed \t - \tBatch loss: 0.000176035\n",
            "Epoch number : 420\n",
            "Epoch 421/1000 completed \t - \tBatch loss: 0.000175384\n",
            "Epoch number : 421\n",
            "Epoch 422/1000 completed \t - \tBatch loss: 0.000174757\n",
            "Epoch number : 422\n",
            "Epoch 423/1000 completed \t - \tBatch loss: 0.000174169\n",
            "Epoch number : 423\n",
            "Epoch 424/1000 completed \t - \tBatch loss: 0.000173586\n",
            "Epoch number : 424\n",
            "Epoch 425/1000 completed \t - \tBatch loss: 0.000173011\n",
            "Epoch number : 425\n",
            "Epoch 426/1000 completed \t - \tBatch loss: 0.000172454\n",
            "Epoch number : 426\n",
            "Epoch 427/1000 completed \t - \tBatch loss: 0.000171886\n",
            "Epoch number : 427\n",
            "Epoch 428/1000 completed \t - \tBatch loss: 0.000171308\n",
            "Epoch number : 428\n",
            "Epoch 429/1000 completed \t - \tBatch loss: 0.000170748\n",
            "Epoch number : 429\n",
            "Epoch 430/1000 completed \t - \tBatch loss: 0.000170186\n",
            "Epoch number : 430\n",
            "Epoch 431/1000 completed \t - \tBatch loss: 0.000169610\n",
            "Epoch number : 431\n",
            "Epoch 432/1000 completed \t - \tBatch loss: 0.000169062\n",
            "Epoch number : 432\n",
            "Epoch 433/1000 completed \t - \tBatch loss: 0.000168518\n",
            "Epoch number : 433\n",
            "Epoch 434/1000 completed \t - \tBatch loss: 0.000167958\n",
            "Epoch number : 434\n",
            "Epoch 435/1000 completed \t - \tBatch loss: 0.000167414\n",
            "Epoch number : 435\n",
            "Epoch 436/1000 completed \t - \tBatch loss: 0.000166855\n",
            "Epoch number : 436\n",
            "Epoch 437/1000 completed \t - \tBatch loss: 0.000166304\n",
            "Epoch number : 437\n",
            "Epoch 438/1000 completed \t - \tBatch loss: 0.000165777\n",
            "Epoch number : 438\n",
            "Epoch 439/1000 completed \t - \tBatch loss: 0.000165252\n",
            "Epoch number : 439\n",
            "Epoch 440/1000 completed \t - \tBatch loss: 0.000164727\n",
            "Epoch number : 440\n",
            "Epoch 441/1000 completed \t - \tBatch loss: 0.000164216\n",
            "Epoch number : 441\n",
            "Epoch 442/1000 completed \t - \tBatch loss: 0.000163721\n",
            "Epoch number : 442\n",
            "Epoch 443/1000 completed \t - \tBatch loss: 0.000163228\n",
            "Epoch number : 443\n",
            "Epoch 444/1000 completed \t - \tBatch loss: 0.000162760\n",
            "Epoch number : 444\n",
            "Epoch 445/1000 completed \t - \tBatch loss: 0.000162295\n",
            "Epoch number : 445\n",
            "Epoch 446/1000 completed \t - \tBatch loss: 0.000161841\n",
            "Epoch number : 446\n",
            "Epoch 447/1000 completed \t - \tBatch loss: 0.000161397\n",
            "Epoch number : 447\n",
            "Epoch 448/1000 completed \t - \tBatch loss: 0.000160974\n",
            "Epoch number : 448\n",
            "Epoch 449/1000 completed \t - \tBatch loss: 0.000160538\n",
            "Epoch number : 449\n",
            "Epoch 450/1000 completed \t - \tBatch loss: 0.000160112\n",
            "Epoch number : 450\n",
            "Epoch 451/1000 completed \t - \tBatch loss: 0.000159681\n",
            "Epoch number : 451\n",
            "Epoch 452/1000 completed \t - \tBatch loss: 0.000159260\n",
            "Epoch number : 452\n",
            "Epoch 453/1000 completed \t - \tBatch loss: 0.000158844\n",
            "Epoch number : 453\n",
            "Epoch 454/1000 completed \t - \tBatch loss: 0.000158411\n",
            "Epoch number : 454\n",
            "Epoch 455/1000 completed \t - \tBatch loss: 0.000157995\n",
            "Epoch number : 455\n",
            "Epoch 456/1000 completed \t - \tBatch loss: 0.000157577\n",
            "Epoch number : 456\n",
            "Epoch 457/1000 completed \t - \tBatch loss: 0.000157158\n",
            "Epoch number : 457\n",
            "Epoch 458/1000 completed \t - \tBatch loss: 0.000156739\n",
            "Epoch number : 458\n",
            "Epoch 459/1000 completed \t - \tBatch loss: 0.000156321\n",
            "Epoch number : 459\n",
            "Epoch 460/1000 completed \t - \tBatch loss: 0.000155895\n",
            "Epoch number : 460\n",
            "Epoch 461/1000 completed \t - \tBatch loss: 0.000155471\n",
            "Epoch number : 461\n",
            "Epoch 462/1000 completed \t - \tBatch loss: 0.000155033\n",
            "Epoch number : 462\n",
            "Epoch 463/1000 completed \t - \tBatch loss: 0.000154592\n",
            "Epoch number : 463\n",
            "Epoch 464/1000 completed \t - \tBatch loss: 0.000154150\n",
            "Epoch number : 464\n",
            "Epoch 465/1000 completed \t - \tBatch loss: 0.000153689\n",
            "Epoch number : 465\n",
            "Epoch 466/1000 completed \t - \tBatch loss: 0.000153246\n",
            "Epoch number : 466\n",
            "Epoch 467/1000 completed \t - \tBatch loss: 0.000152776\n",
            "Epoch number : 467\n",
            "Epoch 468/1000 completed \t - \tBatch loss: 0.000152315\n",
            "Epoch number : 468\n",
            "Epoch 469/1000 completed \t - \tBatch loss: 0.000151845\n",
            "Epoch number : 469\n",
            "Epoch 470/1000 completed \t - \tBatch loss: 0.000151371\n",
            "Epoch number : 470\n",
            "Epoch 471/1000 completed \t - \tBatch loss: 0.000150888\n",
            "Epoch number : 471\n",
            "Epoch 472/1000 completed \t - \tBatch loss: 0.000150391\n",
            "Epoch number : 472\n",
            "Epoch 473/1000 completed \t - \tBatch loss: 0.000149903\n",
            "Epoch number : 473\n",
            "Epoch 474/1000 completed \t - \tBatch loss: 0.000149419\n",
            "Epoch number : 474\n",
            "Epoch 475/1000 completed \t - \tBatch loss: 0.000148931\n",
            "Epoch number : 475\n",
            "Epoch 476/1000 completed \t - \tBatch loss: 0.000148452\n",
            "Epoch number : 476\n",
            "Epoch 477/1000 completed \t - \tBatch loss: 0.000147970\n",
            "Epoch number : 477\n",
            "Epoch 478/1000 completed \t - \tBatch loss: 0.000147503\n",
            "Epoch number : 478\n",
            "Epoch 479/1000 completed \t - \tBatch loss: 0.000147020\n",
            "Epoch number : 479\n",
            "Epoch 480/1000 completed \t - \tBatch loss: 0.000146528\n",
            "Epoch number : 480\n",
            "Epoch 481/1000 completed \t - \tBatch loss: 0.000146043\n",
            "Epoch number : 481\n",
            "Epoch 482/1000 completed \t - \tBatch loss: 0.000145571\n",
            "Epoch number : 482\n",
            "Epoch 483/1000 completed \t - \tBatch loss: 0.000145098\n",
            "Epoch number : 483\n",
            "Epoch 484/1000 completed \t - \tBatch loss: 0.000144641\n",
            "Epoch number : 484\n",
            "Epoch 485/1000 completed \t - \tBatch loss: 0.000144186\n",
            "Epoch number : 485\n",
            "Epoch 486/1000 completed \t - \tBatch loss: 0.000143732\n",
            "Epoch number : 486\n",
            "Epoch 487/1000 completed \t - \tBatch loss: 0.000143293\n",
            "Epoch number : 487\n",
            "Epoch 488/1000 completed \t - \tBatch loss: 0.000142855\n",
            "Epoch number : 488\n",
            "Epoch 489/1000 completed \t - \tBatch loss: 0.000142417\n",
            "Epoch number : 489\n",
            "Epoch 490/1000 completed \t - \tBatch loss: 0.000141994\n",
            "Epoch number : 490\n",
            "Epoch 491/1000 completed \t - \tBatch loss: 0.000141587\n",
            "Epoch number : 491\n",
            "Epoch 492/1000 completed \t - \tBatch loss: 0.000141186\n",
            "Epoch number : 492\n",
            "Epoch 493/1000 completed \t - \tBatch loss: 0.000140795\n",
            "Epoch number : 493\n",
            "Epoch 494/1000 completed \t - \tBatch loss: 0.000140404\n",
            "Epoch number : 494\n",
            "Epoch 495/1000 completed \t - \tBatch loss: 0.000140020\n",
            "Epoch number : 495\n",
            "Epoch 496/1000 completed \t - \tBatch loss: 0.000139659\n",
            "Epoch number : 496\n",
            "Epoch 497/1000 completed \t - \tBatch loss: 0.000139315\n",
            "Epoch number : 497\n",
            "Epoch 498/1000 completed \t - \tBatch loss: 0.000139002\n",
            "Epoch number : 498\n",
            "Epoch 499/1000 completed \t - \tBatch loss: 0.000138711\n",
            "Epoch number : 499\n",
            "Epoch 500/1000 completed \t - \tBatch loss: 0.000138429\n",
            "Final loss for training  set: 0.000138429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cRkMl3vwUwSl",
        "colab_type": "code",
        "outputId": "4f93dd21-023f-4982-97eb-5ef9cc3ff2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(t_loss_history)\n",
        "\n",
        "plt.ylabel('train loss value')\n",
        "plt.xlabel('train batches')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wOECZZOQTPHqQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JI-AoqN3vhE1",
        "colab_type": "code",
        "outputId": "fc68e4e2-78ad-4733-cf3d-30453cc0f707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        }
      },
      "cell_type": "code",
      "source": [
        "test_loss = []\n",
        "for train_epoch_num, train_epoch in enumerate(generate_epoch(train_X, train_y, 1, 128)):\n",
        "  for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n",
        "    loss = 0\n",
        "    p = fw(batch_X, batch_y)\n",
        "    print(p)\n",
        "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p, labels= batch_y))\n",
        "    bloss = (loss/128)\n",
        "  test_loss.append(bloss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-13.6841955  -13.4219885  -13.711524   ...   2.0606859   -2.207641\n",
            "  -13.633846  ]\n",
            " [-13.637561   -13.550079   -14.012177   ...   2.5237226   -6.3801694\n",
            "  -13.567944  ]\n",
            " [-13.682084   -13.77179    -14.183525   ...   2.7521276   13.575354\n",
            "  -13.739816  ]\n",
            " ...\n",
            " [-10.395074   -10.546428   -10.885821   ...  -5.3196087   12.838566\n",
            "  -10.561732  ]\n",
            " [-11.691525   -11.627308   -11.794419   ...   4.4635153   -0.5219212\n",
            "  -11.733229  ]\n",
            " [-13.073224   -13.338928   -13.481073   ...  -0.76699924   8.710459\n",
            "  -13.066405  ]], shape=(128, 37), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-17.168985   -16.977589   -17.563375   ...  11.808264     0.85489696\n",
            "  -17.211412  ]\n",
            " [-12.597093   -12.521985   -12.827556   ...   4.6536922    1.526659\n",
            "  -12.51619   ]\n",
            " [-13.811156   -13.616852   -13.904601   ...   2.1351175   12.789604\n",
            "  -13.910947  ]\n",
            " ...\n",
            " [-11.022982   -10.761906   -11.009429   ...   5.681772     5.9171095\n",
            "  -11.077863  ]\n",
            " [-15.105015   -14.648195   -15.297137   ...  13.121843    10.006006\n",
            "  -15.096066  ]\n",
            " [-11.1960745  -11.2019     -11.255228   ...   1.638777     6.096256\n",
            "  -11.212258  ]], shape=(128, 37), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-14.880334  -14.815592  -15.188504  ...   8.219981   10.199759\n",
            "  -14.766633 ]\n",
            " [-11.669292  -11.5153265 -11.634707  ...   8.102003   10.292119\n",
            "  -11.610079 ]\n",
            " [-12.602138  -12.400647  -12.891081  ...   9.759351   -3.8705468\n",
            "  -12.62157  ]\n",
            " ...\n",
            " [-11.596959  -11.548037  -11.818887  ...   9.791437  -11.493288\n",
            "  -11.5765085]\n",
            " [-15.016993  -14.7095375 -15.618274  ...   9.195999   15.451254\n",
            "  -15.154561 ]\n",
            " [-12.852892  -12.761391  -12.93636   ...   6.729768  -11.886393\n",
            "  -12.842931 ]], shape=(128, 37), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-12.489422   -12.461475   -12.860883   ...   0.6355331    3.105203\n",
            "  -12.483163  ]\n",
            " [-11.804268   -11.755365   -12.110668   ...   8.591832    -0.99833995\n",
            "  -11.817498  ]\n",
            " [-12.835095   -12.603737   -13.060126   ...  11.775333    -9.545606\n",
            "  -12.776793  ]\n",
            " ...\n",
            " [-12.747443   -12.98631    -13.11466    ...   2.0481517    6.7095714\n",
            "  -12.822623  ]\n",
            " [-12.66484    -12.736844   -13.198211   ...  -6.905158    21.509422\n",
            "  -12.814595  ]\n",
            " [-13.803485   -13.522177   -14.060876   ...  27.155983    -7.7082086\n",
            "  -13.651299  ]], shape=(128, 37), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-12.476862  -12.442199  -12.790156  ...  -0.4106505  -0.4544176\n",
            "  -12.421497 ]\n",
            " [-14.24904   -14.0529175 -14.806853  ...   9.542714    8.04164\n",
            "  -14.169469 ]\n",
            " [-17.133976  -16.724964  -17.508253  ...   9.761438   -9.112296\n",
            "  -17.21884  ]\n",
            " ...\n",
            " [-12.106914  -12.117462  -12.399792  ...   5.531864    2.6703715\n",
            "  -12.142761 ]\n",
            " [-10.795256  -10.902269  -11.026736  ...  -8.9364      8.872957\n",
            "  -11.041426 ]\n",
            " [-17.57364   -17.26002   -18.212727  ...  13.18224    -1.3717027\n",
            "  -17.516218 ]], shape=(128, 37), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WqyQ8C0r1yg7",
        "colab_type": "code",
        "outputId": "75e625af-e730-440e-9481-6bbd7134a874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(test_loss)\n",
        "\n",
        "plt.ylabel('test loss value')\n",
        "plt.xlabel('test batches')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEKCAYAAACc3WsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNFaKWERENNb/B9oU5gKuu28tAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=2152162, shape=(), dtype=float32, numpy=1.0794033e-06>]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}