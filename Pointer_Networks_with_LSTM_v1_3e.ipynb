{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pointer_Networks_with_LSTM_v1-3e.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushbooG9/Pointer-Networks-Using-Fast-Weights/blob/master/Pointer_Networks_with_LSTM_v1_3e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y9rWiYuRrp0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Dev incorporation in training\n",
        "- Inference print layout difference\n",
        "- Better multi epoch training code\n",
        "- Updated loss to per output\n",
        "- Function for data prep"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zWFaAnmetOTr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-zyK1oRMrp0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()\n",
        "np.random.seed(42)\n",
        "tf.random.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KH5gPEjcrp0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment Configuration"
      ]
    },
    {
      "metadata": {
        "id": "sWdDXmrXrp0n",
        "colab_type": "code",
        "outputId": "b9d3b178-b9ea-48e3-fcd8-bbf48d320e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "hidden_dimensions=300 #tune\n",
        "\n",
        "minSeqSize=3\n",
        "maxSeqSize=6\n",
        "\n",
        "batchSize=16\n",
        "numOfBatches=100\n",
        "datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "datasize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wmYMjSIyL2F8"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "metadata": {
        "id": "CS-oQ9TQrp0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches):\n",
        "    X={}\n",
        "    Y={}\n",
        "    datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "    \n",
        "    for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "        X[seqLen]=[]\n",
        "        Y[seqLen]=[]\n",
        "\n",
        "\n",
        "        for dataidx in range(int(datasize/(maxSeqSize-minSeqSize+1))):\n",
        "\n",
        "            # aSeq:\n",
        "            #       - seqLen x 2\n",
        "            #       - col 1: zero and col 2: numbers\n",
        "\n",
        "            # X\n",
        "            seqBase=np.random.uniform(size=(1,seqLen-1))\n",
        "            aSeq=np.concatenate((np.zeros((1,seqLen-1),dtype=np.float32),seqBase),axis=0)\n",
        "            aSeq=aSeq.T\n",
        "            aSeq=np.concatenate((np.array([[1,0]],dtype=np.float32),aSeq),axis=0)\n",
        "            X[seqLen]+=[aSeq]\n",
        "\n",
        "            # Y\n",
        "            aRec=[]\n",
        "            for e in np.sort(seqBase[0]):\n",
        "                idx=list(seqBase[0]).index(e)\n",
        "                aRec+=[np.zeros(seqLen,dtype=np.float32)]\n",
        "                aRec[-1][idx+1]=1\n",
        "            aRec+=[np.zeros(seqLen,dtype=np.float32)]\n",
        "            aRec[-1][0]=1\n",
        "            Y[seqLen]+=[aRec]\n",
        "\n",
        "\n",
        "        X[seqLen]=np.array(X[seqLen],dtype=np.float32)\n",
        "        Y[seqLen]=np.array(Y[seqLen],dtype=np.float32)\n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NuO8Af9grp0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainX,trainY=makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsYIS0Terp0t",
        "colab_type": "code",
        "outputId": "3942efa8-7519-45fe-8665-e685327a4af0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('Train X for',minSeqSize,':',trainX[minSeqSize].shape)\n",
        "print('Train Y for',minSeqSize,':',trainY[minSeqSize].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train X for 3 : (1600, 3, 2)\n",
            "Train Y for 3 : (1600, 3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "97GlpqgvL8PK"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder & Decoder Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uqAZrGBMTGDj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, hidden_dimensions):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = layers.CuDNNLSTM(hidden_dimensions, return_sequences=True, return_state=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        output, state_h, state_c  = self.lstm(x)        \n",
        "        return output, [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tLNzTuJfWPhk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "     def __init__(self, hidden_dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = layers.CuDNNLSTM(hidden_dimensions, return_sequences=True, return_state=True)\n",
        "     \n",
        "     def call(self, x, hidden_states):\n",
        "        dec_output, state_h, state_c  = self.lstm(x, initial_state=hidden_states)\n",
        "        # dec_output shape -> (batch_size, 1, hidden_dimension)\n",
        "\n",
        "        return dec_output, [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eTSf9SdkBjcM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.Model):\n",
        "     def __init__(self, hidden_dimensions):\n",
        "        super(Attention, self).__init__()\n",
        "        # Note: Dense layer -> dot(input, kernel) -> so now Ui = vT . tanh(W1 . e + W2 . di)  becomes Ui = tanh(e . W1 + di . W2) . v\n",
        "        self.W1 = tf.keras.layers.Dense(hidden_dimensions, use_bias=False) # weights -> (256, 256)\n",
        "        self.W2 = tf.keras.layers.Dense(hidden_dimensions, use_bias=False) # weights -> (256, 256)\n",
        "        self.V = tf.keras.layers.Dense(1, use_bias=False) # weights -> (256, 1)\n",
        "        \n",
        "     \n",
        "     def call(self, encoder_outputs, dec_output):\n",
        "        # encoder_outputs shape -> (batch_size, input_sequence_length, hidden_dimension) -> (32, 9, 256)\n",
        "        # dec_output shape -> (batch_size, 1, hidden_dimension) -> (32, 1, 256)\n",
        "\n",
        "        # w1_e -> (*, 9, 256) * (*, 256, 256) -> (*, 9, 256)\n",
        "        w1_e = self.W1(encoder_outputs)\n",
        "        \n",
        "        # w2_e -> (*, 1, 256) * (*, 256, 256) -> (*, 1, 256)\n",
        "        w2_d = self.W2(dec_output)\n",
        "        \n",
        "        # tanh_output -> (*, 9, 256) + (*, 1, 256) -> (*, 9, 256)\n",
        "        tanh_output = tf.nn.tanh(w1_e + w2_d)\n",
        "        \n",
        "        # tanh_output -> (*, 9, 256) + (*, 256, 1) -> (*, 9, 1)\n",
        "        v_dot_tanh = self.V(tanh_output)\n",
        "        \n",
        "        # attention_weights -> (batch_size, input_sequence_length, 1) -> (32, 9, 1)\n",
        "        attention_weights = tf.nn.softmax(v_dot_tanh, axis=1)\n",
        "        \n",
        "        return tf.reshape(attention_weights, (attention_weights.shape[0], attention_weights.shape[1])) # (batch_size, input_sequence_length) -> (32, 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "w_shAefxqfVs"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize encoder and decoder network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xLlEdEXmVFGq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(hidden_dimensions)\n",
        "decoder = Decoder(hidden_dimensions)\n",
        "attention = Attention(hidden_dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiwTmjWWrp02",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch Infer Function"
      ]
    },
    {
      "metadata": {
        "id": "N0C6_Sz_rp03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def infer(encoder,decoder,attention,X,Y,verbose=0):\n",
        "    loss=0\n",
        "    \n",
        "    if verbose==2:\n",
        "        print('X:')\n",
        "        print(X)\n",
        "        print('Y:')\n",
        "        print(Y)\n",
        "\n",
        "    #Initialize values\n",
        "    attention_vector_array = []\n",
        "    final_output_sequence = []\n",
        "\n",
        "    encoder_input, target_data=X,Y\n",
        "    # encoder_input -> (batch_size, input_sequence_length, input_dimension)\n",
        "    # target_data -> (batch_size, input_sequence_length, input_sequence_length)\n",
        "\n",
        "    # run the decoder\n",
        "    encoder_outputs, encoder_states = encoder(encoder_input)\n",
        "\n",
        "    # prepare initial decoder input\n",
        "    dec_input = tf.convert_to_tensor(np.array([[[0,0]]]*X.shape[0],dtype=np.float32))\n",
        "    # dec_input -> (batch_size, 1, input_dimension)\n",
        "\n",
        "    # loading the final encoder states to decoder network as initial hidden states\n",
        "    decoder_states = encoder_states\n",
        "    # decoder states -> (batch_size, hidden_state_dimension)\n",
        "\n",
        "    # iterrate over the times of input sequence size\n",
        "    if verbose==2:\n",
        "        print('\\n\\nPREDICTIONS')\n",
        "        print('-----------')\n",
        "    \n",
        "    result=[]\n",
        "    for i in range(0, encoder_input.shape[1]):\n",
        "        result+=[[0]*encoder_input.shape[0]]\n",
        "        # run decoder's single step for one input\n",
        "        decoder_output, decoder_states = decoder(dec_input, decoder_states)\n",
        "        # decoder outputs -> (batch_size, input_lenght=1, hidden_state_dimension)\n",
        "        # decoder states -> (batch_size, hidden_state_dimension)\n",
        "\n",
        "        # target prediction -> (batch_size, input_sequence_length)\n",
        "        # target prediction points to one of the input sequence element -> element with highest value\n",
        "        target_prediction = attention(encoder_outputs, decoder_output)\n",
        "\n",
        "        # used for training the network by using the target data\n",
        "        ei_slice_stack=[]\n",
        "        tarCounter=0\n",
        "        for tar_data_slice in target_prediction:\n",
        "            ei_slice_stack+=[encoder_input[tarCounter,np.argmax(tar_data_slice)]]\n",
        "            tarCounter+=1\n",
        "        ei_slice_stack=tf.convert_to_tensor(np.array(ei_slice_stack,dtype=np.float32))\n",
        "\n",
        "        if verbose==2:\n",
        "            outputCounter=0\n",
        "            print('\\nFor position:',i)\n",
        "            for x in X:\n",
        "                print(\"\\tInput %d, Predicted:%d->%s\"%(outputCounter,np.argmax(target_prediction[outputCounter]), str(x[np.argmax(target_prediction[outputCounter])])),\n",
        "                     '\\tExpected:%d->%s:'%(np.argmax(target_data[outputCounter,i]),encoder_input[outputCounter,np.argmax(target_data[outputCounter,i])]))\n",
        "                outputCounter+=1\n",
        "        if verbose==1:\n",
        "            outputCounter=0\n",
        "            for x in X:\n",
        "                result[-1][outputCounter]=encoder_input[outputCounter,np.argmax(target_prediction[outputCounter])]\n",
        "                outputCounter+=1\n",
        "\n",
        "\n",
        "        loss += tf.reduce_mean(tf.keras.backend.categorical_crossentropy(target_data[:,i], target_prediction))\n",
        "        \n",
        "        # load the input state to decoder network for next prediction\n",
        "        dec_input = tf.expand_dims(ei_slice_stack, 1)\n",
        "\n",
        "    if verbose==1:\n",
        "        outputCounter=-1\n",
        "        for x in X:\n",
        "            outputCounter+=1\n",
        "            print('\\n===== PREDICTED:',outputCounter,'=====\\n')\n",
        "            for i in range(0, encoder_input.shape[1]):\n",
        "                print('\\t-',result[i][outputCounter])\n",
        "            print('\\n--- ACTUAL:',outputCounter,'---\\n')\n",
        "            for i in range(0, encoder_input.shape[1]):\n",
        "                print('\\t-',encoder_input[outputCounter][np.argmax(target_data[outputCounter][i])])\n",
        "\n",
        "            \n",
        "    return loss.numpy()/(X.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LkM2Lp_zq3VK"
      },
      "cell_type": "markdown",
      "source": [
        "#### Output of network before training"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "TRc1TdSrrp05",
        "colab_type": "code",
        "outputId": "3b0da33e-f464-444f-ceeb-9870682cf86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# Use data point\n",
        "seqLen=minSeqSize\n",
        "infer(encoder,decoder,attention,trainX[seqLen][0:2],trainY[seqLen][0:2],1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.37454012]\n",
            "\t- [0.        0.9507143]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.        0.5986585]\n",
            "\t- [0.        0.7319939]\n",
            "\t- [1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.098620096842448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "01MfSZZoMNwM"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ]
    },
    {
      "metadata": {
        "id": "dER3y_P7rp07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Prep training data"
      ]
    },
    {
      "metadata": {
        "id": "hZegfAE5rp08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batchedDataset={}\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "    batchedDataset[seqLen]=[]\n",
        "    for aBatch in tf.data.Dataset.from_tensor_slices((trainX[seqLen],trainY[seqLen])).batch(batchSize):\n",
        "        batchedDataset[seqLen]+=[aBatch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5EutlgJrp0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Config and run training"
      ]
    },
    {
      "metadata": {
        "id": "sA7UgeVErp0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lastepoch=0\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "loss_history = []\n",
        "wholeLoss_history=[]\n",
        "devLoss_history=[]\n",
        "total_attention = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYNF6TS2rp1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "devBatchSize=64\n",
        "overfitFlag=False\n",
        "tolWindow=5\n",
        "tol=0.5\n",
        "simtolWindow=10\n",
        "simtolFraction=0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DT5Eb-mbrp1D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "overfitFlag=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "HrfsigX4rp1F",
        "colab_type": "code",
        "outputId": "0933c8f7-8d01-4075-9953-9a804c33e320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7514
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(lastepoch,epochs+lastepoch):\n",
        "    if overfitFlag:\n",
        "        break\n",
        "    print(\"NEW EPOCH:\",epoch+1)\n",
        "    lastepoch=epoch\n",
        "    for batch in range(numOfBatches):\n",
        "        wholeLoss=0\n",
        "        for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "            # init batch specific parameters\n",
        "            loss=0\n",
        "            with tf.GradientTape() as tape:            \n",
        "                # get data for this batch\n",
        "                encoder_input, target_data=batchedDataset[seqLen][batch]\n",
        "                # encoder_input -> (batch_size, input_sequence_length, input_dimension)\n",
        "                # target_data -> (batch_size, input_sequence_length, input_sequence_length)\n",
        "\n",
        "                # run the decoder\n",
        "                encoder_outputs, encoder_states = encoder(encoder_input)\n",
        "\n",
        "                # prepare initial decoder input\n",
        "                dec_input = tf.convert_to_tensor(np.array([[[0,0]]]*batchSize,dtype=np.float32))\n",
        "                # dec_input -> (batch_size, 1, input_dimension)\n",
        "\n",
        "                # loading the final encoder states to decoder network as initial hidden states\n",
        "                decoder_states = encoder_states\n",
        "                # decoder states -> (batch_size, hidden_state_dimension)\n",
        "\n",
        "                # iterrate over the times of input sequence size\n",
        "                for i in range(encoder_input.shape[1]):\n",
        "                    # run decoder's single step for one input\n",
        "                    decoder_output, decoder_states = decoder(dec_input, decoder_states)\n",
        "                    # decoder outputs -> (batch_size, input_lenght=1, hidden_state_dimension)\n",
        "                    # decoder states -> (batch_size, hidden_state_dimension)\n",
        "\n",
        "                    # target prediction -> (batch_size, input_sequence_length)\n",
        "                    # target prediction points to one of the input sequence element -> element with highest value\n",
        "                    target_prediction = attention(encoder_outputs, decoder_output)\n",
        "\n",
        "                    # used for training the network by using the target data\n",
        "                    tar_data = target_data[:, i]                \n",
        "                    tarCounter=0\n",
        "                    ei_slice_stack=[]\n",
        "                    for tar_data_slice in tar_data:\n",
        "                        ei_slice_stack+=[encoder_input[tarCounter,np.argmax(tar_data_slice)]]\n",
        "                        tarCounter+=1\n",
        "                    ei_slice_stack=tf.convert_to_tensor(np.array(ei_slice_stack,dtype=np.float32))\n",
        "\n",
        "                    # load the input state to decoder network for next prediction\n",
        "                    dec_input = tf.expand_dims(ei_slice_stack, 1)\n",
        "\n",
        "                    # loss value calculated as categorical crossentropy for output i averaged across the batch\n",
        "                    loss += tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))\n",
        "\n",
        "            # loss per output for a batch of same sized seq\n",
        "            batch_loss = (loss / (encoder_input.shape[1].value))\n",
        "\n",
        "            # combined loss per output across batches of ALL seq sizes\n",
        "            wholeLoss+=batch_loss\n",
        "\n",
        "            # fetch the trainable variables\n",
        "            variables = encoder.variables + decoder.variables\n",
        "            \n",
        "            # calculate the gradient\n",
        "            grads = tape.gradient(loss, variables)\n",
        "            \n",
        "            # update the weights of the network\n",
        "            optimizer.apply_gradients(zip(grads, variables), global_step=tf.train.get_or_create_global_step())\n",
        "            \n",
        "            # store the loss history \n",
        "            loss_history.append(batch_loss.numpy())\n",
        "\n",
        "        # normalize wholeLoss\n",
        "        wholeLoss=(wholeLoss/(maxSeqSize-minSeqSize+1))\n",
        "        \n",
        "        # store the loss history \n",
        "        wholeLoss_history.append(wholeLoss.numpy())\n",
        "        \n",
        "\n",
        "            \n",
        "        if batch % 10 == 0:\n",
        "            print(\"\\tEpoch {:03d}/{:03d}: Loss at step {:02d}: {:.9f}\".format((epoch+1), epochs, batch, tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())\n",
        "            # check if we need to stop for over-fitting or training stablizing\n",
        "            devLoss=0\n",
        "            for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "                devX,devY=makeData(seqLen,seqLen,devBatchSize,1)\n",
        "                devLoss+=infer(encoder,decoder,attention,devX[seqLen],devY[seqLen])\n",
        "            devLoss=(devLoss/(maxSeqSize-minSeqSize+1))\n",
        "            devLoss_history.append(devLoss)\n",
        "\n",
        "            if abs(wholeLoss-devLoss)/wholeLoss>tol and devLoss>wholeLoss:\n",
        "                print('Dev loss too different than Train loss. Stopping Training')\n",
        "                overfitFlag=True\n",
        "                break\n",
        "\n",
        "#             if np.argmin(np.array(wholeLoss_history))<len(wholeLoss_history)-tolWindow:\n",
        "#                 print('Train loss seems to be going up. Stopping Training')\n",
        "#                 overfitFlag=True\n",
        "#                 break            \n",
        "\n",
        "            print('\\t               WholeLoss:',np.round(wholeLoss.numpy(),5),'DevLoss:',np.round(devLoss,5))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Epoch {:03d}/{:03d} completed \\t - \\tBatch loss: {:.9f}\".format((epoch+1), epochs, tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())\n",
        "print(\"Final loss: {:.9f}\".format(tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEW EPOCH: 1\n",
            "\tEpoch 001/020: Loss at step 00: 1.774850845 2019-04-27 22:25:11.753694\n",
            "\t               WholeLoss: 1.47156 DevLoss: 1.47153\n",
            "\tEpoch 001/020: Loss at step 10: 1.019877434 2019-04-27 22:25:18.128177\n",
            "\t               WholeLoss: 1.37659 DevLoss: 1.3994\n",
            "\tEpoch 001/020: Loss at step 20: 0.606992483 2019-04-27 22:25:24.142205\n",
            "\t               WholeLoss: 1.2274 DevLoss: 1.20721\n",
            "\tEpoch 001/020: Loss at step 30: 0.360908806 2019-04-27 22:25:30.205328\n",
            "\t               WholeLoss: 1.06723 DevLoss: 1.07735\n",
            "\tEpoch 001/020: Loss at step 40: 0.144467443 2019-04-27 22:25:36.192963\n",
            "\t               WholeLoss: 0.99634 DevLoss: 0.99687\n",
            "\tEpoch 001/020: Loss at step 50: 0.089069478 2019-04-27 22:25:42.304153\n",
            "\t               WholeLoss: 0.97623 DevLoss: 0.97299\n",
            "\tEpoch 001/020: Loss at step 60: 0.067138426 2019-04-27 22:25:48.303545\n",
            "\t               WholeLoss: 0.96076 DevLoss: 0.95864\n",
            "\tEpoch 001/020: Loss at step 70: 0.049373575 2019-04-27 22:25:54.371293\n",
            "\t               WholeLoss: 0.94746 DevLoss: 0.94705\n",
            "\tEpoch 001/020: Loss at step 80: 0.045881964 2019-04-27 22:26:00.416763\n",
            "\t               WholeLoss: 0.93819 DevLoss: 0.93854\n",
            "\tEpoch 001/020: Loss at step 90: 0.043956619 2019-04-27 22:26:06.407413\n",
            "\t               WholeLoss: 0.91781 DevLoss: 0.921\n",
            "Epoch 001/020 completed \t - \tBatch loss: 0.047867622 2019-04-27 22:26:11.946657\n",
            "NEW EPOCH: 2\n",
            "\tEpoch 002/020: Loss at step 00: 0.049559254 2019-04-27 22:26:12.513739\n",
            "\t               WholeLoss: 0.90343 DevLoss: 0.89828\n",
            "\tEpoch 002/020: Loss at step 10: 0.045191757 2019-04-27 22:26:18.527993\n",
            "\t               WholeLoss: 0.88628 DevLoss: 0.88224\n",
            "\tEpoch 002/020: Loss at step 20: 0.057173252 2019-04-27 22:26:24.581834\n",
            "\t               WholeLoss: 0.85658 DevLoss: 0.86148\n",
            "\tEpoch 002/020: Loss at step 30: 0.049040981 2019-04-27 22:26:31.478257\n",
            "\t               WholeLoss: 0.84928 DevLoss: 0.83451\n",
            "\tEpoch 002/020: Loss at step 40: 0.066904195 2019-04-27 22:26:38.019538\n",
            "\t               WholeLoss: 0.82534 DevLoss: 0.8166\n",
            "\tEpoch 002/020: Loss at step 50: 0.046279393 2019-04-27 22:26:44.073235\n",
            "\t               WholeLoss: 0.82142 DevLoss: 0.80011\n",
            "\tEpoch 002/020: Loss at step 60: 0.046644472 2019-04-27 22:26:50.082614\n",
            "\t               WholeLoss: 0.77062 DevLoss: 0.78363\n",
            "\tEpoch 002/020: Loss at step 70: 0.048393257 2019-04-27 22:26:56.156572\n",
            "\t               WholeLoss: 0.77173 DevLoss: 0.76531\n",
            "\tEpoch 002/020: Loss at step 80: 0.051685862 2019-04-27 22:27:02.706871\n",
            "\t               WholeLoss: 0.77112 DevLoss: 0.77037\n",
            "\tEpoch 002/020: Loss at step 90: 0.049101442 2019-04-27 22:27:09.169957\n",
            "\t               WholeLoss: 0.7713 DevLoss: 0.75239\n",
            "Epoch 002/020 completed \t - \tBatch loss: 0.054996334 2019-04-27 22:27:14.612073\n",
            "NEW EPOCH: 3\n",
            "\tEpoch 003/020: Loss at step 00: 0.051379085 2019-04-27 22:27:15.184412\n",
            "\t               WholeLoss: 0.74097 DevLoss: 0.75017\n",
            "\tEpoch 003/020: Loss at step 10: 0.047669135 2019-04-27 22:27:21.208726\n",
            "\t               WholeLoss: 0.71367 DevLoss: 0.72586\n",
            "\tEpoch 003/020: Loss at step 20: 0.052878663 2019-04-27 22:27:27.202130\n",
            "\t               WholeLoss: 0.73552 DevLoss: 0.7232\n",
            "\tEpoch 003/020: Loss at step 30: 0.054459035 2019-04-27 22:27:33.250938\n",
            "\t               WholeLoss: 0.73286 DevLoss: 0.71861\n",
            "\tEpoch 003/020: Loss at step 40: 0.057315715 2019-04-27 22:27:39.219481\n",
            "\t               WholeLoss: 0.70092 DevLoss: 0.70331\n",
            "\tEpoch 003/020: Loss at step 50: 0.048204105 2019-04-27 22:27:45.253708\n",
            "\t               WholeLoss: 0.68467 DevLoss: 0.69415\n",
            "\tEpoch 003/020: Loss at step 60: 0.051052779 2019-04-27 22:27:52.294619\n",
            "\t               WholeLoss: 0.66306 DevLoss: 0.68265\n",
            "\tEpoch 003/020: Loss at step 70: 0.052001245 2019-04-27 22:27:58.755404\n",
            "\t               WholeLoss: 0.68473 DevLoss: 0.67099\n",
            "\tEpoch 003/020: Loss at step 80: 0.054848269 2019-04-27 22:28:04.843627\n",
            "\t               WholeLoss: 0.68192 DevLoss: 0.6451\n",
            "\tEpoch 003/020: Loss at step 90: 0.064443462 2019-04-27 22:28:10.837235\n",
            "\t               WholeLoss: 0.68725 DevLoss: 0.6522\n",
            "Epoch 003/020 completed \t - \tBatch loss: 0.057487801 2019-04-27 22:28:17.609662\n",
            "NEW EPOCH: 4\n",
            "\tEpoch 004/020: Loss at step 00: 0.055066474 2019-04-27 22:28:18.318877\n",
            "\t               WholeLoss: 0.64592 DevLoss: 0.64001\n",
            "\tEpoch 004/020: Loss at step 10: 0.053708158 2019-04-27 22:28:24.969689\n",
            "\t               WholeLoss: 0.61686 DevLoss: 0.62318\n",
            "\tEpoch 004/020: Loss at step 20: 0.057368897 2019-04-27 22:28:30.969565\n",
            "\t               WholeLoss: 0.6561 DevLoss: 0.62922\n",
            "\tEpoch 004/020: Loss at step 30: 0.059194520 2019-04-27 22:28:37.037672\n",
            "\t               WholeLoss: 0.64187 DevLoss: 0.6304\n",
            "\tEpoch 004/020: Loss at step 40: 0.068625718 2019-04-27 22:28:43.049638\n",
            "\t               WholeLoss: 0.6486 DevLoss: 0.66016\n",
            "\tEpoch 004/020: Loss at step 50: 0.050764970 2019-04-27 22:28:49.114294\n",
            "\t               WholeLoss: 0.60181 DevLoss: 0.64305\n",
            "\tEpoch 004/020: Loss at step 60: 0.059482880 2019-04-27 22:28:55.134790\n",
            "\t               WholeLoss: 0.5677 DevLoss: 0.58577\n",
            "\tEpoch 004/020: Loss at step 70: 0.058746509 2019-04-27 22:29:01.183770\n",
            "\t               WholeLoss: 0.58779 DevLoss: 0.58521\n",
            "\tEpoch 004/020: Loss at step 80: 0.056045808 2019-04-27 22:29:07.400174\n",
            "\t               WholeLoss: 0.60164 DevLoss: 0.57316\n",
            "\tEpoch 004/020: Loss at step 90: 0.056064971 2019-04-27 22:29:14.388369\n",
            "\t               WholeLoss: 0.65141 DevLoss: 0.55843\n",
            "Epoch 004/020 completed \t - \tBatch loss: 0.056860466 2019-04-27 22:29:20.109825\n",
            "NEW EPOCH: 5\n",
            "\tEpoch 005/020: Loss at step 00: 0.062046282 2019-04-27 22:29:20.669834\n",
            "\t               WholeLoss: 0.58084 DevLoss: 0.56509\n",
            "\tEpoch 005/020: Loss at step 10: 0.055967376 2019-04-27 22:29:26.680879\n",
            "\t               WholeLoss: 0.55237 DevLoss: 0.54292\n",
            "\tEpoch 005/020: Loss at step 20: 0.061315961 2019-04-27 22:29:32.750425\n",
            "\t               WholeLoss: 0.58704 DevLoss: 0.5751\n",
            "\tEpoch 005/020: Loss at step 30: 0.058770929 2019-04-27 22:29:38.770739\n",
            "\t               WholeLoss: 0.54548 DevLoss: 0.5379\n",
            "\tEpoch 005/020: Loss at step 40: 0.067729622 2019-04-27 22:29:44.851870\n",
            "\t               WholeLoss: 0.59658 DevLoss: 0.57673\n",
            "\tEpoch 005/020: Loss at step 50: 0.055210177 2019-04-27 22:29:50.832626\n",
            "\t               WholeLoss: 0.52015 DevLoss: 0.53402\n",
            "\tEpoch 005/020: Loss at step 60: 0.058769464 2019-04-27 22:29:56.900500\n",
            "\t               WholeLoss: 0.50495 DevLoss: 0.5295\n",
            "\tEpoch 005/020: Loss at step 70: 0.060874980 2019-04-27 22:30:02.924372\n",
            "\t               WholeLoss: 0.53838 DevLoss: 0.52479\n",
            "\tEpoch 005/020: Loss at step 80: 0.057514835 2019-04-27 22:30:08.951695\n",
            "\t               WholeLoss: 0.58503 DevLoss: 0.52899\n",
            "\tEpoch 005/020: Loss at step 90: 0.058242172 2019-04-27 22:30:15.013760\n",
            "\t               WholeLoss: 0.55103 DevLoss: 0.50505\n",
            "Epoch 005/020 completed \t - \tBatch loss: 0.053790566 2019-04-27 22:30:20.441648\n",
            "NEW EPOCH: 6\n",
            "\tEpoch 006/020: Loss at step 00: 0.055258930 2019-04-27 22:30:21.016472\n",
            "\t               WholeLoss: 0.51002 DevLoss: 0.54366\n",
            "\tEpoch 006/020: Loss at step 10: 0.054122910 2019-04-27 22:30:27.264634\n",
            "\t               WholeLoss: 0.48851 DevLoss: 0.52808\n",
            "\tEpoch 006/020: Loss at step 20: 0.055881500 2019-04-27 22:30:34.222717\n",
            "\t               WholeLoss: 0.55237 DevLoss: 0.49365\n",
            "\tEpoch 006/020: Loss at step 30: 0.052843727 2019-04-27 22:30:40.594767\n",
            "\t               WholeLoss: 0.55049 DevLoss: 0.56423\n",
            "\tEpoch 006/020: Loss at step 40: 0.061332669 2019-04-27 22:30:46.604642\n",
            "\t               WholeLoss: 0.54264 DevLoss: 0.58312\n",
            "\tEpoch 006/020: Loss at step 50: 0.055760793 2019-04-27 22:30:52.629623\n",
            "\t               WholeLoss: 0.4766 DevLoss: 0.50146\n",
            "\tEpoch 006/020: Loss at step 60: 0.056392938 2019-04-27 22:30:58.717427\n",
            "\t               WholeLoss: 0.46736 DevLoss: 0.47812\n",
            "\tEpoch 006/020: Loss at step 70: 0.056156233 2019-04-27 22:31:04.702556\n",
            "\t               WholeLoss: 0.49299 DevLoss: 0.50444\n",
            "\tEpoch 006/020: Loss at step 80: 0.056404755 2019-04-27 22:31:10.777286\n",
            "\t               WholeLoss: 0.53017 DevLoss: 0.49086\n",
            "\tEpoch 006/020: Loss at step 90: 0.056133077 2019-04-27 22:31:16.793648\n",
            "\t               WholeLoss: 0.51336 DevLoss: 0.45779\n",
            "Epoch 006/020 completed \t - \tBatch loss: 0.051183172 2019-04-27 22:31:22.301079\n",
            "NEW EPOCH: 7\n",
            "\tEpoch 007/020: Loss at step 00: 0.051824875 2019-04-27 22:31:22.861023\n",
            "\t               WholeLoss: 0.48357 DevLoss: 0.48308\n",
            "\tEpoch 007/020: Loss at step 10: 0.051985685 2019-04-27 22:31:28.844147\n",
            "\t               WholeLoss: 0.46068 DevLoss: 0.49386\n",
            "\tEpoch 007/020: Loss at step 20: 0.056070805 2019-04-27 22:31:34.832275\n",
            "\t               WholeLoss: 0.49582 DevLoss: 0.46551\n",
            "\tEpoch 007/020: Loss at step 30: 0.051137924 2019-04-27 22:31:40.921005\n",
            "\t               WholeLoss: 0.45372 DevLoss: 0.45084\n",
            "\tEpoch 007/020: Loss at step 40: 0.056791309 2019-04-27 22:31:46.950018\n",
            "\t               WholeLoss: 0.45263 DevLoss: 0.45856\n",
            "\tEpoch 007/020: Loss at step 50: 0.049921993 2019-04-27 22:31:54.012780\n",
            "\t               WholeLoss: 0.46607 DevLoss: 0.52413\n",
            "\tEpoch 007/020: Loss at step 60: 0.055059139 2019-04-27 22:32:00.412858\n",
            "\t               WholeLoss: 0.44423 DevLoss: 0.51143\n",
            "\tEpoch 007/020: Loss at step 70: 0.055778578 2019-04-27 22:32:06.808467\n",
            "\t               WholeLoss: 0.45111 DevLoss: 0.49344\n",
            "\tEpoch 007/020: Loss at step 80: 0.055024259 2019-04-27 22:32:13.432572\n",
            "\t               WholeLoss: 0.50073 DevLoss: 0.46326\n",
            "\tEpoch 007/020: Loss at step 90: 0.052534185 2019-04-27 22:32:19.475933\n",
            "\t               WholeLoss: 0.46958 DevLoss: 0.43052\n",
            "Epoch 007/020 completed \t - \tBatch loss: 0.051425412 2019-04-27 22:32:24.932100\n",
            "NEW EPOCH: 8\n",
            "\tEpoch 008/020: Loss at step 00: 0.050179444 2019-04-27 22:32:25.492330\n",
            "\t               WholeLoss: 0.44993 DevLoss: 0.45348\n",
            "\tEpoch 008/020: Loss at step 10: 0.052769564 2019-04-27 22:32:31.492406\n",
            "\t               WholeLoss: 0.46051 DevLoss: 0.46028\n",
            "\tEpoch 008/020: Loss at step 20: 0.057229772 2019-04-27 22:32:37.548604\n",
            "\t               WholeLoss: 0.49179 DevLoss: 0.43471\n",
            "\tEpoch 008/020: Loss at step 30: 0.052225091 2019-04-27 22:32:43.543627\n",
            "\t               WholeLoss: 0.43318 DevLoss: 0.42734\n",
            "\tEpoch 008/020: Loss at step 40: 0.061887652 2019-04-27 22:32:49.590757\n",
            "\t               WholeLoss: 0.54394 DevLoss: 0.54442\n",
            "\tEpoch 008/020: Loss at step 50: 0.056882955 2019-04-27 22:32:55.607427\n",
            "\t               WholeLoss: 0.43429 DevLoss: 0.44134\n",
            "\tEpoch 008/020: Loss at step 60: 0.055899519 2019-04-27 22:33:01.708951\n",
            "\t               WholeLoss: 0.42057 DevLoss: 0.4411\n",
            "\tEpoch 008/020: Loss at step 70: 0.057316225 2019-04-27 22:33:07.785499\n",
            "\t               WholeLoss: 0.43883 DevLoss: 0.43272\n",
            "\tEpoch 008/020: Loss at step 80: 0.058192968 2019-04-27 22:33:14.790449\n",
            "\t               WholeLoss: 0.4905 DevLoss: 0.4715\n",
            "\tEpoch 008/020: Loss at step 90: 0.054225743 2019-04-27 22:33:21.190435\n",
            "\t               WholeLoss: 0.44276 DevLoss: 0.4119\n",
            "Epoch 008/020 completed \t - \tBatch loss: 0.051764883 2019-04-27 22:33:27.864294\n",
            "NEW EPOCH: 9\n",
            "\tEpoch 009/020: Loss at step 00: 0.051207453 2019-04-27 22:33:28.587650\n",
            "\t               WholeLoss: 0.43377 DevLoss: 0.40853\n",
            "\tEpoch 009/020: Loss at step 10: 0.053391203 2019-04-27 22:33:35.359607\n",
            "\t               WholeLoss: 0.43935 DevLoss: 0.44733\n",
            "\tEpoch 009/020: Loss at step 20: 0.057333071 2019-04-27 22:33:41.340902\n",
            "\t               WholeLoss: 0.47317 DevLoss: 0.44926\n",
            "\tEpoch 009/020: Loss at step 30: 0.053483937 2019-04-27 22:33:47.435468\n",
            "\t               WholeLoss: 0.4226 DevLoss: 0.4414\n",
            "\tEpoch 009/020: Loss at step 40: 0.057723720 2019-04-27 22:33:53.447594\n",
            "\t               WholeLoss: 0.46473 DevLoss: 0.45763\n",
            "\tEpoch 009/020: Loss at step 50: 0.054125443 2019-04-27 22:33:59.445174\n",
            "\t               WholeLoss: 0.39581 DevLoss: 0.40708\n",
            "\tEpoch 009/020: Loss at step 60: 0.056074593 2019-04-27 22:34:05.516321\n",
            "\t               WholeLoss: 0.41829 DevLoss: 0.42679\n",
            "\tEpoch 009/020: Loss at step 70: 0.053353228 2019-04-27 22:34:11.505558\n",
            "\t               WholeLoss: 0.40967 DevLoss: 0.40592\n",
            "\tEpoch 009/020: Loss at step 80: 0.061382502 2019-04-27 22:34:17.581959\n",
            "\t               WholeLoss: 0.45244 DevLoss: 0.41206\n",
            "\tEpoch 009/020: Loss at step 90: 0.053578436 2019-04-27 22:34:23.610726\n",
            "\t               WholeLoss: 0.46112 DevLoss: 0.39428\n",
            "Epoch 009/020 completed \t - \tBatch loss: 0.051003493 2019-04-27 22:34:29.374694\n",
            "NEW EPOCH: 10\n",
            "\tEpoch 010/020: Loss at step 00: 0.049427707 2019-04-27 22:34:30.027239\n",
            "\t               WholeLoss: 0.40694 DevLoss: 0.4141\n",
            "\tEpoch 010/020: Loss at step 10: 0.053151250 2019-04-27 22:34:37.048658\n",
            "\t               WholeLoss: 0.42579 DevLoss: 0.43693\n",
            "\tEpoch 010/020: Loss at step 20: 0.060126387 2019-04-27 22:34:43.192461\n",
            "\t               WholeLoss: 0.4231 DevLoss: 0.43149\n",
            "\tEpoch 010/020: Loss at step 30: 0.052953985 2019-04-27 22:34:49.244540\n",
            "\t               WholeLoss: 0.42302 DevLoss: 0.39518\n",
            "\tEpoch 010/020: Loss at step 40: 0.055435024 2019-04-27 22:34:55.244897\n",
            "\t               WholeLoss: 0.44826 DevLoss: 0.43252\n",
            "\tEpoch 010/020: Loss at step 50: 0.054383963 2019-04-27 22:35:01.339649\n",
            "\t               WholeLoss: 0.38752 DevLoss: 0.41533\n",
            "\tEpoch 010/020: Loss at step 60: 0.055544879 2019-04-27 22:35:07.342026\n",
            "\t               WholeLoss: 0.38825 DevLoss: 0.42351\n",
            "\tEpoch 010/020: Loss at step 70: 0.053154670 2019-04-27 22:35:13.419110\n",
            "\t               WholeLoss: 0.39739 DevLoss: 0.43916\n",
            "\tEpoch 010/020: Loss at step 80: 0.058942407 2019-04-27 22:35:19.415460\n",
            "\t               WholeLoss: 0.45899 DevLoss: 0.41057\n",
            "\tEpoch 010/020: Loss at step 90: 0.051185083 2019-04-27 22:35:25.472406\n",
            "\t               WholeLoss: 0.4322 DevLoss: 0.41543\n",
            "Epoch 010/020 completed \t - \tBatch loss: 0.052198786 2019-04-27 22:35:30.912566\n",
            "NEW EPOCH: 11\n",
            "\tEpoch 011/020: Loss at step 00: 0.049765028 2019-04-27 22:35:31.479273\n",
            "\t               WholeLoss: 0.40208 DevLoss: 0.38431\n",
            "\tEpoch 011/020: Loss at step 10: 0.052600116 2019-04-27 22:35:37.448736\n",
            "\t               WholeLoss: 0.3915 DevLoss: 0.41711\n",
            "\tEpoch 011/020: Loss at step 20: 0.057370380 2019-04-27 22:35:43.526563\n",
            "\t               WholeLoss: 0.40837 DevLoss: 0.3855\n",
            "\tEpoch 011/020: Loss at step 30: 0.052354284 2019-04-27 22:35:49.763227\n",
            "\t               WholeLoss: 0.43337 DevLoss: 0.40642\n",
            "\tEpoch 011/020: Loss at step 40: 0.052890461 2019-04-27 22:35:56.821564\n",
            "\t               WholeLoss: 0.3894 DevLoss: 0.38946\n",
            "\tEpoch 011/020: Loss at step 50: 0.054554485 2019-04-27 22:36:03.013065\n",
            "\t               WholeLoss: 0.38868 DevLoss: 0.39833\n",
            "\tEpoch 011/020: Loss at step 60: 0.055265427 2019-04-27 22:36:09.075692\n",
            "\t               WholeLoss: 0.3796 DevLoss: 0.40285\n",
            "\tEpoch 011/020: Loss at step 70: 0.052481528 2019-04-27 22:36:15.151424\n",
            "\t               WholeLoss: 0.41507 DevLoss: 0.51807\n",
            "\tEpoch 011/020: Loss at step 80: 0.053830974 2019-04-27 22:36:21.164508\n",
            "\t               WholeLoss: 0.40941 DevLoss: 0.37389\n",
            "\tEpoch 011/020: Loss at step 90: 0.051741008 2019-04-27 22:36:27.217010\n",
            "\t               WholeLoss: 0.42978 DevLoss: 0.38349\n",
            "Epoch 011/020 completed \t - \tBatch loss: 0.052082200 2019-04-27 22:36:32.659214\n",
            "NEW EPOCH: 12\n",
            "\tEpoch 012/020: Loss at step 00: 0.050244227 2019-04-27 22:36:33.239470\n",
            "\t               WholeLoss: 0.37692 DevLoss: 0.36367\n",
            "\tEpoch 012/020: Loss at step 10: 0.053558283 2019-04-27 22:36:39.296068\n",
            "\t               WholeLoss: 0.36748 DevLoss: 0.39029\n",
            "\tEpoch 012/020: Loss at step 20: 0.052951127 2019-04-27 22:36:45.308209\n",
            "\t               WholeLoss: 0.4348 DevLoss: 0.43335\n",
            "\tEpoch 012/020: Loss at step 30: 0.053426065 2019-04-27 22:36:51.380720\n",
            "\t               WholeLoss: 0.3939 DevLoss: 0.38039\n",
            "\tEpoch 012/020: Loss at step 40: 0.050545275 2019-04-27 22:36:57.372522\n",
            "\t               WholeLoss: 0.41547 DevLoss: 0.41305\n",
            "\tEpoch 012/020: Loss at step 50: 0.054145195 2019-04-27 22:37:03.438832\n",
            "\t               WholeLoss: 0.35744 DevLoss: 0.3817\n",
            "\tEpoch 012/020: Loss at step 60: 0.054105334 2019-04-27 22:37:09.611186\n",
            "\t               WholeLoss: 0.38654 DevLoss: 0.41409\n",
            "\tEpoch 012/020: Loss at step 70: 0.052219070 2019-04-27 22:37:16.890342\n",
            "\t               WholeLoss: 0.36645 DevLoss: 0.38528\n",
            "\tEpoch 012/020: Loss at step 80: 0.053559862 2019-04-27 22:37:23.150767\n",
            "\t               WholeLoss: 0.40435 DevLoss: 0.38805\n",
            "\tEpoch 012/020: Loss at step 90: 0.051519096 2019-04-27 22:37:29.136284\n",
            "\t               WholeLoss: 0.38593 DevLoss: 0.36222\n",
            "Epoch 012/020 completed \t - \tBatch loss: 0.053157881 2019-04-27 22:37:34.619236\n",
            "NEW EPOCH: 13\n",
            "\tEpoch 013/020: Loss at step 00: 0.051343545 2019-04-27 22:37:35.192474\n",
            "\t               WholeLoss: 0.3652 DevLoss: 0.3858\n",
            "\tEpoch 013/020: Loss at step 10: 0.052364286 2019-04-27 22:37:41.177568\n",
            "\t               WholeLoss: 0.41751 DevLoss: 0.41053\n",
            "\tEpoch 013/020: Loss at step 20: 0.056137044 2019-04-27 22:37:47.244562\n",
            "\t               WholeLoss: 0.40197 DevLoss: 0.404\n",
            "\tEpoch 013/020: Loss at step 30: 0.051607426 2019-04-27 22:37:53.244090\n",
            "\t               WholeLoss: 0.38161 DevLoss: 0.34574\n",
            "\tEpoch 013/020: Loss at step 40: 0.052008905 2019-04-27 22:37:59.240671\n",
            "\t               WholeLoss: 0.41301 DevLoss: 0.39812\n",
            "\tEpoch 013/020: Loss at step 50: 0.052804440 2019-04-27 22:38:05.332542\n",
            "\t               WholeLoss: 0.3496 DevLoss: 0.37495\n",
            "\tEpoch 013/020: Loss at step 60: 0.053958092 2019-04-27 22:38:11.310263\n",
            "\t               WholeLoss: 0.48854 DevLoss: 0.43596\n",
            "\tEpoch 013/020: Loss at step 70: 0.051184542 2019-04-27 22:38:17.390474\n",
            "\t               WholeLoss: 0.38423 DevLoss: 0.4214\n",
            "\tEpoch 013/020: Loss at step 80: 0.052602284 2019-04-27 22:38:23.371981\n",
            "\t               WholeLoss: 0.39734 DevLoss: 0.3555\n",
            "\tEpoch 013/020: Loss at step 90: 0.052151904 2019-04-27 22:38:29.593006\n",
            "\t               WholeLoss: 0.48438 DevLoss: 0.39147\n",
            "Epoch 013/020 completed \t - \tBatch loss: 0.052276418 2019-04-27 22:38:36.240633\n",
            "NEW EPOCH: 14\n",
            "\tEpoch 014/020: Loss at step 00: 0.050299250 2019-04-27 22:38:36.958543\n",
            "\t               WholeLoss: 0.36698 DevLoss: 0.35194\n",
            "\tEpoch 014/020: Loss at step 10: 0.051372476 2019-04-27 22:38:44.053172\n",
            "\t               WholeLoss: 0.36286 DevLoss: 0.3984\n",
            "\tEpoch 014/020: Loss at step 20: 0.057017360 2019-04-27 22:38:50.138908\n",
            "\t               WholeLoss: 0.38509 DevLoss: 0.38425\n",
            "\tEpoch 014/020: Loss at step 30: 0.052025273 2019-04-27 22:38:56.171184\n",
            "\t               WholeLoss: 0.36808 DevLoss: 0.34411\n",
            "\tEpoch 014/020: Loss at step 40: 0.049925450 2019-04-27 22:39:02.241674\n",
            "\t               WholeLoss: 0.33887 DevLoss: 0.35487\n",
            "\tEpoch 014/020: Loss at step 50: 0.053820834 2019-04-27 22:39:08.233021\n",
            "\t               WholeLoss: 0.34747 DevLoss: 0.37922\n",
            "\tEpoch 014/020: Loss at step 60: 0.053164165 2019-04-27 22:39:14.300963\n",
            "\t               WholeLoss: 0.38457 DevLoss: 0.3804\n",
            "\tEpoch 014/020: Loss at step 70: 0.051704679 2019-04-27 22:39:20.272472\n",
            "\t               WholeLoss: 0.33849 DevLoss: 0.38186\n",
            "\tEpoch 014/020: Loss at step 80: 0.054521240 2019-04-27 22:39:26.342836\n",
            "\t               WholeLoss: 0.46783 DevLoss: 0.39123\n",
            "\tEpoch 014/020: Loss at step 90: 0.050808582 2019-04-27 22:39:32.340985\n",
            "\t               WholeLoss: 0.41829 DevLoss: 0.35074\n",
            "Epoch 014/020 completed \t - \tBatch loss: 0.050738346 2019-04-27 22:39:37.796515\n",
            "NEW EPOCH: 15\n",
            "\tEpoch 015/020: Loss at step 00: 0.049132019 2019-04-27 22:39:38.358137\n",
            "\t               WholeLoss: 0.35748 DevLoss: 0.34874\n",
            "\tEpoch 015/020: Loss at step 10: 0.052168764 2019-04-27 22:39:44.434894\n",
            "\t               WholeLoss: 0.38746 DevLoss: 0.40466\n",
            "\tEpoch 015/020: Loss at step 20: 0.054874957 2019-04-27 22:39:50.637594\n",
            "\t               WholeLoss: 0.40347 DevLoss: 0.36329\n",
            "\tEpoch 015/020: Loss at step 30: 0.052237988 2019-04-27 22:39:57.691439\n",
            "\t               WholeLoss: 0.37418 DevLoss: 0.34343\n",
            "\tEpoch 015/020: Loss at step 40: 0.049532354 2019-04-27 22:40:03.931299\n",
            "\t               WholeLoss: 0.3276 DevLoss: 0.36729\n",
            "\tEpoch 015/020: Loss at step 50: 0.053644840 2019-04-27 22:40:10.022794\n",
            "\t               WholeLoss: 0.32848 DevLoss: 0.34258\n",
            "\tEpoch 015/020: Loss at step 60: 0.052124474 2019-04-27 22:40:16.028299\n",
            "\t               WholeLoss: 0.39618 DevLoss: 0.35475\n",
            "\tEpoch 015/020: Loss at step 70: 0.050262831 2019-04-27 22:40:22.022850\n",
            "\t               WholeLoss: 0.35306 DevLoss: 0.42841\n",
            "\tEpoch 015/020: Loss at step 80: 0.052185193 2019-04-27 22:40:28.076839\n",
            "\t               WholeLoss: 0.40659 DevLoss: 0.34673\n",
            "\tEpoch 015/020: Loss at step 90: 0.052776761 2019-04-27 22:40:34.054647\n",
            "\t               WholeLoss: 0.39097 DevLoss: 0.35396\n",
            "Epoch 015/020 completed \t - \tBatch loss: 0.049804859 2019-04-27 22:40:39.539471\n",
            "NEW EPOCH: 16\n",
            "\tEpoch 016/020: Loss at step 00: 0.048148204 2019-04-27 22:40:40.107475\n",
            "\t               WholeLoss: 0.3381 DevLoss: 0.3601\n",
            "\tEpoch 016/020: Loss at step 10: 0.051831104 2019-04-27 22:40:46.111735\n",
            "\t               WholeLoss: 0.34259 DevLoss: 0.38561\n",
            "\tEpoch 016/020: Loss at step 20: 0.054323401 2019-04-27 22:40:52.225602\n",
            "\t               WholeLoss: 0.37973 DevLoss: 0.34719\n",
            "\tEpoch 016/020: Loss at step 30: 0.051675003 2019-04-27 22:40:58.227021\n",
            "\t               WholeLoss: 0.35444 DevLoss: 0.34008\n",
            "\tEpoch 016/020: Loss at step 40: 0.050720248 2019-04-27 22:41:04.241795\n",
            "\t               WholeLoss: 0.42041 DevLoss: 0.41949\n",
            "\tEpoch 016/020: Loss at step 50: 0.051494170 2019-04-27 22:41:10.406668\n",
            "\t               WholeLoss: 0.33484 DevLoss: 0.34019\n",
            "\tEpoch 016/020: Loss at step 60: 0.053343769 2019-04-27 22:41:17.399582\n",
            "\t               WholeLoss: 0.40364 DevLoss: 0.36129\n",
            "\tEpoch 016/020: Loss at step 70: 0.052599881 2019-04-27 22:41:23.766728\n",
            "\t               WholeLoss: 0.3368 DevLoss: 0.36244\n",
            "\tEpoch 016/020: Loss at step 80: 0.050042003 2019-04-27 22:41:29.782228\n",
            "\t               WholeLoss: 0.36852 DevLoss: 0.33386\n",
            "\tEpoch 016/020: Loss at step 90: 0.050371006 2019-04-27 22:41:35.827315\n",
            "\t               WholeLoss: 0.34832 DevLoss: 0.3268\n",
            "Epoch 016/020 completed \t - \tBatch loss: 0.050485514 2019-04-27 22:41:41.256878\n",
            "NEW EPOCH: 17\n",
            "\tEpoch 017/020: Loss at step 00: 0.048513245 2019-04-27 22:41:41.828976\n",
            "\t               WholeLoss: 0.34889 DevLoss: 0.34616\n",
            "\tEpoch 017/020: Loss at step 10: 0.051998168 2019-04-27 22:41:47.899054\n",
            "\t               WholeLoss: 0.33505 DevLoss: 0.37572\n",
            "\tEpoch 017/020: Loss at step 20: 0.053898633 2019-04-27 22:41:53.897893\n",
            "\t               WholeLoss: 0.35411 DevLoss: 0.35917\n",
            "\tEpoch 017/020: Loss at step 30: 0.050246075 2019-04-27 22:41:59.896888\n",
            "\t               WholeLoss: 0.36029 DevLoss: 0.35506\n",
            "\tEpoch 017/020: Loss at step 40: 0.051367968 2019-04-27 22:42:05.952019\n",
            "\t               WholeLoss: 0.31257 DevLoss: 0.33963\n",
            "\tEpoch 017/020: Loss at step 50: 0.053387523 2019-04-27 22:42:11.931262\n",
            "\t               WholeLoss: 0.32311 DevLoss: 0.35394\n",
            "\tEpoch 017/020: Loss at step 60: 0.053393930 2019-04-27 22:42:18.528624\n",
            "\t               WholeLoss: 0.43914 DevLoss: 0.43428\n",
            "\tEpoch 017/020: Loss at step 70: 0.050483815 2019-04-27 22:42:24.919749\n",
            "\t               WholeLoss: 0.33157 DevLoss: 0.37005\n",
            "\tEpoch 017/020: Loss at step 80: 0.051785067 2019-04-27 22:42:31.161843\n",
            "\t               WholeLoss: 0.36039 DevLoss: 0.313\n",
            "\tEpoch 017/020: Loss at step 90: 0.051252469 2019-04-27 22:42:38.141598\n",
            "\t               WholeLoss: 0.3519 DevLoss: 0.34794\n",
            "Epoch 017/020 completed \t - \tBatch loss: 0.050818197 2019-04-27 22:42:43.850692\n",
            "NEW EPOCH: 18\n",
            "\tEpoch 018/020: Loss at step 00: 0.049159024 2019-04-27 22:42:44.411957\n",
            "\t               WholeLoss: 0.3355 DevLoss: 0.32938\n",
            "\tEpoch 018/020: Loss at step 10: 0.051915772 2019-04-27 22:42:50.493067\n",
            "\t               WholeLoss: 0.33625 DevLoss: 0.3492\n",
            "\tEpoch 018/020: Loss at step 20: 0.056183908 2019-04-27 22:42:56.523354\n",
            "\t               WholeLoss: 0.35476 DevLoss: 0.33685\n",
            "\tEpoch 018/020: Loss at step 30: 0.051438268 2019-04-27 22:43:02.586785\n",
            "\t               WholeLoss: 0.34591 DevLoss: 0.33827\n",
            "\tEpoch 018/020: Loss at step 40: 0.050753988 2019-04-27 22:43:08.597996\n",
            "\t               WholeLoss: 0.35163 DevLoss: 0.36349\n",
            "\tEpoch 018/020: Loss at step 50: 0.052959945 2019-04-27 22:43:14.644600\n",
            "\t               WholeLoss: 0.31512 DevLoss: 0.34732\n",
            "\tEpoch 018/020: Loss at step 60: 0.054151706 2019-04-27 22:43:20.634361\n",
            "\t               WholeLoss: 0.42279 DevLoss: 0.35334\n",
            "\tEpoch 018/020: Loss at step 70: 0.051755875 2019-04-27 22:43:26.608854\n",
            "\t               WholeLoss: 0.30672 DevLoss: 0.30905\n",
            "\tEpoch 018/020: Loss at step 80: 0.052870117 2019-04-27 22:43:32.654688\n",
            "\t               WholeLoss: 0.34754 DevLoss: 0.33577\n",
            "\tEpoch 018/020: Loss at step 90: 0.050165422 2019-04-27 22:43:38.650869\n",
            "\t               WholeLoss: 0.34684 DevLoss: 0.32184\n",
            "Epoch 018/020 completed \t - \tBatch loss: 0.051619913 2019-04-27 22:43:44.561907\n",
            "NEW EPOCH: 19\n",
            "\tEpoch 019/020: Loss at step 00: 0.049117550 2019-04-27 22:43:45.284422\n",
            "\t               WholeLoss: 0.32667 DevLoss: 0.3212\n",
            "\tEpoch 019/020: Loss at step 10: 0.052255102 2019-04-27 22:43:52.796287\n",
            "\t               WholeLoss: 0.32594 DevLoss: 0.36871\n",
            "\tEpoch 019/020: Loss at step 20: 0.053840470 2019-04-27 22:43:59.875776\n",
            "\t               WholeLoss: 0.39561 DevLoss: 0.33431\n",
            "\tEpoch 019/020: Loss at step 30: 0.052113272 2019-04-27 22:44:05.981962\n",
            "\t               WholeLoss: 0.33683 DevLoss: 0.33255\n",
            "\tEpoch 019/020: Loss at step 40: 0.051234823 2019-04-27 22:44:12.073704\n",
            "\t               WholeLoss: 0.29447 DevLoss: 0.32888\n",
            "\tEpoch 019/020: Loss at step 50: 0.053441569 2019-04-27 22:44:18.137318\n",
            "\t               WholeLoss: 0.3079 DevLoss: 0.34979\n",
            "\tEpoch 019/020: Loss at step 60: 0.053604577 2019-04-27 22:44:24.183982\n",
            "\t               WholeLoss: 0.45764 DevLoss: 0.37866\n",
            "\tEpoch 019/020: Loss at step 70: 0.051511675 2019-04-27 22:44:30.328467\n",
            "\t               WholeLoss: 0.32548 DevLoss: 0.32611\n",
            "\tEpoch 019/020: Loss at step 80: 0.052815493 2019-04-27 22:44:36.383376\n",
            "\t               WholeLoss: 0.35745 DevLoss: 0.3129\n",
            "\tEpoch 019/020: Loss at step 90: 0.050614908 2019-04-27 22:44:42.506806\n",
            "\t               WholeLoss: 0.33392 DevLoss: 0.32297\n",
            "Epoch 019/020 completed \t - \tBatch loss: 0.050947934 2019-04-27 22:44:48.023992\n",
            "NEW EPOCH: 20\n",
            "\tEpoch 020/020: Loss at step 00: 0.049134552 2019-04-27 22:44:48.599785\n",
            "\t               WholeLoss: 0.34599 DevLoss: 0.35106\n",
            "\tEpoch 020/020: Loss at step 10: 0.050638422 2019-04-27 22:44:54.770081\n",
            "\t               WholeLoss: 0.32839 DevLoss: 0.33842\n",
            "\tEpoch 020/020: Loss at step 20: 0.054340202 2019-04-27 22:45:00.884169\n",
            "\t               WholeLoss: 0.3491 DevLoss: 0.31833\n",
            "\tEpoch 020/020: Loss at step 30: 0.052541371 2019-04-27 22:45:06.987994\n",
            "\t               WholeLoss: 0.32689 DevLoss: 0.33974\n",
            "\tEpoch 020/020: Loss at step 40: 0.050186083 2019-04-27 22:45:13.480623\n",
            "\t               WholeLoss: 0.29095 DevLoss: 0.3224\n",
            "\tEpoch 020/020: Loss at step 50: 0.051733710 2019-04-27 22:45:20.534206\n",
            "\t               WholeLoss: 0.33999 DevLoss: 0.34248\n",
            "\tEpoch 020/020: Loss at step 60: 0.052735545 2019-04-27 22:45:26.703427\n",
            "\t               WholeLoss: 0.35326 DevLoss: 0.34814\n",
            "\tEpoch 020/020: Loss at step 70: 0.049985789 2019-04-27 22:45:32.774670\n",
            "\t               WholeLoss: 0.32022 DevLoss: 0.35164\n",
            "\tEpoch 020/020: Loss at step 80: 0.049983777 2019-04-27 22:45:38.889900\n",
            "\t               WholeLoss: 0.3364 DevLoss: 0.3361\n",
            "\tEpoch 020/020: Loss at step 90: 0.049941316 2019-04-27 22:45:44.935123\n",
            "\t               WholeLoss: 0.32576 DevLoss: 0.3216\n",
            "Epoch 020/020 completed \t - \tBatch loss: 0.050231665 2019-04-27 22:45:50.431720\n",
            "Final loss: 0.050231665 2019-04-27 22:45:50.433257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0GCUvlzuXxya"
      },
      "cell_type": "markdown",
      "source": [
        "#### Loss plot over the entire training sequence"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "miigFwpMW_lO",
        "outputId": "8c68624b-5810-490a-d27a-50b7112f0777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_history)\n",
        "plt.ylabel('loss value')\n",
        "plt.xlabel('batches')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/8SFrh2ktywZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ySe7cdhArp1K",
        "colab_type": "code",
        "outputId": "886ebca8-11d5-41ac-ba9d-a00a5c167ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(wholeLoss_history)\n",
        "plt.ylabel('wholeLoss value')\n",
        "plt.xlabel('batches')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGOM8bKkYIwxxuv/ASWD0UQDcohHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nR8v-8CYrp1M",
        "colab_type": "code",
        "outputId": "40fd2d20-a652-40fa-cb55-fed2bc76ae55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(devLoss_history)\n",
        "plt.ylabel('devLoss value')\n",
        "plt.xlabel('batches')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4XNW18OHf0qiNepdlSVaxXMEV\nAcYYm95DC6EFAqGFG0iDJBc+SCH3pickcCEQJ5RAKCEJxRDAgAEbbFxkG/duWbZk9d7b7O+PczSW\nZJVxGY2kWe/z6PHo6MycpSN5lnZbW4wxKKWUUgABvg5AKaXU8KFJQSmllJsmBaWUUm6aFJRSSrlp\nUlBKKeWmSUEppZSbJgWllFJumhSUUkq5aVJQSinlFujrAI5UQkKCyczM9HUYSik1oqxdu7bCGJM4\n2HkjLilkZmaSl5fn6zCUUmpEEZECT87T7iOllFJumhSUUkq5aVJQSinlpklBKaWUmyYFpZRSbpoU\nlFJKuWlSUEop5eY3SWFHST2/eW87tc3tvg5FKaWGLb9JCgWVjfzpkz0UVDb6OhSllBq2/CYppMWG\nAVBY3ezjSJRSavjym6SQGusEoLC6yceRKKXU8OU3SSHaGURUaKC2FJRSagB+kxTA6kLSpKCUUv3z\ns6Tg1O4jpZQagJ8lBaulYIzxdShKKTUs+VlScNLU1kl1k65VUEqpvvhdUgCdgaSUUv3xWlIQkWdE\npExENg9y3ski0iEiV3srli6HpqXqYLNSSvXFmy2F54ALBzpBRBzAr4H3vRiHW9cCtiJNCkop1Sev\nJQVjzDKgapDTvgX8GyjzVhzdRTuDiAwN1O4jpZTqh8/GFEQkFbgSeNKDc+8UkTwRySsvLz+m66bG\nOCmqaTmm11BKqdHKlwPNfwT+2xjjGuxEY8xCY0yuMSY3MTHxmC4aFRpEY2vHMb2GUkqNVoE+vHYu\n8IqIACQAF4tIhzHmDW9eNCzEQXVjmzcvoZRSI5bPkoIxJqvrsYg8B7zt7YQAEB6s9Y+UUqo/XksK\nIvIycCaQICKFwE+AIABjzFPeuu5gnMEOmrT7SCml+uS1pGCMuf4Izr3FW3H0Fh7soLGtc6gup5RS\nI4pfrWgGCAsJpKlNWwpKKdUXv0sK4cEO2jsNbR2DTnpSSim/43dJISzY6jHT1oJSSh3O75JCeIgD\nQMcVlFKqD36XFNwtBZ2BpJRSh/G7pKAtBaWU6p/fJQVtKSilVP/8LimE20lBWwpKKXU4v0sKYXb3\nkc4+Ukqpw/ldUnC3FFq1paCUUr35XVLQloJSSvXP/5JCkD37SFsKSil1GL9LCoGOAEICA7SloJRS\nffC7pAAQHhJIoyYFpZQ6jF8mhbBgB03afaSUUofxy6QQHqwtBaWU6otfJoWwEAdNunhNKaUO45dJ\nITw4kEYtc6GUUofxy6QQFqwtBaWU6otfJgWdfaSUUn3zy6Sgs4+UUqpvfpkUtKWglFJ988ukEBbs\noKXdRafL+DoUpZQaVvwyKXRVStVSF0op1ZNfJoVDlVJ1XEEppbrzWlIQkWdEpExENvfz9a+KyEYR\n2SQiK0Rkhrdi6e3QngraUlBKqe682VJ4DrhwgK/nAwuMMdOA/wEWejGWHsKCtaWglFJ9CfTWCxtj\nlolI5gBfX9Ht05VAmrdi6S0ixPq261rah+qSSik1IgyXMYXbgHf7+6KI3CkieSKSV15efswXiw4L\nAqCuWZOCUkp15/OkICJnYSWF/+7vHGPMQmNMrjEmNzEx8ZivGRMWDECtJgWllOrBa91HnhCR6cBf\ngYuMMZVDdd1op9VSqGnSpKCUUt35rKUgIuOA14CbjDE7h/La4cEOAgOEGm0pKKVUD15rKYjIy8CZ\nQIKIFAI/AYIAjDFPAT8G4oE/iQhAhzEm11vx9IqNmLAgbSkopVQv3px9dP0gX78duN1b1x9MlDNI\nB5qVUqoXnw80+0qMM4ia5jZfh6GUUsOK/yaFsGDtPlJKqV78Nyk4g3RKqlJK9eK3SSHKGUStthSU\nUqoHv00KMWFB1Ld20N7p8nUoSik1bPhvUnBqqQullOrNf5OClrpQSqnD+G1ScJe60KSglFJu/psU\n7EqpOtislFKH+G1SiHG3FHQBm1JKdfHfpNA1pqAtBaWUcvPbpBAVapV90jEFpZQ6xG+TQqAjgMiQ\nQC11oZRS3XiUFEQkQ0TOtR87RSTSu2ENjegwrZSqlFLdDZoUROQO4F/An+1DacAb3gxqqEQ7g7T7\nSCmluvGkpXA3cDpQB2CM2QUkeTOooRIXHkxlQ6uvw1BKqWHDk6TQaoxxz9sUkUDAeC+koTMmKpTi\n2hZfh6GUUsOGJ0lhqYj8P8ApIucB/wTe8m5YQyMlxkl5Q6sWxVNKKZsnSeF+oBzYBHwDeAd4yJtB\nDZWx0aEYA6V12lpQSinwYI9mY4wL+Iv9MaqMiQ4FoLi2hbTYMB9Ho5RSvjdoUhCRfPoYQzDGZHsl\noiE0NsYJoOMKSillGzQpALndHocCXwHivBPO0HK3FGqafRyJUkoND4OOKRhjKrt9FBlj/ghcMgSx\neV1UaBARIYHaUlBKKZsn3Uezu30agNVy8KSFMSKkRIdSXKstBaWUAs/e3H/f7XEHsA+4xivR+MCY\n6FBKtKWglFKAZ7OPzjqaFxaRZ4BLgTJjzIl9fF2AR4GLgSbgFmPMuqO51rEYG+1ke0n9UF9WKaWG\npX6TgojcO9ATjTGPDPLazwGPA8/38/WLgAn2x6nAk/a/Q2pMdCgVDa20dbgIDvTborFKKQUMPNAc\nOcjHgIwxy4CqAU65HHjeWFYCMSKS4mngx8vYGF3AppRSXfptKRhjHvbytVOBA90+L7SPFfc+UUTu\nBO4EGDdu3HENIiXaWqtQVNNMepwuYFNK+TdPZh+FArcBJ2CtUwDAGHOrF+PqwRizEFgIkJube1yL\n8Z0wNorAAOHj7WXMyY4/ni+tlFIjjied6C8AY4ALgKVY+ykcj5HZIiC92+dp9rEhFR8RwlmTk3ht\nfREdWhhPKeXnPEkKOcaYHwGNxpi/YS1cOx4DwouAr4llDlBrjDms62gofOWkNMrrW1m2q9wXl1dK\nqWHDk3UKXVuT1YjIiUAJHmyyIyIvA2cCCSJSCPwECAIwxjyFVW31YmA31pTUrx9p8MfLWZOTiA8P\n5uXVBzh7crKvwlBKKZ/zJCksFJFY4EdYf91H2I8HZIy5fpCvG6xd3XwuyBHAV+dk8NiSXazYXcHc\nnARfh6SUUj7hSffRs8aYamPMUmNMtjEmyRjz58GfNrJ888zxZMSH8eAbm2lp7/R1OEop5ROeJIV8\nEVkoIufYq5BHpdAgBz+/Yhr5FY384p1tvg5HKaV8wpOkMBn4EKurZ5+IPC4i87wblm/Mm5DAHWdk\n8fznBbyxfsgnQimllM95Ujq7yRjzqjHmKmAmEIU1NXVU+uGFkzklK44f/GsD7232yWQopZTyGY+K\n/YjIAhH5E7AWawHbqKmS2luQI4C/fC2XaanR3P3Sej7VaapKKT8yaFIQkX3Ad4FPgWnGmGuMMf/2\ndmC+FO0M4oXbTiU91skv3tmOy3VcF1ErpdSw5UlLYbox5kpjzMvGmEavRzRMhIcE8p1zJ7CtuI73\ntpT4OhyllBoSnowp1A1FIMPRZTNSyUmK4I8f7sRaVqGUUqObbiAwAEeAcNeC8ewsbWB1/kBVwJVS\nanTQpDCIS6alEBkayCtrDgx+slJKjXCeDDR/R0Si7MJ1T4vIOhE5fyiCGw6cwQ6unJXKfzYVU9PU\n5utwlFLKqzxpKdxqjyucD8QCNwG/8mpUw8x1J4+jrcOlrQWl1KjnSVLoKm1xMfCCMWZLt2N+YerY\nKM6clMgTH++mqlFbC0qp0cuTpLBWRN7HSgqLRSQS8LvdaP7fxVNobO3gsSW7fB2KUkp5jSdJ4Tbg\nfuBkY0wT1p4IPtv7wFcmJkdy3Snj+PvKAkpqW3wdjlJKeYUnSeE0YIcxpkZEbgQeAmq9G9bwdNf8\n8XQaw99XFvg6FKWU8gpPksKTQJOIzADuA/YAz3s1qmFqXHwY50xO5qXV+3XPBaXUqORJUuiwd0m7\nHHjcGPMEEOndsIavW0/PpKqxjUUbDvo6FKWUOu48SQr1IvIA1lTU/4hIAPZey/7otPHxZCWE8+YX\nut+CUmr08SQpXAu0Yq1XKAHSgN96NaphTES4ZFoKn++ppKKh1dfhKKXUceVJQbwS4EUgWkQuBVqM\nMX45ptDl4mkpuAws1uqpSqlRxpMyF9cAq4GvYG2us0pErvZ2YMPZlJRIshPCeWeT7symlBpdAj04\n50GsNQplACKSiLVn87+8GdhwJiJcMj2FJz7eTUFlIxnx4b4OSSmljgtPxhQCuhKCrdLD541qN83J\nIDgwgEc/1BXOSqnRw5M39/dEZLGI3CIitwD/Ad71bljDX1JUKDfPzeT1L4rYWVrv63CUUuq48GSg\n+QfAn4Hp9sdCY8wPPXlxEblQRHaIyG4Rub+Pr48TkY9FZL2IbBSRi4/0G/Clu+aPJyI4kN+/v8PX\noSil1HHhUTeQMeY1Y8y99sfrIrJ8sOeIiAN4ArgImApcLyJTe532EPCqMWYWcB3wpyML37diw4O5\nY342i7eUsuFAja/DUUqpY3a0YwPjPDjnFGC3MWavMaYNeAVrVXR3BoiyH0cDI26Z8K3zsogLD+Z3\n2lpQSo0CR5sUPNnFPhXovitNoX2su58CN4pIIfAO8K2+XkhE7hSRPBHJKy8vP4pwvSciJJBvnjme\nT3dVsHx3ha/DUUqpY9JvUhCRq/r5+DLgPE7Xvx54zhiThr2Jj11GowdjzEJjTK4xJjcxMfE4Xfr4\nuXFOBqkxTn757jZcLk/ypVJKDU8DrVP40gBfe9uD1y4C0rt9nmYf6+424EIAY8znIhIKJABljCCh\nQQ6+f8FEvvePDby18SCXz+zdIFJKqZGh36RgjPk6WAPGxpijqRO9BpggIllYyeA64IZe5+wHzgGe\nE5EpQCgwvPqHPHT5jFQWLsvnjx/u4tLpY3EE+NWOpUqpUcKTMYV8EVkoIueIiMfvdMaYDuAeYDGw\nDWuW0RYR+ZmIXGafdh9wh4hsAF4GbrHLdI84AQHCd87JIb+ikbc3HqSopln3c1ZKjTgy2HuwiIQB\nl2L9pT8bq+voFWPMZ94P73C5ubkmLy/PF5celMtluOjRTymtb6G+pYOpKVEsuud0jiCXKqWUV4jI\nWmNM7mDnebJ4rckY86ox5ipgFtYU0qXHIcZRJyBAuO/8iTS2dnByZiybimpZsm1EDY8opfycR1NS\nRWSBiPwJWIvV73+NV6Mawc4/YQzbfnYhL9x2KuPiwvjjkp20dujWnUqpkcGT0tn7gO8CnwLTjDHX\nGGP+7e3ARrJARwBBjgC+c84ENhfVMf2n7/PYEi2cp5Qa/jwpnT3dGFPn9UhGoatmpxIXEcxfP93L\n4x/v5ua5mUQ7/XYnU6XUCOBJ99EYEVkiIpsBRGS6iDzk5bhGBRHhrElJ/PCCybR1uHRTHqXUsOdJ\nUvgL8ADQDmCM2Yg1E0l5aHpaNDlJEfx7baGvQ1FKqQF5khTCjDGrex3r8EYwo5WIcNXsVPIKqnni\n493sr2zydUhKKdUnT5JChYiMxy6CZ+/PrP0gR+ja3HRmpsfw28U7OOv3n/D9f26grqXd12EppVQP\nngw03w0sBCaLSBGQD9zo1ahGofiIEN64+3SKapp55rN8nluxjyBHAL+8apqvQ1NKKbdBk4IxZi9w\nroiEY+3XrHtPHoPUGCc/unQqxsCzK/K54ZRxTEuL9nVYSikFDJAUROTefo4DYIx5xEsx+YXvnjeB\nRRuKeOD1jfzjztMID/Gk0aaUUt410JhCpP2RC/wX1gY5qcBdWDWQ1DGICg3il1dNZ1txPbc+t4am\nNh27V0r5Xr9JwRjzsDHmYax9EGYbY+4zxtwHnIRn23GqQZw3NZlHrpnBmn1V/PBfGxmhBWKVUqOI\nJ7OPkoHuNaDb7GPqOLh8Zir3nT+JtzcW8+zyfb4ORynl5zzpyH4eWC0ir9ufXwE857WI/NB/LRjP\n+v01/OKdbUxPiyY3M87XISml/JQnpbN/DnwdqLY/vm6M+aW3A/MnAQHC76+ZQWqsk2++uI7y+lZf\nh6SU8lMelc42xqwzxjxqf6z3dlD+KNoZxFM3nkR1Uxt/XrrH1+EopfyUR0lBDY0pKVEsmJjE2xuL\ncbkGHnSua2nn2eX5tHe6hig6pZQ/0KQwzFw2cywldS2s3ldFVWMbnf0kh1dW7+fht7by8ur9Qxyh\nUmo006QwzJw7JQlnkIP/eXsrc36xhO/944s+p6p+aG/z+diS3brGQSl13GhSGGbCggM5b2oyWw7W\nkRwdwqINB3llzYEe51Q3tpG3r4ozJiRQ0dDK31YU+ChapdRoo0lhGLr/osn85urpfHTfmczLSeDh\nt7ZwsKbZ/fVPdpbhMnDf+ZOYNS6GD7eV+jBapdRooklhGBob4+Sa3HSCHAH86svTcLng0Q930drR\nyUfbS3l51QESI0OYnhrN5DFR7C1v8HXISqlRQquwDXNpsWHcdFoGzy7PJ6+gij3ljQB8Y342AQHC\n+MRwqpvaqWxoJT4ixMfRKqVGOk0KI8DdZ+Xwat4BGlo7eOrG2ZycGUdceDAAOUkRAOwpb9SkoJQ6\nZl5NCiJyIfAo4AD+aoz5VR/nXAP8FGtntw3GmBu8GdNIFBcezAffW0BEaCARvUpsj0/sSgoNnJKl\n5TGUUsfGa0lBRBzAE8B5QCGwRkQWGWO2djtnAvAAcLoxplpEkrwVz0g3Jjq0z+OpMU5CAgPYU6bj\nCkqpY+fNgeZTgN3GmL3GmDbgFeDyXufcATxhjKkGMMaUeTGeUSkgQMhOjGCPDjYrpY4DbyaFVKD7\nBPtC+1h3E4GJIrJcRFba3U2HEZE7RSRPRPLKy8u9FO7IlZMU4R6AVkqpY+HrKamBwATgTOB64C8i\nEtP7JGPMQmNMrjEmNzExcYhDHP7GJ4ZzoLqJlvZOX4eilBrhvJkUioD0bp+n2ce6KwQWGWPajTH5\nwE6sJKGOwPjECIyBix/7lEc/3OX1620vqWN7SZ3Xr6OUGnreTAprgAkikiUiwcB1wKJe57yB1UpA\nRBKwupP2ejGmUWn+xERumpNBQngIf/hwJ29+0Tv3WrrXUNpYWHNUNZPaOlzc8swaHnx981HHq5Qa\nvryWFIwxHcA9wGJgG/CqMWaLiPxMRC6zT1sMVIrIVuBj4AfGmEpvxTRaRTuD+J8rTuSlO04lNyOW\n//faJgoqe44xVDW2MePh93lvcwmVDa1c+acV/PXT/CO+1uvrCympa6Gounnwk5VSI45XxxSMMe8Y\nYyYaY8bbO7hhjPmxMWaR/dgYY+41xkw1xkwzxrzizXhGu0BHAI9dPwsR4UdvbqHTZVi+u4KOThfL\ndpZT19LBh9tKySuoptNlWLe/+ohev9NlePITawOgsvoWOnQvB6VGHV8PNKvjbGyMk3vPm8iyneVc\n9OgyvvrXVTyzPJ+lO61ZW6vzq8jbVwXAhgM1fZbl7s+qvZXsq2xiXk4CLgNlum2oUqOOJoVR6Gun\nZXDC2Cj2VzWRFuvkpVX7+XRXOcGOAPZXNfHu5hIAqpvaOVDleTfQ3gqrS+qS6SkAFNe2HP/glVI+\npUlhFAp0BPDSHXNY9oOz+P75k9hX2URFQxs3zskAoLC6mTMmJADwRWGNx69bWN1MkEOYkWbNGi7R\npKDUqKNJYZSKdgaRFBXKhSeOITYsCIBvLMh210766qkZhAQGsOFA/0nhqaV7eHvjQffnB6qbSI1x\nkhrjBKC4dmgHmxtbO7jp6VW6elspL9IqqaNcaJCDb509gc0Ha0mOCiU3M5ZPdpRzalYcJ6ZG95sU\nWjs6eeSDnQQ7Ajg1K57EyBAKq5pIjwsjyhmIM8gx5N1He8ob+HRXBZ/vqXQXAlRKHV/aUvADt87L\n4pFrZlqPT8/im2eOJzY8mFnpMXxxoIZfvrONqsY2wPrrv7mtk81FtbR1uGho7eB3i3cAVvdRWqwT\nESElOnTIu4+qm9oBqGxoG9LrKuVPtKXgZ+ZPTGT+RKtUyDfPyqGqsY2Fn+7l7ysLmJEew4o9lVyb\nm05WYjgAV81K5dW1B7jl9EwqG9tIiw0DrKqtx9J9tLagitSYsH6rv/al2k5clY0660kpb9GWgh+L\nCw/mkWtn8sH35nPm5CQKKpuYkhLFWxsPsmxnOVkJ4Xz7nAkYAy+v3g9AWqw1npAS7TzqloIxhq8/\nu4abnl51RPWaulozFQ0jPykU1zbr2IgaljQpKHKSInnihtksv/9s/veKE2hq62TFnkpyM2LJiA8j\nNcbJG+ut0hnpcVZLISU6lNL6Vjpdnq9z6FLZ2EZdSwe7yhr41bvbPX5edVNXUhj53Uf/+/Y2vvPK\nel+HodRhNCmoHmaPiyUrweo6ys2MRUSYOz6euharTlJ6t+6jTpeh3F7A5jqC5FBQ2QTAlJQonlux\nj40eTovtSgqVo6ClUFLXQkX9yE9uavTRpKB6EBG+kpsGwClZ8QCcnmOtaQgNCiAhwtobOsUeCyiu\nbeZAVRPTH36/30J8ve2vshbB/eLKE4l2Bnlc2bW60R5obhz5b6bVjW3UtbT7OgylDqNJQR3mjjOy\nef2bc90thrnjreSQFhuGiAAwISkSgFX5VSzacJCG1g5+9MZmSut6jjMU1zYf1oooqGxCxGop3HFG\nFku2l3nUWugaU6hpaqd9hNddqmpqo6mtc8R/H2r00aSgDhPkCGDWuFj350lRoUxNiWJC0qG1AePi\nw8jNiOXVvAO8teEg2YnhtHa4+MmbW9znfLyjjNN/9RFPLt3T4/X3VzaREhVKaJCDm+dmEu0M8qhi\na1f3ERxKECNRR6eL2marlVDfcuTly5XyJk0KyiPP3Xoyv7hyWo9j1+Sms7e8ke0l9dw0J4Nb5mby\nwbZS6lra2V1Wz7dfWo/LwFsbDvZ4XkFVE+PirbGJyNAgLjghmY93lA36V3N1UxvRTmt19kiegVTb\n3E5XHcK6Zu1CUsOLJgXlkaTIUGLDg3scu2R6CmHBDkTgkmkpnDU5iU6X4fM9lfz+/Z04HMLt87LY\nXlLPvopD+zsUVDaRERfu/vzsycnUt3SQt6//Ut7GGKob292tlZE8A6l7i0fHFdRwo0lBHbXwkEDu\nnJ/NtbnpJEWFMntcLGHBDv6zsZgl28q4clYqN8/NBGDxFqsya2NrBxUNre6WAsC8CQkEOwJYsq20\n32s1tXXS1uliQrKVFHrPQPp4e5l7cdtwV9V4KBHUNWv3kRpeNCmoY/Ldcyfyqy9PByA4MIA52fEs\n2nCQtk4XV8xMJT0ujBNTo3jPTgpd01EzuiWFiJBATs2O46PtZf1ep2sMIcce4O5e6qKktoWvP7eG\nv32+73h+a17TfTxEWwpquNGkoI6rrpLcWQnhTE+LBuDS6WNZv7+GlXsr3dNRu3cfAZw7JZm9FY3k\nd+tm+nBrKSv2VACHulzGxYURHBhARbdSF3kF1qZBu8sGXiHc3uk6ok2FvKVH95GOKahhRpOCOq66\n6ipdMTPVPX315tMySYt18tAbm3lq6V6CHQFkJoT1eN4C+3nLdpbT3unigdc2cvvzeXznlS/odBn3\nX9dx4UEkhAf3WPjVNRaxt7znvtTdtXW4uOSxT/nZ21uP3zd7lLSloIYzLYinjqvxiRG8cuccZqbH\nuI85gx08fNkJ3Pa3PEICA3js+plEhgb1eF5mQjjj4sL4dFc5ziAHL68+wLycBD7bXcHKvZXU2BVS\nY8OCSYgM6VEUr6ulkF/RiDEGEaGhtYM3vyhic1EtZ05KorC6mZ2lDQQ5fP93UHVjG84gB60dnTqm\noIYdTQrquJuTHX/YsXOmJPObq6czNSWKE1Oj+3zeGRMSeH19EWX1reQkRfDXm3PJ/d8PefOLIiaP\niQKspBAfHsy+yiaW7SxnWmo0Ww/WkRgZQnl9KyV1LaREO3lueT6/e38nIYEBvLz6AKFBVjLYVdZA\nR6eLQB8mh6qmNuLCg2ls69CWghp2fP9nk/Ib1+Sm95sQAM6YkEhTWycbC2u57uR0QoMcXHDCGN7d\nXEJpXQsBAlHOIDLiw8mvaORrz6zmsic+w2XgqtmpwKEupFX5VUxKjmTDT87nshljcRm444ws2jpc\n7Kvsv5sJrLGHPy/dw7biun7PaW7r5PLHP+PVvANHfB+qG62kEBUaNKRjCoXVTazb3/+039HIGHNE\nlXiVJgU1jMzNiccRIAQ5hCtnWW/yV8waS31LBy+u2k9MWDCOAOHBS6bwzrfP4OdXnsjBmhZE4OrZ\nVr2mvRWNdLoM6/fXkJsZS2iQg0evm0neQ+dyhf2a24rrB4zjvc0l/PLd7Vz6f5/xp092A/DupmLu\n/ccX7pIdTy3dw4bCWl5ctf+Iv8+qpnZiw4OJcga6Cw1219bh4kv/9xnv2zO2jpffLd7BdX9e2WMw\nfyi5XIbWjqF9g37zi4Oc+oslNLVpN52nNCmoYSMqNIgLTxjDNbnpxEeEADAvJ4F7z5tIQ2uHe6/p\nIEcAU8dG8dVTM3jh1lP4+RXTyEmKwBnkYG95A9tL6mho7SA30yrVISJEhQaRkxSBI0DYUVLPz/+z\n1f2G39sra/aTGuPkjAkJ/PHDXbS0d/LS6v28tr6IdzeXcLCmmT8v20NESCAbDtQc8b4S1Y1txIUF\n9dtS2FFSz6aiWj7eUX5ErzuY3eUNtHW6+NEbm30yC+vFVQWc8euPj6rc+tHacrCW2uZ2nyXCkUjH\nFNSw8sRXZ/f4XET49jkTmJkeQ1vH4WUw5uYkMDfHepyVEM7e8kbWFlhdJLkZcT3ODQl0kJ0QzuIt\nJewqayA0KIAbThlHTNihldoFlY0s313JvedN5ISxUXyyo5y1BdXu1/zd+zsIdgRgDDx+wyxueXYN\nH2wt4abTMt2vUdPURkigA2ewo8/vsbqxjdjwYFraXeytOHwa7Rd2ccCdpQO3aI6EMYb88kaSIkP4\nbHcFH20v45wpycft9T2xsbCWsvpWKhpaSY4aeMe9D7aWEh7sYK5dofdoHbQTdkFlEyeM7b/rUh3i\n1ZaCiFwoIjtEZLeI3D/AeV8qOZT9AAAZwElEQVQWESMiud6MR41c8ycmcu7Ugd/EshPD2V3WwKr8\nKpKjQty7xHU3OSWKXWUNBDmElnYX/1hzgH0VjRyoshbV/WPNAQIEvpKbRm5mHCLw9Gf5NLV1ctmM\nseRXNFJY3cQzt5zMgomJZCeE88LKAq54YjkvrdqPMYYvP7mCB1/f1GeMbR0u6ls7iAuzu4/6mH20\n8YCdFErqMcbwzqZid3xPLd3Dit0V/d6Djn7qR5U3tNLY1smd87MJEPjigGd7WBxPRTXW9q3FHrSs\nfvXuNh5d4llJ9YEU29ccbBxJHeK1loKIOIAngPOAQmCNiCwyxmztdV4k8B1glbdiUf5h9rhY3t5Y\nTFFNM5dMT3Gvk+hu8phI3tpgDXrvKW/giY9387v3d5AUGco73zmDl1fv5+zJyaREWwllakqUe6X1\nQ5dMYda4GE7NimfqWGs21AUnjuHJT6wqsG0dLuZkx7GnvJHy+lY6Ol3sKW8kJDCATLsMeY29cC02\nPJja5vY+Zx9tsFsK9a0dbDlYxzdfXMeVs1L50aVT+fV72zk5I67Pv6DXFlRz/V9WcuvpWdx3/sQe\n02/z7QH4icmRjI1xuleWHy/GGIyBgIDD73mXwmrrDbqkthm6TVnu67UO1rTQ3nns3UxdXXsFFcf3\n+x3NvNlSOAXYbYzZa4xpA14BLu/jvP8Bfg0c3Ya/StlunpvJUzfO5rIZY7lpTkaf58zLSWBsdCh3\nzs/mzvnZ1LV0MCs9lqKaZr7+7Gqqm9q5c362+/xT7Y2GMuPDSIoK5eunZ7kTAsDdZ+Xw9M25/OCC\nSWwtruOfawsBqGvpYPW+Km58ehU3P7ua9k4XnS7Du5utweO48GCinEGH7anQ0GptU9q1h8Uzn1kl\nxZftLGfZznKMsdZl9FUl9sWVBXS6DE8t3cMDr/VsqXT1qWclhJMRH8b+qsPfJI9lnOHHb27ha8+s\n7vfrLpehuNazlkJtczvN7Z2U1LYc0Y5+vXW6DKX2zoAFVdpS8JQ3k0Iq0H2+XqF9zE1EZgPpxpj/\nDPRCInKniOSJSF55+fEdfFOjhyNAuPDEFB67flafayUAZqTHsOKBc8iID+fsycmsfvAc/vGNOZyU\nEcu6/TXMTI/h5MxDe0nMybbGJU7Jiuvz9SJCAjlnSjLnTEkCrK6m5KgQHAHCj9/cQnl9KwWVTTzz\nWT7XLfycnyzawuQxkcwdH09UqNVQ776nwuaiWoyBq0+yZlO9tdEqO17Z2MZTS/cQ5BBcBj7a1rNO\nVENrB+9uLuGa3HQumZ7i7mJ65P0dfPPFteRXNBLsCGBsjJNxceGHJQVjDOf9YRn/d5RdNqvzq/h8\nbyXNbX3PLiqrb3X/5T/YwPzBGuvrbZ2uY9plr9zeQ9wRIMe9ZTSa+Wz2kYgEAI8A9w12rjFmoTEm\n1xiTm5iY6P3glN9IigxFRLjv/ImIwH+dOb5Ht9Op2fGkxji56MSUAV9nUnIkKdGhtHW4OG9qMrPH\nxbC7rIHsxHBOzozll+9uZ/3+Gn5z9XTe+fYZxIRZLQXoWf+oawe6BRMTSYgIob3TMC8nARHYXlLP\n+SeMITXGaQ2Wl9a7N+t5d1Mxze2dXH1SKlPGRHKwtoWmtg7e31rKO5tKWLK9jIz4MBwBQkZ8GFWN\nbdQ2tfONF/JYsaeC/VVN7C5r4NEluwZcnwFW4rr7xXXuch0dndaAeafLsKmots/nFNUcelM+OEhS\n6GpR9H58pA7az52WGk1xbYuuV/CQN5NCEZDe7fM0+1iXSOBE4BMR2QfMARbpYLPyhbnjE1jz4Llc\ncMKYHsejnUEsv/9szpqcNODzRYQzJ1nnnDEh0f34lrmZ/L+Lp5AW6+SJr87mmtx0d797lF3qI6+g\nmuX2X/af7ChnfGI48REhTBpjlQm/bOZYZqRZffALJiRy3tRklmwv47w/LOOH/9oAwL/WFpKVEM7s\ncbFkJ1rP21na4F7Mt7uswb29akacVXfqnc3FLN5SymvrithcZCWCgADh/tc29dmV1NbhIm9fFTc9\nvYr/bCp2lzrfX9XkbgWs72dxXNd4Qnx4sDWm0M3GwhpuenqVey1B96TR1Wo4GsX2c0+zu+L66jJT\nh/PmlNQ1wAQRycJKBtcBN3R90RhTC7hHy0TkE+D7xpg8L8akVL8S7LURR+u6k9MpqGxkXk4Cs8bF\nUNXYxtUnpREWHMhn/332Yed3tRS+/88NBDsCeOPu01m5t5J7zrLm2E5MjmT57krOmJBAWV0Lm4pq\nmT8xkZOz4qhraaeioY2PtpexuaiWVflVfP/8iYgI4+2k8NG2Uto6XYQHO2hs6yQr0UoKXXtZ/GON\n1bu7tqCapMgQAgOEH14wif/9zzb2lDe4y5Q/tmQX/1pbSGF1Ey4DyVEhRIQEsm5/DV/JTXdXp3UE\nSL+zmrqSwkkZsWzt1RJ5+rN8Pt1VwZp91SyYmOieMQRwsGbwlkJ1YxsxYUGHTSzoamWclh3Pk5/s\nYV9FIxOTIwd9vePlxVUFpMeGuYtEjhReaykYYzqAe4DFwDbgVWPMFhH5mYhc5q3rKuUrM9JjeOmO\nOYSHBJIUGcqPLp1KWHD/f3dFOa2vJUeF0O5ycdff1+IycOmMsQDcNi+LR6+bSUq0k9vPyObtb81j\nTHQoWQnhPHLNTH5w/iTaOw3ffmU9InClvao7Iz4MEdyD2t+0k8z4BCtZjLNbCl1v4PkVjSzbVc7E\n5Eh3C2ddgfW14tpm/vDhThIjQ7jn7An84doZvPWteczOiHW3CnaXW0lhwcRE1u/vOykU1TQTGxZE\ndmIEpXWHBpCb2jp4f4vV4sjbV2Vfs4Wx0aGEBAYM2n1UWtfCnF8uYeGyvYd9rbi2BWeQw13CfSjH\nFRpaO3h40dY+4xruvDqmYIx5xxgz0Rgz3hjzc/vYj40xi/o490xtJSh/kpMYwV0LxvPKnadx/tRk\n9lc1MSEpwv3XbFpsGJfPtOZmhAY5mJIS1eP5J6ZGuRfsnWaPfXSdmx4bxq6yBgIEbj09i/+7fhZf\nspNNZGgQcfbWqpPsa20uquOEsVFkJ4QT7Qxy10h6fX0RxsAj18zg3vMmcuWsNJIiQ5mVHsOO0nrq\nW9rZXdZAclQI8yckUFLXQnFtM20dLu7/90Z2lFgL8Iqqm0mLDWNsTCjtncY9gPzB1lKa2zuJCAlk\njZ0UDtY0MzbGSUp06KDjDx9uK6W1w8XjH+3uUZIcrAHtlJhQYsKCiQkLIt/DtQp9LZLsy6INB7ns\n8c/6XBuydEc5bZ2u47oAcahomQulfCTQEcD9F00mKyHcPQ320uljPX6+iLjf6L9stxK6ZNtdRZnx\n4TiDHXxpxtgeK6y7Wgu3n5FFkMPqdjkxNZqAAGH2uBjWFlRjjOG1dUXkZsSSEd9zU6TZGbEYAxsO\n1LKnrIGcpAhmjbNmbX2+p5Il20p5Zc0BnlpqreEoqmkmNcbJGHslc0ltC50uw7/WFpISHcrVJ6Xx\nxYEa2jtdFNe2kBLjJCXaSXFNM6+tK+Tyxz9jwW8/PuxN9sOtpcTbFWf/8MHOHlNYD9Y2M9Zeb5Kd\nEM6eQTZhAtheUseJP1084NawXT7YWsrGwto+B9c/2Gq10srqW0fMNrFdNCkoNQyclBHHy3fM6bFG\nwhM3n5bBPWflcMn0nrOjusYV+utD79oOdcGkRHf5hxNTrZbI7HGx7Cpr4JOd5ewua+DLJ6Ud9vyu\n/TLWFlSzu6yBnMQIpqVGk5UQzt9W7HOv13hvcwn1Le0UVTeTGut0LwrMK6ji+oUr+XRXBV89dRyn\nZMXR0u5iU1EtJXb3UUpMKPsqm/jRG5tpaO2gqLqZf68rdMfQ2NrB8j2VXD4zletPGccLKwu44I/L\n2Gy/SRfXtDAm2kpCOUkR7BlgE6Yu/8wrpK3Dxf/+Zxst7Z2s2FPR7yrxrQet66zYU9njeHuniyXb\ny9wtt2NpLfhit0BNCkoNE6eNj++3XlJ/4iNC+P4FkwgN6vm8rpbCxDF9J4Uvz07jG/OzSYoM5dSs\nOIIc4t6zYnaG9Rf/3S+uIyEi+LCEA9asrAlJETy7Ip/Gtk5ykiMJCBBum5fFhsJaPtpexpzsOJrb\nO7n5mdU0t3eSmxHrfpN++K2tbCuu43dfmcHdZ+WQa19z8ZYS2jpdpESHMjbaSVVjG41tnfzh2pmc\nNj6e97eUut8kP91VQVuHi3OnJvHwZSfwh2tnUNPczg/+tZEVuysoqWtxd4+NT4ygoqGV2qZ21hZU\ns9ceB3ljfRHv2WMvnS7Dog0HSY1xkl/RyJm//YQb/rKK19YX9f72aWrrYK+9IHB5r7IjK/dWUt/S\nwV0LrATfOyk89MYm7nw+b8CFeZuLarnnpXWc8OPFPGm3toaKJgWlRqGuFsKUfpLC/ImJPHDxFMAa\niP7nXXMJD7EGvmekxxAg0NFp+PNNue6ps709cPFkZo+LJSsh3L0C+8uz09zjFT+/chrpcU7W7a/h\nshljufDEMcSHBxMbZlWsffOe07n6pDREhKSoUCYmR/Ds8n0AVvdRjJVAZo2LYXpaDBecMIb8ikb3\nbKe3Nx4kKjSQkzPjCHQEcOWsNH76pRPYVlzHbX/LIz3OyY32yvacJKvltKusntv/tob/+vs6yupb\n+OG/N3LPS+vYcKCGFXsqKK9v5cFLprBgYiLN7Z04gxx9zqjaXlKPMdYK8byC6h5rIF5evZ9oZxBX\nn5ROZGggO7olhcbWDv6ZV8j7W0v562eHBqG7Ep0xhl++u43LHv+MT3dVkJkQxqMf7nLXvhoKWiVV\nqVEoNyOWhTed5FEl1GhnUI/tUyNCAvneuROZNCaSkzJi+33e2ZOTOXtyz9d3Bju4/6LJbD1Yx/jE\nCL55Zg6vryvi51eeiIggAu99dz7RzqDDWjcLb8rlGy+sZUdpPRnxYYTbM7dumZsJwHlTk3nojc0s\n3lJCaJCDdzYVc/sZ2T1qPF08bQxzsuNYubeKn18xzd3y6upOe3tjMdVN7VQ3tXPXC2tp73SREBHC\nXX9fizPYQWRIIGdPTuKcKUm4XHD782vYVNjVTVTBzPQYwoID2XLQmlZ727wsHnpjM3n7qpk3IYGi\nmmYWbynl9jOycAY7mJQcyc6SQ2MZS3eW09rhIicpgt8u3sFZk5IICXRwyWOf8tuvzCA0KIA/L93L\n1Sel8aNLp9LU1sE5v1/Kw29t5a83D80SLk0KSo1CIsL5vRbiHYlvnTPhqJ97Te6hNavXnzKO608Z\n1+Pr/ZXNzkwI5/W757KxsJbJY6JwJRlevP1UdyskOSqUWeNi+PvK/WworMURINx6elaP1xARHr9h\nNpsKa3usD0iLdRLsCHCPSSRFhrBufw3nT03mzvnZfPvl9USGBnHPWTk9ktW01Bie/mwvm4tqueEv\nq7hyVip/uHYmWw/WERMWxBWzUnn4rS3c8uxqTkiNJtoZhDGGr9ml1CeOieQ/G4vde4e/t7mEuPBg\nXrrjVBb85hOeWrqXuPAg6ls7+PV72xkTFUpyVAi/uHIawYEBRDuD+NbZE/j1e9v5aHvpYUnYG7T7\nSCk1bIQFB7rrVgUECKfnJPRYlPbwZSfgMoYPtpZyxcxU9xhFdwkRIYetQA90BJCVEE59SwfZCeH8\n4IJJiMA3FmSTmxnHigfO4c27T+eqXrO4pqdF095p+O3iHYA1RffjHWVsPVjL1JQoIkICef7WU7lj\nfjZtHS6W7SznwhPHuAeZJyZFUNvcTlFNM60dnXy0vYzzpiSTFBnKNblpLNpQxCtrDpAeZ41jfL63\nkpvnZhIceOit+bZ5WWQnhvPTRVuHpFSHthSUUiPG9LQY3v72PJ7+NJ+v92olDGZ8Ujg7Sus5bXw8\nV5+Uxuk5CYyNOXzPje6m2XuKL91ZzsmZsdQ0tXPbc2twGbh9nnX908bHc9r4eH54wSR2lNb3eM35\nExMJcgi/eGcbU8ZE0dDawYXTrBbcrfOyeH5lAfUtHfz1a7n84t3t7Cyp54ZeLavgwAB+dtmJ3Pj0\nKhYu28u3j6EV5wlNCkqpESUpMtQ9SH4kcuxxhbnjrdbHYAkBrG6n2LAgqpvauWJWKvMnJPJq3gHq\nWzq44dSeb94ih2ZwdclOjOC7507kt4t38M6mEq6YOZYFE6xurYz4cK6YmUp+RSOnZMXx5FdnU17f\n2mMnwC7zJiRw91nj3V1p3qRJQSnlF+bmJPDmhoOcnuP5G6uIMC0thuW7K7jwhDHER4Rw3/mTjui6\n35ifzcq9lYQGOfjN1TN6bET0+6/MwGWPN4yNcQ6YqH5wweQjuu7REl9s4H0scnNzTV6eVsNQSg2N\nFXsq2F3W4B48PhpdA82+JCJrjTGDTmHSloJSSg1g7vgE5o4/fPvTI+HrhHAkdPaRUkopN00KSiml\n3DQpKKWUctOkoJRSyk2TglJKKTdNCkoppdw0KSillHLTpKCUUsptxK1oFpFyoOAon54AVAx6lm8M\n19g0riMzXOOC4RubxnVkjjauDGNM4mAnjbikcCxEJM+TZd6+MFxj07iOzHCNC4ZvbBrXkfF2XNp9\npJRSyk2TglJKKTd/SwoLfR3AAIZrbBrXkRmuccHwjU3jOjJejcuvxhSUUkoNzN9aCkoppQbgN0lB\nRC4UkR0isltE7vdhHOki8rGIbBWRLSLyHfv4T0WkSES+sD8u9kFs+0Rkk339PPtYnIh8ICK77H9j\nfRDXpG735QsRqROR7/rinonIMyJSJiKbux3r8x6J5TH7d26jiMwe4rh+KyLb7Wu/LiIx9vFMEWnu\ndt+eGuK4+v25icgD9v3aISIXeCuuAWL7R7e49onIF/bxobxn/b1HDM3vmTFm1H8ADmAPkA0EAxuA\nqT6KJQWYbT+OBHYCU4GfAt/38X3aByT0OvYb4H778f3Ar4fBz7IEyPDFPQPmA7OBzYPdI+Bi4F1A\ngDnAqiGO63wg0H78625xZXY/zwf3q8+fm/3/YAMQAmTZ/2cdQxlbr6//HvixD+5Zf+8RQ/J75i8t\nhVOA3caYvcaYNuAV4HJfBGKMKTbGrLMf1wPbgFRfxOKhy4G/2Y//Blzhw1gAzgH2GGOOdgHjMTHG\nLAOqeh3u7x5dDjxvLCuBGBFJGaq4jDHvG2M67E9XAmneuPaRxjWAy4FXjDGtxph8YDfW/90hj02s\nrdKuAV721vX7M8B7xJD8nvlLUkgFDnT7vJBh8EYsIpnALGCVfegeu/n3jC+6aQADvC8ia0XkTvtY\nsjGm2H5cAiT7IK7urqPnf1Rf3zPo/x4Np9+7W7H+muySJSLrRWSpiJzhg3j6+rkNp/t1BlBqjNnV\n7diQ37Ne7xFD8nvmL0lh2BGRCODfwHeNMXXAk8B4YCZQjNV0HWrzjDGzgYuAu0VkfvcvGqut6rPp\naiISDFwG/NM+NBzuWQ++vkd9EZEHgQ7gRftQMTDOGDMLuBd4SUSihjCkYfdz68P19PzjY8jvWR/v\nEW7e/D3zl6RQBKR3+zzNPuYTIhKE9cN+0RjzGoAxptQY02mMcQF/wYvN5v4YY4rsf8uA1+0YSrua\nova/ZUMdVzcXAeuMMaUwPO6Zrb975PPfOxG5BbgU+Kr9RoLdPVNpP16L1Xc/cahiGuDn5vP7BSAi\ngcBVwD+6jg31PevrPYIh+j3zl6SwBpggIln2X5vXAYt8EYjdV/k0sM0Y80i34937AK8ENvd+rpfj\nCheRyK7HWIOUm7Hu0832aTcDbw5lXL30+OvN1/esm/7u0SLga/bskDlAbbfmv9eJyIXAD4HLjDFN\n3Y4niojDfpwNTAD2DmFc/f3cFgHXiUiIiGTZca0eqri6ORfYbowp7DowlPesv/cIhur3bChG04fD\nB9YI/U6sDP+gD+OYh9Xs2wh8YX9cDLwAbLKPLwJShjiubKyZHxuALV33CIgHlgC7gA+BOB/dt3Cg\nEojudmzI7xlWUioG2rH6bm/r7x5hzQZ5wv6d2wTkDnFcu7H6mrt+z56yz/2y/TP+AlgHfGmI4+r3\n5wY8aN+vHcBFQ/2ztI8/B9zV69yhvGf9vUcMye+ZrmhWSinl5i/dR0oppTygSUEppZSbJgWllFJu\nmhSUUkq5aVJQSinlpklB+T27AqbHaxxE5BYRGevBOY8fe3RKDS1NCkoduVuAAZOCUiOVJgWlLIEi\n8qKIbBORf4lImIj8WETWiMhmEVlorxi9GsgFXrTr6jtF5GQRWSEiG0RkddfKcGCsiLxn17//TdeF\nROR8EflcRNaJyD/tGjeIyK/EqqG/UUR+54N7oJQuXlPKrkSZj1UQcLmIPANsBZ4xxlTZ57wAvGqM\neUtEPsHaDyDPLpuyHbjWGLPGLpLWBNwI/BirwmUr1grdeUAz8BrWat1GEflvrP0DngBWAJONMUZE\nYowxNUN0C5RyC/R1AEoNEweMMcvtx38Hvg3ki8gPgTAgDqvMwVu9njcJKDbGrAEwdjVLq3wNS4wx\ntfbnW7E2BorB2jBluX1OMPA5UAu0AE+LyNvA2975NpUamCYFpSy9m8wG+BNWHZkDIvJTIPQIX7O1\n2+NOrP9vAnxgjLm+98kicgrWJkJXA/cAZx/h9ZQ6ZjqmoJRlnIicZj++AfjMflxh9/lf3e3ceqxt\nEsHqFkoRkZMBRCTSLr3cn5XA6SKSY58fLiIT7WtEG2PeAb4HzDgu35VSR0hbCkpZdmBtLNQ1nvAk\nEItV1rkEq/x6l+eAp0SkGTgNuBb4PxFxYo0ZnNvfRYwx5fYeBy+LSIh9+CGsRPOmiIRitSbuPX7f\nmlKe04FmpZRSbtp9pJRSyk2TglJKKTdNCkoppdw0KSillHLTpKCUUspNk4JSSik3TQpKKaXcNCko\npZRy+/+PFkDduKqBcwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3D8wqL9Rrp1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "gj8KZRWIrp1O",
        "colab_type": "code",
        "outputId": "19a76404-ffa4-494e-a8cc-e966759dd30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12410
        }
      },
      "cell_type": "code",
      "source": [
        "# Use data point\n",
        "for seqLen in [minSeqSize,int((minSeqSize+maxSeqSize)/2)]+list(range(maxSeqSize,maxSeqSize+5)):\n",
        "    X,Y=makeData(seqLen,seqLen,5,1)\n",
        "    print('Seq:',seqLen)\n",
        "    print('\\tloss:',infer(encoder,decoder,attention,X[seqLen],Y[seqLen],1))\n",
        "    print('\\n\\n==============\\n\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq: 3\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.       0.747449]\n",
            "\t- [0.         0.85821843]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.       0.747449]\n",
            "\t- [0.         0.85821843]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.36103067]\n",
            "\t- [0.         0.50673795]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.36103067]\n",
            "\t- [0.         0.50673795]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.23713359]\n",
            "\t- [0.         0.45917594]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.23713359]\n",
            "\t- [0.         0.45917594]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.        0.2292821]\n",
            "\t- [0.        0.5740424]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.        0.2292821]\n",
            "\t- [0.        0.5740424]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.       0.826495]\n",
            "\t- [0.         0.87104225]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.       0.826495]\n",
            "\t- [0.         0.87104225]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.04802196224530538\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 4\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.48130018]\n",
            "\t- [0.         0.75372434]\n",
            "\t- [0.        0.8875887]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.48130018]\n",
            "\t- [0.         0.75372434]\n",
            "\t- [0.        0.8875887]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.        0.5017324]\n",
            "\t- [0.        0.9118939]\n",
            "\t- [0.        0.9118939]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.        0.5017324]\n",
            "\t- [0.        0.8809846]\n",
            "\t- [0.        0.9118939]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.37185782]\n",
            "\t- [0.         0.75146276]\n",
            "\t- [0.         0.82095236]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.37185782]\n",
            "\t- [0.         0.75146276]\n",
            "\t- [0.         0.82095236]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.         0.01962745]\n",
            "\t- [0.         0.17303123]\n",
            "\t- [0.         0.63064134]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.         0.01962745]\n",
            "\t- [0.         0.17303123]\n",
            "\t- [0.         0.63064134]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.         0.22408827]\n",
            "\t- [0.         0.63269204]\n",
            "\t- [0.         0.94905496]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.         0.22408827]\n",
            "\t- [0.         0.63269204]\n",
            "\t- [0.         0.94905496]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.18417486548423767\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 6\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.        0.2042041]\n",
            "\t- [0.         0.24697216]\n",
            "\t- [0.         0.32772171]\n",
            "\t- [0.         0.35922498]\n",
            "\t- [0.      0.44504]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.        0.2042041]\n",
            "\t- [0.         0.24697216]\n",
            "\t- [0.         0.32772171]\n",
            "\t- [0.         0.35922498]\n",
            "\t- [0.      0.44504]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.        0.4870649]\n",
            "\t- [0.         0.49728924]\n",
            "\t- [0.         0.59287876]\n",
            "\t- [0.         0.74603105]\n",
            "\t- [0.         0.85358036]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.        0.4870649]\n",
            "\t- [0.         0.49728924]\n",
            "\t- [0.         0.59287876]\n",
            "\t- [0.         0.74603105]\n",
            "\t- [0.         0.85358036]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.17577711]\n",
            "\t- [0.         0.12911157]\n",
            "\t- [0.         0.17577711]\n",
            "\t- [0.         0.35444728]\n",
            "\t- [0.         0.95676666]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.12911157]\n",
            "\t- [0.         0.14026205]\n",
            "\t- [0.         0.17577711]\n",
            "\t- [0.         0.35444728]\n",
            "\t- [0.         0.95676666]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.         0.19571602]\n",
            "\t- [0.         0.22839418]\n",
            "\t- [0.         0.41149712]\n",
            "\t- [0.         0.65653324]\n",
            "\t- [0.         0.74645054]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.         0.19571602]\n",
            "\t- [0.         0.22839418]\n",
            "\t- [0.         0.41149712]\n",
            "\t- [0.         0.65653324]\n",
            "\t- [0.         0.74645054]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.         0.40307248]\n",
            "\t- [0.        0.5374116]\n",
            "\t- [0.        0.8181222]\n",
            "\t- [0.        0.8181222]\n",
            "\t- [0.         0.80229676]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.         0.40307248]\n",
            "\t- [0.        0.5374116]\n",
            "\t- [0.         0.58630425]\n",
            "\t- [0.         0.80229676]\n",
            "\t- [0.        0.8181222]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.7768815358479818\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 7\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.29840574]\n",
            "\t- [0.         0.30202165]\n",
            "\t- [0.         0.60349923]\n",
            "\t- [0.         0.84804994]\n",
            "\t- [0.        0.7307514]\n",
            "\t- [0.         0.91465473]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.29840574]\n",
            "\t- [0.         0.30202165]\n",
            "\t- [0.         0.60349923]\n",
            "\t- [0.        0.7307514]\n",
            "\t- [0.         0.84804994]\n",
            "\t- [0.         0.91465473]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.21236376]\n",
            "\t- [0.         0.21236376]\n",
            "\t- [0.         0.17161937]\n",
            "\t- [0.         0.69290775]\n",
            "\t- [0.        0.9311347]\n",
            "\t- [0.        0.9882747]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.11135434]\n",
            "\t- [0.         0.17161937]\n",
            "\t- [0.         0.21236376]\n",
            "\t- [0.         0.69290775]\n",
            "\t- [0.        0.9311347]\n",
            "\t- [0.        0.9882747]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.22202495]\n",
            "\t- [0.         0.16833565]\n",
            "\t- [0.         0.20992093]\n",
            "\t- [0.         0.20992093]\n",
            "\t- [0.         0.33418277]\n",
            "\t- [0.        0.7204985]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.13073839]\n",
            "\t- [0.         0.16833565]\n",
            "\t- [0.         0.20992093]\n",
            "\t- [0.         0.22202495]\n",
            "\t- [0.         0.33418277]\n",
            "\t- [0.        0.7204985]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.        0.5868573]\n",
            "\t- [0.        0.5868573]\n",
            "\t- [0.       0.869849]\n",
            "\t- [0.         0.98808706]\n",
            "\t- [0.         0.98808706]\n",
            "\t- [0.       0.970855]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.        0.5868573]\n",
            "\t- [0.        0.6516161]\n",
            "\t- [0.        0.7268405]\n",
            "\t- [0.       0.869849]\n",
            "\t- [0.       0.970855]\n",
            "\t- [0.         0.98808706]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.         0.12096162]\n",
            "\t- [0.         0.42542294]\n",
            "\t- [0.        0.6685194]\n",
            "\t- [0.        0.6685194]\n",
            "\t- [0.         0.99103653]\n",
            "\t- [0.         0.99103653]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.         0.12096162]\n",
            "\t- [0.         0.42542294]\n",
            "\t- [0.         0.55479103]\n",
            "\t- [0.        0.6685194]\n",
            "\t- [0.         0.88960344]\n",
            "\t- [0.         0.99103653]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.9328758375985282\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 8\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.01759722]\n",
            "\t- [0.         0.01759722]\n",
            "\t- [0.         0.03964604]\n",
            "\t- [0.        0.5366481]\n",
            "\t- [0.        0.7589621]\n",
            "\t- [0.        0.7589621]\n",
            "\t- [0.         0.82206106]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.01759722]\n",
            "\t- [0.         0.03964604]\n",
            "\t- [0.        0.3296821]\n",
            "\t- [0.        0.5366481]\n",
            "\t- [0.        0.5668192]\n",
            "\t- [0.        0.7589621]\n",
            "\t- [0.         0.82206106]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.06270584]\n",
            "\t- [0.         0.06270584]\n",
            "\t- [0.         0.22447538]\n",
            "\t- [0.         0.43010527]\n",
            "\t- [0.        0.6802942]\n",
            "\t- [0.         0.91289586]\n",
            "\t- [0.         0.92603344]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.06270584]\n",
            "\t- [0.         0.22447538]\n",
            "\t- [0.         0.27879152]\n",
            "\t- [0.         0.43010527]\n",
            "\t- [0.        0.6802942]\n",
            "\t- [0.         0.91289586]\n",
            "\t- [0.         0.92603344]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.04096513]\n",
            "\t- [0.         0.04096513]\n",
            "\t- [0.        0.3381566]\n",
            "\t- [0.        0.3381566]\n",
            "\t- [0.         0.63791513]\n",
            "\t- [0.         0.63791513]\n",
            "\t- [0.        0.9132598]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.04096513]\n",
            "\t- [0.         0.15806666]\n",
            "\t- [0.         0.26019698]\n",
            "\t- [0.        0.3381566]\n",
            "\t- [0.        0.6125527]\n",
            "\t- [0.         0.63791513]\n",
            "\t- [0.        0.9132598]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.         0.09492125]\n",
            "\t- [0.         0.18197104]\n",
            "\t- [0.         0.20575121]\n",
            "\t- [0.         0.49120027]\n",
            "\t- [0.         0.51933205]\n",
            "\t- [0.        0.8667135]\n",
            "\t- [0.        0.8667135]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.         0.09492125]\n",
            "\t- [0.         0.18197104]\n",
            "\t- [0.         0.20575121]\n",
            "\t- [0.         0.49120027]\n",
            "\t- [0.         0.51933205]\n",
            "\t- [0.        0.8041966]\n",
            "\t- [0.        0.8667135]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.         0.03412947]\n",
            "\t- [0.         0.16520134]\n",
            "\t- [0.        0.3104265]\n",
            "\t- [0.       0.272395]\n",
            "\t- [0.        0.7197228]\n",
            "\t- [0.        0.7791126]\n",
            "\t- [0.        0.7791126]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.         0.03412947]\n",
            "\t- [0.         0.16520134]\n",
            "\t- [0.       0.272395]\n",
            "\t- [0.        0.3104265]\n",
            "\t- [0.        0.7197228]\n",
            "\t- [0.        0.7791126]\n",
            "\t- [0.        0.9848032]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.9990978837013245\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 9\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.02296265]\n",
            "\t- [0.         0.02296265]\n",
            "\t- [0.        0.3071479]\n",
            "\t- [0.        0.3071479]\n",
            "\t- [0.        0.3071479]\n",
            "\t- [0.         0.37982255]\n",
            "\t- [0.        0.7655599]\n",
            "\t- [0.        0.7655599]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.02296265]\n",
            "\t- [0.        0.0739265]\n",
            "\t- [0.         0.24280044]\n",
            "\t- [0.        0.3071479]\n",
            "\t- [0.         0.32781774]\n",
            "\t- [0.         0.37982255]\n",
            "\t- [0.         0.40526885]\n",
            "\t- [0.        0.7655599]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.22909343]\n",
            "\t- [0.         0.53187317]\n",
            "\t- [0.         0.33470532]\n",
            "\t- [0.         0.33470532]\n",
            "\t- [0.         0.82540786]\n",
            "\t- [0.        0.8475109]\n",
            "\t- [0.        0.9619499]\n",
            "\t- [0.        0.9619499]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.22909343]\n",
            "\t- [0.         0.33470532]\n",
            "\t- [0.         0.53187317]\n",
            "\t- [0.        0.6383281]\n",
            "\t- [0.         0.82540786]\n",
            "\t- [0.        0.8281409]\n",
            "\t- [0.        0.8475109]\n",
            "\t- [0.        0.9619499]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.10551477]\n",
            "\t- [0.         0.34770006]\n",
            "\t- [0.         0.34770006]\n",
            "\t- [0.         0.55274445]\n",
            "\t- [0.        0.7159896]\n",
            "\t- [0.         0.82901376]\n",
            "\t- [0.         0.82901376]\n",
            "\t- [0.         0.90674835]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.02969612]\n",
            "\t- [0.         0.10551477]\n",
            "\t- [0.         0.34770006]\n",
            "\t- [0.         0.55274445]\n",
            "\t- [0.        0.7159896]\n",
            "\t- [0.         0.82901376]\n",
            "\t- [0.        0.8700025]\n",
            "\t- [0.         0.90674835]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.         0.13914058]\n",
            "\t- [0.         0.13914058]\n",
            "\t- [0.         0.13914058]\n",
            "\t- [0.         0.31793025]\n",
            "\t- [0.         0.43196964]\n",
            "\t- [0.        0.5740588]\n",
            "\t- [0.        0.5740588]\n",
            "\t- [0.         0.99462515]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.         0.06306107]\n",
            "\t- [0.         0.06521652]\n",
            "\t- [0.         0.13914058]\n",
            "\t- [0.         0.31793025]\n",
            "\t- [0.         0.43196964]\n",
            "\t- [0.         0.50590324]\n",
            "\t- [0.        0.5740588]\n",
            "\t- [0.         0.99462515]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.         0.29558766]\n",
            "\t- [0.         0.41446117]\n",
            "\t- [0.         0.48089164]\n",
            "\t- [0.         0.48089164]\n",
            "\t- [0.         0.68817097]\n",
            "\t- [0.        0.7983086]\n",
            "\t- [0.        0.7983086]\n",
            "\t- [0.        0.7983086]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.         0.16669096]\n",
            "\t- [0.         0.29558766]\n",
            "\t- [0.         0.41446117]\n",
            "\t- [0.         0.48089164]\n",
            "\t- [0.         0.66501796]\n",
            "\t- [0.         0.68817097]\n",
            "\t- [0.        0.7172887]\n",
            "\t- [0.        0.7983086]\n",
            "\t- [1. 0.]\n",
            "\tloss: 1.266355938381619\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 10\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.01597487]\n",
            "\t- [0.         0.01597487]\n",
            "\t- [0.         0.05086629]\n",
            "\t- [0.         0.05086629]\n",
            "\t- [0.         0.13025518]\n",
            "\t- [0.         0.40763524]\n",
            "\t- [0.         0.40763524]\n",
            "\t- [0.         0.77421653]\n",
            "\t- [0.         0.77421653]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.01597487]\n",
            "\t- [0.         0.05086629]\n",
            "\t- [0.         0.05635012]\n",
            "\t- [0.         0.13025518]\n",
            "\t- [0.         0.35518274]\n",
            "\t- [0.       0.382553]\n",
            "\t- [0.         0.40763524]\n",
            "\t- [0.         0.49404404]\n",
            "\t- [0.         0.77421653]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.25081837]\n",
            "\t- [0.         0.25081837]\n",
            "\t- [0.         0.25355607]\n",
            "\t- [0.        0.5482569]\n",
            "\t- [0.        0.5482569]\n",
            "\t- [0.        0.8204968]\n",
            "\t- [0.        0.8204968]\n",
            "\t- [0.         0.99789774]\n",
            "\t- [0.         0.99789774]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.10004683]\n",
            "\t- [0.         0.25081837]\n",
            "\t- [0.         0.25355607]\n",
            "\t- [0.        0.5482569]\n",
            "\t- [0.         0.81414276]\n",
            "\t- [0.        0.8204968]\n",
            "\t- [0.         0.82760453]\n",
            "\t- [0.        0.8277665]\n",
            "\t- [0.         0.99789774]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 2 =====\n",
            "\n",
            "\t- [0.         0.15842572]\n",
            "\t- [0.         0.21213473]\n",
            "\t- [0.         0.28803995]\n",
            "\t- [0.         0.28803995]\n",
            "\t- [0.         0.82407266]\n",
            "\t- [0.       0.953099]\n",
            "\t- [0.       0.953099]\n",
            "\t- [0.       0.953099]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 2 ---\n",
            "\n",
            "\t- [0.         0.15842572]\n",
            "\t- [0.         0.21213473]\n",
            "\t- [0.         0.28803995]\n",
            "\t- [0.       0.565882]\n",
            "\t- [0.        0.6232203]\n",
            "\t- [0.         0.69529766]\n",
            "\t- [0.         0.82407266]\n",
            "\t- [0.         0.88470745]\n",
            "\t- [0.       0.953099]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 3 =====\n",
            "\n",
            "\t- [0.         0.07717723]\n",
            "\t- [0.         0.07717723]\n",
            "\t- [0.         0.31230733]\n",
            "\t- [0.         0.31230733]\n",
            "\t- [0.         0.52819985]\n",
            "\t- [0.         0.52819985]\n",
            "\t- [0.         0.69217026]\n",
            "\t- [0.         0.69217026]\n",
            "\t- [0.         0.69217026]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 3 ---\n",
            "\n",
            "\t- [0.         0.07717723]\n",
            "\t- [0.         0.10511298]\n",
            "\t- [0.         0.20882875]\n",
            "\t- [0.         0.31230733]\n",
            "\t- [0.         0.47770396]\n",
            "\t- [0.         0.52819985]\n",
            "\t- [0.        0.5964574]\n",
            "\t- [0.        0.6194695]\n",
            "\t- [0.         0.69217026]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 4 =====\n",
            "\n",
            "\t- [0.        0.2738984]\n",
            "\t- [0.         0.44605884]\n",
            "\t- [0.         0.64253783]\n",
            "\t- [0.         0.64253783]\n",
            "\t- [0.        0.7834352]\n",
            "\t- [0.        0.7834352]\n",
            "\t- [0.        0.7463783]\n",
            "\t- [0.         0.95955414]\n",
            "\t- [0.         0.95955414]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 4 ---\n",
            "\n",
            "\t- [0.        0.2738984]\n",
            "\t- [0.         0.44605884]\n",
            "\t- [0.        0.4489792]\n",
            "\t- [0.         0.64253783]\n",
            "\t- [0.         0.69852096]\n",
            "\t- [0.        0.7463783]\n",
            "\t- [0.        0.7675564]\n",
            "\t- [0.        0.7834352]\n",
            "\t- [0.         0.95955414]\n",
            "\t- [1. 0.]\n",
            "\tloss: 1.4297353744506835\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TGScfolJrp10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nK1yoiwmrp12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}